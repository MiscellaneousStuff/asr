{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython import display\n",
    "\n",
    "from jiwer import wer, cer\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torchaudio\n",
    "\n",
    "from torchnlp.encoders import LabelEncoder\n",
    "\n",
    "import os\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SilentSpeech(torch.utils.data.Dataset):\n",
    "    def __init__(self, metadata_path, dataset_type=None):\n",
    "        with open(metadata_path) as metadata:\n",
    "            flist = csv.reader(metadata, delimiter=\"|\", quotechar=\"'\", quoting=csv.QUOTE_MINIMAL)\n",
    "            self._flist = list(flist)\n",
    "            fis = []\n",
    "            if dataset_type:\n",
    "                for fi in self._flist:\n",
    "                    line = fi\n",
    "                    _, _, cur_dataset_type, modality = line\n",
    "                    if cur_dataset_type == dataset_type:\n",
    "                        fis.append(fi)\n",
    "            print(\"LIST OF FILES:\", self._flist[0])\n",
    "            self._flist = fis\n",
    "\n",
    "    def __getitem__(self, n):\n",
    "        line = self._flist[n]\n",
    "        cur_path, text, dataset_type, _ = line\n",
    "        waveform, sr = torchaudio.load(cur_path)\n",
    "        return (waveform, sr, text, dataset_type)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._flist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SilentSpeechPred(torch.utils.data.Dataset):\n",
    "    def __init__(self, metadata_path, dataset_type=None, silent_only=False, voiced_only=False):\n",
    "        with open(metadata_path) as metadata:\n",
    "            flist = csv.reader(metadata, delimiter=\"|\", quotechar=\"'\", quoting=csv.QUOTE_MINIMAL)\n",
    "            self._flist = list(flist)\n",
    "            fis = []\n",
    "            if dataset_type:\n",
    "                for fi in self._flist:\n",
    "                    line = fi\n",
    "                    _, _, cur_dataset_type, modality = line\n",
    "                    if cur_dataset_type == dataset_type:\n",
    "                        if silent_only and modality == \"silent\":\n",
    "                            fis.append(fi)\n",
    "                        elif voiced_only and modality == \"voiced\":\n",
    "                            fis.append(fi)\n",
    "                        elif not silent_only and not voiced_only:\n",
    "                            fis.append(fi)\n",
    "                        else:\n",
    "                            Exception(\"You've selected silent only and voiced only.\")\n",
    "\n",
    "            self._flist = fis\n",
    "            print(\"(1) LIST OF FILES:\", len(self._flist))\n",
    "            print(\"(2) LIST OF FILES:\", self._flist[0])\n",
    "\n",
    "    def __getitem__(self, n):\n",
    "        line = self._flist[n]\n",
    "        cur_path, text, dataset_type, _ = line\n",
    "        # waveform, sr = torchaudio.load(cur_path)\n",
    "        mel_spectrogram = torch.load(cur_path)\n",
    "        return (mel_spectrogram, text, dataset_type)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._flist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<unk>', ' ', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '-'] 29\n"
     ]
    }
   ],
   "source": [
    "# characters = [x for x in \" abcdefghijklmnopqrstuvwxyz0123456789-\"]\n",
    "characters = [x for x in \" abcdefghijklmnopqrstuvwxyz-\"]\n",
    "encoder = LabelEncoder(characters)\n",
    "\n",
    "print(encoder.vocab, len(encoder.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PretrainedFiles(lexicon='/home/joe/.cache/torch/hub/torchaudio/decoder-assets/librispeech-4-gram/lexicon.txt', tokens='/home/joe/.cache/torch/hub/torchaudio/decoder-assets/librispeech-4-gram/tokens.txt', lm='/home/joe/.cache/torch/hub/torchaudio/decoder-assets/librispeech-4-gram/lm.bin')\n"
     ]
    }
   ],
   "source": [
    "from torchaudio.prototype.ctc_decoder import download_pretrained_files\n",
    "files = download_pretrained_files(\"librispeech-4-gram\")\n",
    "print(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joe/.local/lib/python3.8/site-packages/torchaudio/functional/functional.py:539: UserWarning: At least one mel filterbank has all zero values. The value for `n_mels` (128) may be set too high. Or, the value for `n_freqs` (257) may be set too low.\n",
      "  warnings.warn(\n",
      "/home/joe/.local/lib/python3.8/site-packages/torchaudio/functional/functional.py:539: UserWarning: At least one mel filterbank has all zero values. The value for `n_mels` (128) may be set too high. Or, the value for `n_freqs` (201) may be set too low.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from torchaudio.prototype.ctc_decoder import ctc_decoder\n",
    "\n",
    "CUR_DATASET = \"SILENT_SPEECH\" # \"LJSPEECH\", \"SILENT_SPEECH\"\n",
    "if CUR_DATASET == \"SILENT_SPEECH\":\n",
    "    SR = 16000 # Silent Speech 22_050 # LJSpeech\n",
    "else:\n",
    "    SR = 22_050\n",
    "\n",
    "train_audio_transforms = nn.Sequential(\n",
    "    torchaudio.transforms.MelSpectrogram(\n",
    "        sample_rate=SR,\n",
    "        n_mels=128,\n",
    "        hop_length=160,\n",
    "        win_length=432,\n",
    "        n_fft=512,\n",
    "        center=False),\n",
    "    torchaudio.transforms.FrequencyMasking(freq_mask_param=15),\n",
    "    torchaudio.transforms.TimeMasking(time_mask_param=35)\n",
    ")\n",
    "\n",
    "valid_audio_transforms = torchaudio.transforms.MelSpectrogram()\n",
    "\n",
    "import jiwer\n",
    "transformation = jiwer.Compose(\\\n",
    "        [jiwer.RemovePunctuation(), jiwer.ToLowerCase()])\n",
    "\n",
    "def data_processing(data, data_type=\"train\"):\n",
    "    spectrograms = []\n",
    "    labels = []\n",
    "    input_lengths = []\n",
    "    label_lengths = []\n",
    "\n",
    "    # for (waveform, _, utterance) in data:\n",
    "    for cur in data:\n",
    "        if CUR_DATASET == \"SILENT_SPEECH\":\n",
    "            waveform, _, utterance, dataset_type = cur\n",
    "        else:\n",
    "            waveform, _, _, utterance = cur\n",
    "\n",
    "        if data_type == 'train':\n",
    "            spec = train_audio_transforms(waveform).squeeze(0).transpose(0, 1)\n",
    "        elif data_type == \"valid\":\n",
    "            spec = valid_audio_transforms(waveform).squeeze(0).transpose(0, 1)\n",
    "        else:\n",
    "            raise Exception('data_type should be train or valid')\n",
    "        spectrograms.append(spec)\n",
    "\n",
    "        label = transformation(utterance)\n",
    "        label = encoder.batch_encode(utterance.lower())\n",
    "        labels.append(label)\n",
    "        input_lengths.append(spec.shape[0]//2)\n",
    "        label_lengths.append(len(label))\n",
    "\n",
    "    spectrograms = nn.utils.rnn.pad_sequence(spectrograms, batch_first=True).unsqueeze(1).transpose(2, 3)\n",
    "    labels = nn.utils.rnn.pad_sequence(labels, batch_first=True)\n",
    "\n",
    "    return spectrograms, labels, input_lengths, label_lengths\n",
    "\n",
    "def data_processing_preds(data, data_type=\"train\"):\n",
    "    \"\"\"THIS IS ONLY FOR THE PREDICTED MEL_SPECTROGRAMS FOR THE SEMG SILENT SPEECH MODEL!\"\"\"\n",
    "    spectrograms = []\n",
    "    labels = []\n",
    "    input_lengths = []\n",
    "    label_lengths = []\n",
    "\n",
    "    # for (waveform, _, utterance) in data:\n",
    "    for cur in data:\n",
    "        mel_spectrogram, utterance, dataset_type = cur\n",
    "\n",
    "        #print(\"pre-mel shape:\", mel_spectrogram.shape)\n",
    "        # mel_spectrogram = mel_spectrogram.transpose(0, 1)\n",
    "        #print(\"post-mel shape:\", mel_spectrogram.shape)\n",
    "        spectrograms.append(mel_spectrogram)\n",
    "\n",
    "        label = transformation(utterance)\n",
    "        label = encoder.batch_encode(utterance.lower())\n",
    "        labels.append(label)\n",
    "        input_lengths.append(mel_spectrogram.shape[0]//2)\n",
    "        label_lengths.append(len(label))\n",
    "\n",
    "    spectrograms = nn.utils.rnn.pad_sequence(spectrograms, batch_first=True).unsqueeze(1).transpose(2, 3)\n",
    "    labels = nn.utils.rnn.pad_sequence(labels, batch_first=True)\n",
    "\n",
    "    return spectrograms, labels, input_lengths, label_lengths\n",
    "\n",
    "def BeamDecoder(beam_search_decoder,\n",
    "                output,\n",
    "                labels,\n",
    "                label_lengths):\n",
    "    print(\"beam output shape:\", output.shape)\n",
    "\n",
    "    decodes = []\n",
    "    targets = []\n",
    "\n",
    "    for i, pred in enumerate(output):\n",
    "        pred = pred.cpu()\n",
    "        pred = pred.unsqueeze(0)\n",
    "        decode = []\n",
    "        cur_target = labels[i][:label_lengths[i]]\n",
    "        if len(cur_target) > 0:\n",
    "            cur_target = \\\n",
    "                \"\".join(encoder.batch_decode(torch.tensor(cur_target)))\n",
    "        else:\n",
    "            cur_target = \"\"\n",
    "        targets.append(cur_target)\n",
    "\n",
    "        beam_search_result     = beam_search_decoder(pred)\n",
    "        beam_search_transcript = \" \".join(beam_search_result[0][0].words).strip()\n",
    "\n",
    "        # cur_decode = decode\n",
    "        cur_decode = beam_search_transcript\n",
    "\n",
    "        \"\"\"\n",
    "        if len(cur_decode) > 0:\n",
    "            cur_decode = \\\n",
    "                \"\".join(encoder.batch_decode(torch.tensor(cur_decode)))\n",
    "        else:\n",
    "            cur_decode = \"\"\n",
    "        \"\"\"\n",
    "        decodes.append(cur_decode)\n",
    "\n",
    "    return decodes, targets\n",
    "\n",
    "def GreedyDecoder(output, labels, label_lengths, blank_label=28, collapse_repeated=True):\n",
    "    arg_maxes = torch.argmax(output, dim=2)\n",
    "    print(\"greedy output shape:\", output.shape, arg_maxes.shape)\n",
    "    decodes = []\n",
    "    targets = []\n",
    "    for i, args in enumerate(arg_maxes):\n",
    "        decode = []\n",
    "        cur_target = labels[i][:label_lengths[i]]\n",
    "        if len(cur_target) > 0:\n",
    "            cur_target = \\\n",
    "                \"\".join(encoder.batch_decode(torch.tensor(cur_target)))\n",
    "        else:\n",
    "            cur_target = \"\"\n",
    "        targets.append(cur_target)\n",
    "        for j, index in enumerate(args):\n",
    "            if index != blank_label:\n",
    "                if collapse_repeated and j != 0 and index == args[j -1]:\n",
    "                    continue\n",
    "                decode.append(index.item())\n",
    "        cur_decode = decode\n",
    "        if len(cur_decode) > 0:\n",
    "            cur_decode = \\\n",
    "                \"\".join(encoder.batch_decode(torch.tensor(cur_decode)))\n",
    "        else:\n",
    "            cur_decode = \"\"\n",
    "        decodes.append(cur_decode)\n",
    "\n",
    "    return decodes, targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DeepSpeech2 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNLayerNorm(nn.Module):\n",
    "    \"\"\"Layer normalization built for cnns input\"\"\"\n",
    "    def __init__(self, n_feats):\n",
    "        super(CNNLayerNorm, self).__init__()\n",
    "        self.layer_norm = nn.LayerNorm(n_feats)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x (batch, channel, feature, time)\n",
    "        x = x.transpose(2, 3).contiguous() # (batch, channel, time, feature)\n",
    "        x = self.layer_norm(x)\n",
    "        return x.transpose(2, 3).contiguous() # (batch, channel, feature, time) \n",
    "\n",
    "class ResidualCNN(nn.Module):\n",
    "    \"\"\"Residual CNN inspired by https://arxiv.org/pdf/1603.05027.pdf\n",
    "        except with layer norm instead of batch norm\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, kernel, stride, dropout, n_feats):\n",
    "        super(ResidualCNN, self).__init__()\n",
    "\n",
    "        self.cnn1 = nn.Conv2d(in_channels, out_channels, kernel, stride, padding=kernel//2)\n",
    "        self.cnn2 = nn.Conv2d(out_channels, out_channels, kernel, stride, padding=kernel//2)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.layer_norm1 = CNNLayerNorm(n_feats)\n",
    "        self.layer_norm2 = CNNLayerNorm(n_feats)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x  # (batch, channel, feature, time)\n",
    "        x = self.layer_norm1(x)\n",
    "        x = F.gelu(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.cnn1(x)\n",
    "        x = self.layer_norm2(x)\n",
    "        x = F.gelu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.cnn2(x)\n",
    "        x += residual\n",
    "        return x # (batch, channel, feature, time)\n",
    "\n",
    "class BidirectionalGRU(nn.Module):\n",
    "    def __init__(self, rnn_dim, hidden_size, dropout, batch_first):\n",
    "        super(BidirectionalGRU, self).__init__()\n",
    "\n",
    "        self.BiGRU = nn.GRU(\n",
    "            input_size=rnn_dim, hidden_size=hidden_size,\n",
    "            num_layers=1, batch_first=batch_first, bidirectional=True)\n",
    "        self.layer_norm = nn.LayerNorm(rnn_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer_norm(x)\n",
    "        x = F.gelu(x)\n",
    "        x, _ = self.BiGRU(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "class SpeechRecognitionModel(nn.Module):\n",
    "    def __init__(self, n_cnn_layers, n_rnn_layers, rnn_dim, n_class, n_feats, stride=2, dropout=0.1):\n",
    "        super(SpeechRecognitionModel, self).__init__()\n",
    "        n_feats = n_feats//2\n",
    "        self.cnn = nn.Conv2d(1, 32, 3, stride=stride, padding=3//2)  # cnn for extracting heirachal features\n",
    "\n",
    "        # n residual cnn layers with filter size of 32\n",
    "        self.rescnn_layers = nn.Sequential(*[\n",
    "            ResidualCNN(32, 32, kernel=3, stride=1, dropout=dropout, n_feats=n_feats) \n",
    "            for _ in range(n_cnn_layers)\n",
    "        ])\n",
    "        self.fully_connected = nn.Linear(n_feats*32, rnn_dim)\n",
    "        self.birnn_layers = nn.Sequential(*[\n",
    "            BidirectionalGRU(rnn_dim=rnn_dim if i==0 else rnn_dim*2,\n",
    "                             hidden_size=rnn_dim, dropout=dropout, batch_first=i==0)\n",
    "            for i in range(n_rnn_layers)\n",
    "        ])\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(rnn_dim*2, rnn_dim),  # birnn returns rnn_dim*2\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(rnn_dim, n_class)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.cnn(x)\n",
    "        x = self.rescnn_layers(x)\n",
    "        sizes = x.size()\n",
    "        x = x.view(sizes[0], sizes[1] * sizes[2], sizes[3])  # (batch, feature, time)\n",
    "        x = x.transpose(1, 2) # (batch, time, feature)\n",
    "        x = self.fully_connected(x)\n",
    "        x = self.birnn_layers(x)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train / Test Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.cuda.amp.grad_scaler import GradScaler\n",
    "import random\n",
    "\n",
    "amp_enabled = True\n",
    "\n",
    "class IterMeter(object):\n",
    "    \"\"\"keeps track of total iterations\"\"\"\n",
    "    def __init__(self):\n",
    "        self.val = 0\n",
    "\n",
    "    def step(self):\n",
    "        self.val += 1\n",
    "\n",
    "    def get(self):\n",
    "        return self.val\n",
    "\n",
    "def train(model, device, train_loader, criterion, optimizer, scheduler, epoch, iter_meter, run):\n",
    "    model.train()\n",
    "    data_len = len(train_loader.dataset)\n",
    "\n",
    "    # AMP\n",
    "    scaler = GradScaler()\n",
    "\n",
    "    for batch_idx, _data in enumerate(train_loader):\n",
    "        spectrograms, labels, input_lengths, label_lengths = _data\n",
    "        spectrograms, labels = spectrograms.to(device), labels.to(device)\n",
    "\n",
    "        # optimizer.zero_grad()\n",
    "        \n",
    "        with torch.autocast(\n",
    "            enabled=amp_enabled,\n",
    "            dtype=torch.bfloat16,\n",
    "            device_type=\"cuda\"):\n",
    "\n",
    "            output = model(spectrograms)  # (batch, time, n_class)\n",
    "            output = F.log_softmax(output, dim=2)\n",
    "            output = output.transpose(0, 1) # (time, batch, n_class)\n",
    "\n",
    "            loss = criterion(output, labels, input_lengths, label_lengths)\n",
    "\n",
    "        # loss.backward()\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        # scaler.step(scheduler)\n",
    "        scaler.update()\n",
    "\n",
    "        if run:\n",
    "            run[\"train_loss\"].log(loss.item())\n",
    "            run[\"learning_rate\"].log(scheduler.get_last_lr())\n",
    "\n",
    "        #optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        iter_meter.step()\n",
    "        if batch_idx % 100 == 0 or batch_idx == data_len:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(spectrograms), data_len,\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "\n",
    "def test(model, device, test_loader, criterion, run, beam=False, beam_decoder=None):\n",
    "    print('\\nevaluating...')\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    test_cer, test_wer = [], []\n",
    "\n",
    "    if beam:\n",
    "        beam_test_cer, beam_test_wer = [], []\n",
    "\n",
    "    \"\"\"\n",
    "    starter, ender = torch.cuda.Event(enable_timing=True), torch.cuda.Event(enable_timing=True)\n",
    "    repetitions = 300\n",
    "    timings=np.zeros((repetitions,1))\n",
    "\n",
    "    N = 12\n",
    "    torch.set_num_threads(N)\n",
    "\n",
    "    # GPU warmup\n",
    "    DUMMY = [ x[0] for x in iter(test_loader).next() ]\n",
    "    print(\"DUMMY:\", len(DUMMY))\n",
    "    spectrograms, labels, input_lengths, label_lengths = DUMMY\n",
    "    dummy_input = spectrograms.unsqueeze(0).to(device)\n",
    "    for _ in range(10):\n",
    "        _ = model(dummy_input)\n",
    "\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        for i, _data in enumerate(test_loader):\n",
    "            spectrograms, labels, input_lengths, label_lengths = _data \n",
    "            spectrograms, labels = spectrograms.to(device), labels.to(device)\n",
    "            \n",
    "            output = model(spectrograms)  # (batch, time, n_class)\n",
    "\n",
    "            output = F.log_softmax(output, dim=2)\n",
    "            output = output.transpose(0, 1) # (time, batch, n_class)\n",
    "\n",
    "            loss = criterion(output, labels, input_lengths, label_lengths)\n",
    "            test_loss += loss.item() / len(test_loader)\n",
    "\n",
    "            decoded_preds, decoded_targets = GreedyDecoder(output.transpose(0, 1), labels, label_lengths)\n",
    "            if beam:\n",
    "                beam_preds, beam_targets = \\\n",
    "                    BeamDecoder(beam_decoder, output.transpose(0, 1), labels, label_lengths)\n",
    "\n",
    "            print(f\"greedy TEST {i}\", decoded_preds[0:3], decoded_targets[0:3])\n",
    "            if beam:\n",
    "                print(f\"beam TEST {i}\", beam_preds, beam_targets)\n",
    "\n",
    "            #print(\"Targets:\", decoded_targets[0:2])\n",
    "            #print(\"Preds:\", decoded_preds[0:2])\n",
    "                \n",
    "            for j in range(len(decoded_preds)):\n",
    "                test_cer.append(cer(decoded_targets[j], decoded_preds[j]))\n",
    "                test_wer.append(wer(decoded_targets[j], decoded_preds[j]))\n",
    "\n",
    "                if beam:\n",
    "                    beam_test_cer.append(cer(beam_targets[j], beam_preds[j]))\n",
    "                    beam_test_wer.append(cer(beam_targets[j], beam_preds[j]))\n",
    "\n",
    "    \"\"\"\n",
    "    # inference\n",
    "    with torch.no_grad():\n",
    "        for rep in range(repetitions):\n",
    "            starter.record()\n",
    "            _ = model(dummy_input)\n",
    "            ender.record()\n",
    "            torch.cuda.synchronize()\n",
    "            curr_time = starter.elapsed_time(ender)\n",
    "            run[\"infer_log\"].log(curr_time)\n",
    "            timings[rep] = curr_time\n",
    "    mean_syn = np.sum(timings) / repetitions\n",
    "    std_syn = np.std(timings)\n",
    "\n",
    "    repetitions = 100\n",
    "\n",
    "    # throughput\n",
    "    total_time = 0\n",
    "    optimal_batch_size = 1\n",
    "    with torch.no_grad():\n",
    "        for rep in range(repetitions):\n",
    "            starter, ender = torch.cuda.Event(enable_timing=True), torch.cuda.Event(enable_timing=True)\n",
    "            starter.record()\n",
    "            _ = model(dummy_input)\n",
    "            ender.record()\n",
    "            torch.cuda.synchronize()\n",
    "            curr_time = starter.elapsed_time(ender)/1000\n",
    "            run[\"throughput_log\"].log(curr_time)\n",
    "            total_time += curr_time\n",
    "    throughput = (repetitions*optimal_batch_size) / total_time\n",
    "    \"\"\"\n",
    "\n",
    "    avg_cer = sum(test_cer) / len(test_cer)\n",
    "    avg_wer = sum(test_wer) / len(test_wer)\n",
    "\n",
    "    if beam:\n",
    "        avg_beam_cer = sum(beam_test_cer) / len(beam_test_cer)\n",
    "        avg_beam_wer = sum(beam_test_wer) / len(beam_test_wer)\n",
    "\n",
    "    if run:\n",
    "        run[\"test_loss\"].log(test_loss)\n",
    "        run[\"cer\"].log(avg_cer)\n",
    "        run[\"wer\"].log(avg_wer)\n",
    "        if beam:\n",
    "            run[\"beam_cer\"].log(avg_beam_cer)\n",
    "            run[\"beam_wer\"].log(avg_beam_wer)\n",
    "        #run[\"mean_syn\"].log(mean_syn)\n",
    "        #run[\"throughput\"].log(throughput)\n",
    "    \n",
    "    print('Test set: Average loss: {:.4f}, Average CER: {:4f} Average WER: {:.4f}\\n'.format(test_loss, avg_cer, avg_wer))\n",
    "    return test_loss, avg_wer\n",
    "\n",
    "def main(dataset_path, learning_rate=5e-4, batch_size=20, \\\n",
    "    epochs=10, checkpoint_path=\"\", run=None, device=None, quantize=False, semg_eval=False, \\\n",
    "    semg_train=False, silent_only=False, voiced_only=False, beam=False):\n",
    "    hparams = {\n",
    "        \"n_cnn_layers\":  3,\n",
    "        \"n_rnn_layers\":  5,\n",
    "        \"rnn_dim\":       512,\n",
    "        \"n_class\":       len(encoder.vocab),\n",
    "        \"n_feats\":       128,\n",
    "        \"stride\":        2,\n",
    "        \"dropout\":       0.1,\n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"batch_size\":    batch_size,\n",
    "        \"epochs\":        epochs\n",
    "    }\n",
    "\n",
    "    if run:\n",
    "        run[\"hparams\"] = hparams\n",
    "        run[\"quantize\"] = quantize\n",
    "\n",
    "    print(\"hparams:\", hparams)\n",
    "\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    seed = 7\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    if not device:\n",
    "        device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "        print(\"device:\", device)\n",
    "    else:\n",
    "        device = torch.device(device)\n",
    "\n",
    "    if CUR_DATASET == \"SILENT_SPEECH\":\n",
    "        if semg_eval:\n",
    "            train_dataset = None\n",
    "            test_dataset  = SilentSpeechPred(\\\n",
    "                \"./utils/metadata_dgaddy_preds.csv\", dataset_type=\"test\", silent_only=True)\n",
    "        else:\n",
    "            if semg_train:\n",
    "                train_dataset = SilentSpeechPred(\\\n",
    "                    \"./utils/metadata_dgaddy_preds.csv\",\n",
    "                    dataset_type=\"train\",\n",
    "                    silent_only=silent_only,\n",
    "                    voiced_only=voiced_only)\n",
    "                test_dataset  = SilentSpeechPred(\\\n",
    "                    \"./utils/metadata_dgaddy_preds.csv\",\n",
    "                    dataset_type=\"test\",\n",
    "                    silent_only=True)\n",
    "                print(\"LEN TRAIN TEST:\", len(train_dataset), len(test_dataset))\n",
    "            else:\n",
    "                train_dataset = SilentSpeech(\"./utils/metadata_dgaddy.csv\", dataset_type=\"train\")\n",
    "                test_dataset  = SilentSpeech(\"./utils/metadata_dgaddy.csv\", dataset_type=\"test\")\n",
    "                print(\"LEN TRAIN TEST:\", len(train_dataset), len(test_dataset))\n",
    "            \n",
    "    else:\n",
    "        dataset = torchaudio.datasets.LJSPEECH(dataset_path, download=False)\n",
    "\n",
    "    kwargs = {'num_workers': 4, 'pin_memory': True} if use_cuda else {}\n",
    "\n",
    "    \"\"\"\n",
    "    dataset_len = 500 # int(len(dataset) * 1.0)\n",
    "    train_split = int(dataset_len * 0.9)\n",
    "    test_split  = dataset_len - train_split\n",
    "\n",
    "    dataset = torch.utils.data.Subset(dataset, range(0, dataset_len))\n",
    "    train_dataset, test_dataset = \\\n",
    "        torch.utils.data.random_split(dataset, [train_split, test_split])\n",
    "    \"\"\"\n",
    "    \n",
    "    if train_dataset:\n",
    "        if semg_train:\n",
    "            train_loader = data.DataLoader(dataset=train_dataset,\n",
    "                                        batch_size=hparams['batch_size'],\n",
    "                                        shuffle=True,\n",
    "                                        collate_fn=lambda x: data_processing_preds(x, 'train'),\n",
    "                                        **kwargs)\n",
    "        else:\n",
    "            train_loader = data.DataLoader(dataset=train_dataset,\n",
    "                                        batch_size=hparams['batch_size'],\n",
    "                                        shuffle=True,\n",
    "                                        collate_fn=lambda x: data_processing(x, 'train'),\n",
    "                                        **kwargs)\n",
    "\n",
    "    if semg_eval or semg_train:\n",
    "        test_loader = data.DataLoader(dataset=test_dataset,\n",
    "                                    batch_size=hparams['batch_size'],\n",
    "                                    shuffle=False,\n",
    "                                    collate_fn=lambda x: data_processing_preds(x, 'valid'),\n",
    "                                    **kwargs)\n",
    "    else:\n",
    "        test_loader = data.DataLoader(dataset=test_dataset,\n",
    "                                    batch_size=hparams['batch_size'],\n",
    "                                    shuffle=False,\n",
    "                                    collate_fn=lambda x: data_processing(x, 'valid'),\n",
    "                                    **kwargs)\n",
    "\n",
    "    if train_dataset:                       \n",
    "        print(\"(BATCHES) TRAIN LEN, TEST LEN:\", len(train_loader), len(test_loader))\n",
    "\n",
    "    model = SpeechRecognitionModel(\n",
    "        hparams['n_cnn_layers'], hparams['n_rnn_layers'], hparams['rnn_dim'],\n",
    "        hparams['n_class'], hparams['n_feats'], hparams['stride'], hparams['dropout']\n",
    "    ).to(device)\n",
    "\n",
    "    if checkpoint_path:\n",
    "        model.load_state_dict(torch.load(checkpoint_path))\n",
    "    \n",
    "    if quantize:\n",
    "        model = torch.quantization.quantize_dynamic(\n",
    "            model,  # the original model\n",
    "            {torch.nn.GRU, torch.nn.Linear},  # a set of layers to dynamically quantize\n",
    "            dtype=torch.qint8)\n",
    "\n",
    "    # print(\"QUANTIZE:\", quantize)\n",
    "\n",
    "    print(model)\n",
    "    print('Num Model Parameters', sum([param.nelement() for param in model.parameters()]))\n",
    "\n",
    "    if beam:\n",
    "        tokens_path  = \"beam_decoder/tokens.txt\"\n",
    "        lm_weight    = 0 # 6.00 # 3.23\n",
    "        word_score   = 0 # -0.26\n",
    "        lexicon_path = \"beam_decoder/lexicon.txt\"\n",
    "        use_lm = True\n",
    "        beam_size    = 1\n",
    "        nbest        = 1\n",
    "\n",
    "        # beam params\n",
    "        run[\"beam_size\"] = beam_size\n",
    "        run[\"beam_use_lm\"] = use_lm\n",
    "        run[\"beam_lm_weight\"] = lm_weight\n",
    "        run[\"beam_word_score\"] = word_score\n",
    "        run[\"beam_nbest\"] = nbest\n",
    "\n",
    "        lm = files.lm if use_lm else None\n",
    "\n",
    "        beam_search_decoder = ctc_decoder(\n",
    "            lexicon=lexicon_path,\n",
    "            tokens=tokens_path,\n",
    "            lm=lm,\n",
    "            nbest=nbest,\n",
    "            beam_size=beam_size,\n",
    "            lm_weight=lm_weight,\n",
    "            word_score=word_score,\n",
    "            sil_token=\"<unk>\")\n",
    "    else:\n",
    "        beam_search_decoder = None\n",
    "\n",
    "    optimizer = optim.AdamW(model.parameters(), hparams['learning_rate'])\n",
    "    criterion = nn.CTCLoss(blank=28).to(device)\n",
    "    # criterion = nn.CTCLoss(blank=0).to(device)\n",
    "    if train_dataset:\n",
    "        scheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr=hparams['learning_rate'], \n",
    "                                                steps_per_epoch=int(len(train_loader)),\n",
    "                                                epochs=hparams['epochs'],\n",
    "                                                anneal_strategy='linear')\n",
    "    # scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', 0.5, patience=5)\n",
    "\n",
    "    best_test_loss = float(\"inf\")\n",
    "    best_avg_wer = float(\"inf\")\n",
    "\n",
    "    iter_meter = IterMeter()\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        if train_dataset:\n",
    "            train(model, device, train_loader, criterion, optimizer, scheduler, epoch, iter_meter, run)\n",
    "        test_loss, avg_wer = test(model, device, test_loader, criterion, run, beam, beam_search_decoder)\n",
    "\n",
    "        if train_dataset:\n",
    "            # if test_loss < best_test_loss:\n",
    "            if avg_wer < best_avg_wer:\n",
    "                torch.save(model.state_dict(), f\"./models/ds2_DATASET_{CUR_DATASET}_EPOCHS_{epoch}_TEST_LOSS_{test_loss}_WER_{avg_wer}\")\n",
    "                best_avg_wer = avg_wer\n",
    "    \n",
    "    return best_avg_wer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPU Runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Apr 29 22:36:26 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 510.47.03    Driver Version: 510.47.03    CUDA Version: 11.6     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ...  On   | 00000000:2B:00.0  On |                  N/A |\n",
      "| 94%   44C    P8    27W / 200W |    507MiB /  8192MiB |      4%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      1090      G   /usr/lib/xorg/Xorg                 35MiB |\n",
      "|    0   N/A  N/A      1783      G   /usr/lib/xorg/Xorg                129MiB |\n",
      "|    0   N/A  N/A      1914      G   /usr/bin/gnome-shell               51MiB |\n",
      "|    0   N/A  N/A      3420      G   ...076731924428850054,131072      161MiB |\n",
      "|    0   N/A  N/A     14770      G   ...RendererForSitePerProcess      116MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def go(device=None,\n",
    "       quantize=False,\n",
    "       semg_eval=False,\n",
    "       semg_train=False,\n",
    "       silent_only=False,\n",
    "       voiced_only=False,\n",
    "       epochs=200,\n",
    "       checkpoint_path=\"\",\n",
    "       beam=False):\n",
    "    from dotenv import dotenv_values\n",
    "    import neptune.new as neptune\n",
    "    config = dotenv_values(\".env\")\n",
    "\n",
    "    learning_rate = 5e-4\n",
    "    # learning_rate = 5e-5\n",
    "    batch_size = 5 # 20\n",
    "    epochs = epochs # 200 # 50 # 10\n",
    "    dataset_path = \"/mnt/datasets/ljspeech/\"\n",
    "    # checkpoint_path = \"/home/joe/projects/asr/models/silent_speech_asr_5_pc/1/ds2_DATASET_SILENT_SPEECH_EPOCHS_255_TEST_LOSS_-0.4287225107351939\"\n",
    "    checkpoint_path = checkpoint_path\n",
    "\n",
    "    neptune_project = config[\"NEPTUNE_PROJECT\"]\n",
    "    neptune_token   = config[\"NEPTUNE_TOKEN\"]\n",
    "\n",
    "    run = neptune.init(project=neptune_project,\n",
    "                    api_token=neptune_token)\n",
    "    \n",
    "    # run = None\n",
    "\n",
    "    if run:\n",
    "        run[\"dataset\"] = CUR_DATASET\n",
    "        run[\"checkpoint_path\"] = checkpoint_path\n",
    "        run[\"device\"] = device\n",
    "        run[\"beam\"] = beam\n",
    "        \n",
    "    final_wer = main(\n",
    "        dataset_path,\n",
    "        learning_rate,\n",
    "        batch_size,\n",
    "        epochs,\n",
    "        checkpoint_path,\n",
    "        run,\n",
    "        device,\n",
    "        quantize,\n",
    "        semg_eval,\n",
    "        semg_train,\n",
    "        silent_only,\n",
    "        voiced_only,\n",
    "        beam)\n",
    "        \n",
    "    if run:\n",
    "        run.stop()\n",
    "    return final_wer\n",
    "\n",
    "# go(device=\"cuda\", quantize=False)\n",
    "\n",
    "#final_wer = go()\n",
    "#print(final_wer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train on Ground Truth Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = \\\n",
    "    \"/home/joe/projects/asr/models/silent_speech_asr_100_pc/1/\"\\\n",
    "    \"ds2_DATASET_SILENT_SPEECH_EPOCHS_23_TEST_LOSS_0.5270409451460275_45_WER\"\n",
    "\n",
    "final_wer = go(\n",
    "    semg_eval=False,\n",
    "    semg_train=False,\n",
    "    epochs=100, # open vocab parallel := 100, closed vocab := 200\n",
    "    checkpoint_path=checkpoint_path)\n",
    "print(final_wer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train on Transduction Silent Speech Predictions (E<sub>s</sub> Preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://app.neptune.ai/miscellaneousstuff/asr-initial-experiments/e/AS-226\n",
      "Remember to stop your run once you’ve finished logging your metadata (https://docs.neptune.ai/api-reference/run#.stop). It will be stopped automatically only when the notebook kernel/interactive console is terminated.\n",
      "hparams: {'n_cnn_layers': 3, 'n_rnn_layers': 5, 'rnn_dim': 512, 'n_class': 29, 'n_feats': 128, 'stride': 2, 'dropout': 0.1, 'learning_rate': 0.0005, 'batch_size': 5, 'epochs': 100}\n",
      "device: cuda\n",
      "(1) LIST OF FILES: 8192\n",
      "(2) LIST OF FILES: ['/home/joe/projects/silent_speech/pred_audio/open_vocab_parallel/voiced/991', 'Death!\" and leaving him to digest that if he could, I hurried on after the artillery-man.', 'train', 'voiced']\n",
      "(1) LIST OF FILES: 99\n",
      "(2) LIST OF FILES: ['/home/joe/projects/silent_speech/pred_audio/open_vocab_parallel/silent/616', 'They seemed very helpless in that pit of theirs.', 'test', 'silent']\n",
      "LEN TRAIN TEST: 8192 99\n",
      "(BATCHES) TRAIN LEN, TEST LEN: 1639 20\n",
      "SpeechRecognitionModel(\n",
      "  (cnn): Conv2d(1, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "  (rescnn_layers): Sequential(\n",
      "    (0): ResidualCNN(\n",
      "      (cnn1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (cnn2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (dropout1): Dropout(p=0.1, inplace=False)\n",
      "      (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      (layer_norm1): CNNLayerNorm(\n",
      "        (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (layer_norm2): CNNLayerNorm(\n",
      "        (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "    (1): ResidualCNN(\n",
      "      (cnn1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (cnn2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (dropout1): Dropout(p=0.1, inplace=False)\n",
      "      (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      (layer_norm1): CNNLayerNorm(\n",
      "        (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (layer_norm2): CNNLayerNorm(\n",
      "        (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "    (2): ResidualCNN(\n",
      "      (cnn1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (cnn2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (dropout1): Dropout(p=0.1, inplace=False)\n",
      "      (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      (layer_norm1): CNNLayerNorm(\n",
      "        (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (layer_norm2): CNNLayerNorm(\n",
      "        (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (fully_connected): Linear(in_features=2048, out_features=512, bias=True)\n",
      "  (birnn_layers): Sequential(\n",
      "    (0): BidirectionalGRU(\n",
      "      (BiGRU): GRU(512, 512, batch_first=True, bidirectional=True)\n",
      "      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): BidirectionalGRU(\n",
      "      (BiGRU): GRU(1024, 512, bidirectional=True)\n",
      "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (2): BidirectionalGRU(\n",
      "      (BiGRU): GRU(1024, 512, bidirectional=True)\n",
      "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (3): BidirectionalGRU(\n",
      "      (BiGRU): GRU(1024, 512, bidirectional=True)\n",
      "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (4): BidirectionalGRU(\n",
      "      (BiGRU): GRU(1024, 512, bidirectional=True)\n",
      "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=512, bias=True)\n",
      "    (1): GELU(approximate=none)\n",
      "    (2): Dropout(p=0.1, inplace=False)\n",
      "    (3): Linear(in_features=512, out_features=29, bias=True)\n",
      "  )\n",
      ")"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: string series 'monitoring/stdout' value was longer than 1000 characters and was truncated. This warning is printed only once per series.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Num Model Parameters 23705373\n",
      "Train Epoch: 1 [0/8192 (0%)]\tLoss: 3.475727\n",
      "Train Epoch: 1 [500/8192 (6%)]\tLoss: 1.664095\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "checkpoint_path = \\\n",
    "    \"/home/joe/projects/asr/models/silent_speech_open_full/silent_speech_open_parallel(silent_and_vocal_preds_and_ground)/\"\\\n",
    "    \"ds2_DATASET_SILENT_SPEECH_EPOCHS_11_TEST_LOSS_1.9239023327827456_WER_0.6827406467582209\"\n",
    "# checkpoint_path = \"\"\n",
    "\"\"\"\n",
    "\n",
    "checkpoint_path = \\\n",
    "    \"/home/joe/projects/asr/models/silent_speech_asr_100_pc/1/\"\\\n",
    "    \"ds2_DATASET_SILENT_SPEECH_EPOCHS_23_TEST_LOSS_0.5270409451460275_45_WER\"\n",
    "\n",
    "final_wer = go(\n",
    "    semg_eval=False,\n",
    "    semg_train=True,\n",
    "    silent_only=False,\n",
    "    voiced_only=False,\n",
    "    beam=False,\n",
    "    epochs=100, # open vocab parallel := 100, closed vocab := 200\n",
    "    checkpoint_path=checkpoint_path)\n",
    "    \n",
    "print(final_wer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eval on Silent Speech Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://app.neptune.ai/miscellaneousstuff/asr-initial-experiments/e/AS-209\n",
      "Remember to stop your run once you’ve finished logging your metadata (https://docs.neptune.ai/api-reference/run#.stop). It will be stopped automatically only when the notebook kernel/interactive console is terminated.\n",
      "hparams: {'n_cnn_layers': 3, 'n_rnn_layers': 5, 'rnn_dim': 512, 'n_class': 29, 'n_feats': 128, 'stride': 2, 'dropout': 0.1, 'learning_rate': 0.0005, 'batch_size': 5, 'epochs': 1}\n",
      "device: cuda\n",
      "(1) LIST OF FILES: 99\n",
      "(2) LIST OF FILES: ['/home/joe/projects/silent_speech/pred_audio/open_vocab_parallel/silent/616', 'They seemed very helpless in that pit of theirs.', 'test', 'silent']\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'seek'. You can only torch.load from a file that is seekable. Please pre-load the data into a buffer like io.BytesIO and try to load from it instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/serialization.py:309\u001b[0m, in \u001b[0;36m_check_seekable\u001b[0;34m(f)\u001b[0m\n\u001b[1;32m    <a href='file:///home/joe/.local/lib/python3.8/site-packages/torch/serialization.py?line=307'>308</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///home/joe/.local/lib/python3.8/site-packages/torch/serialization.py?line=308'>309</a>\u001b[0m     f\u001b[39m.\u001b[39;49mseek(f\u001b[39m.\u001b[39mtell())\n\u001b[1;32m    <a href='file:///home/joe/.local/lib/python3.8/site-packages/torch/serialization.py?line=309'>310</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mTrue\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'seek'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/joe/projects/asr/train.ipynb Cell 24'\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/joe/projects/asr/train.ipynb#ch0000023?line=0'>1</a>\u001b[0m checkpoint_path \u001b[39m=\u001b[39m \\\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/joe/projects/asr/train.ipynb#ch0000023?line=1'>2</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m/home/joe/projects/asr/models/silent_speech_open_parallel/silent_speech_open_parallel(silent_and_vocal_preds_and_ground)/\u001b[39m\u001b[39m\"\u001b[39m\\\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/joe/projects/asr/train.ipynb#ch0000023?line=2'>3</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mds2_DATASET_SILENT_SPEECH_EPOCHS_9_TEST_LOSS_2.351101124286652_WER_0.798235669013118\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/joe/projects/asr/train.ipynb#ch0000023?line=4'>5</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/joe/projects/asr/train.ipynb#ch0000023?line=5'>6</a>\u001b[0m \u001b[39mcheckpoint_path = \\\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/joe/projects/asr/train.ipynb#ch0000023?line=6'>7</a>\u001b[0m \u001b[39m        \"/home/joe/projects/asr/models/silent_speech_asr_100_pc/2/\"\\\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/joe/projects/asr/train.ipynb#ch0000023?line=7'>8</a>\u001b[0m \u001b[39m        \"ds2_DATASET_SILENT_SPEECH_EPOCHS_41_TEST_LOSS_0.6224472553241907_51_WER\"\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/joe/projects/asr/train.ipynb#ch0000023?line=8'>9</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/joe/projects/asr/train.ipynb#ch0000023?line=10'>11</a>\u001b[0m final_wer \u001b[39m=\u001b[39m go(\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/joe/projects/asr/train.ipynb#ch0000023?line=11'>12</a>\u001b[0m     semg_eval\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/joe/projects/asr/train.ipynb#ch0000023?line=12'>13</a>\u001b[0m     epochs\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/joe/projects/asr/train.ipynb#ch0000023?line=13'>14</a>\u001b[0m     checkpoint_path \u001b[39m=\u001b[39;49m checkpoint_path,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/joe/projects/asr/train.ipynb#ch0000023?line=14'>15</a>\u001b[0m     beam\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/joe/projects/asr/train.ipynb#ch0000023?line=15'>16</a>\u001b[0m \u001b[39mprint\u001b[39m(final_wer)\n",
      "\u001b[1;32m/home/joe/projects/asr/train.ipynb Cell 18'\u001b[0m in \u001b[0;36mgo\u001b[0;34m(device, quantize, semg_eval, semg_train, silent_only, voiced_only, epochs, checkpoint_path, beam)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/joe/projects/asr/train.ipynb#ch0000017?line=32'>33</a>\u001b[0m     run[\u001b[39m\"\u001b[39m\u001b[39mdevice\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m device\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/joe/projects/asr/train.ipynb#ch0000017?line=33'>34</a>\u001b[0m     run[\u001b[39m\"\u001b[39m\u001b[39mbeam\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m beam\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/joe/projects/asr/train.ipynb#ch0000017?line=35'>36</a>\u001b[0m final_wer \u001b[39m=\u001b[39m main(\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/joe/projects/asr/train.ipynb#ch0000017?line=36'>37</a>\u001b[0m     dataset_path,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/joe/projects/asr/train.ipynb#ch0000017?line=37'>38</a>\u001b[0m     learning_rate,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/joe/projects/asr/train.ipynb#ch0000017?line=38'>39</a>\u001b[0m     batch_size,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/joe/projects/asr/train.ipynb#ch0000017?line=39'>40</a>\u001b[0m     epochs,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/joe/projects/asr/train.ipynb#ch0000017?line=40'>41</a>\u001b[0m     checkpoint_path,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/joe/projects/asr/train.ipynb#ch0000017?line=41'>42</a>\u001b[0m     run,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/joe/projects/asr/train.ipynb#ch0000017?line=42'>43</a>\u001b[0m     device,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/joe/projects/asr/train.ipynb#ch0000017?line=43'>44</a>\u001b[0m     quantize,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/joe/projects/asr/train.ipynb#ch0000017?line=44'>45</a>\u001b[0m     semg_eval,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/joe/projects/asr/train.ipynb#ch0000017?line=45'>46</a>\u001b[0m     semg_train,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/joe/projects/asr/train.ipynb#ch0000017?line=46'>47</a>\u001b[0m     silent_only,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/joe/projects/asr/train.ipynb#ch0000017?line=47'>48</a>\u001b[0m     voiced_only,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/joe/projects/asr/train.ipynb#ch0000017?line=48'>49</a>\u001b[0m     beam)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/joe/projects/asr/train.ipynb#ch0000017?line=50'>51</a>\u001b[0m \u001b[39mif\u001b[39;00m run:\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/joe/projects/asr/train.ipynb#ch0000017?line=51'>52</a>\u001b[0m     run\u001b[39m.\u001b[39mstop()\n",
      "\u001b[1;32m/home/joe/projects/asr/train.ipynb Cell 14'\u001b[0m in \u001b[0;36mmain\u001b[0;34m(dataset_path, learning_rate, batch_size, epochs, checkpoint_path, run, device, quantize, semg_eval, semg_train, silent_only, voiced_only, beam)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/joe/projects/asr/train.ipynb#ch0000013?line=271'>272</a>\u001b[0m model \u001b[39m=\u001b[39m SpeechRecognitionModel(\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/joe/projects/asr/train.ipynb#ch0000013?line=272'>273</a>\u001b[0m     hparams[\u001b[39m'\u001b[39m\u001b[39mn_cnn_layers\u001b[39m\u001b[39m'\u001b[39m], hparams[\u001b[39m'\u001b[39m\u001b[39mn_rnn_layers\u001b[39m\u001b[39m'\u001b[39m], hparams[\u001b[39m'\u001b[39m\u001b[39mrnn_dim\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/joe/projects/asr/train.ipynb#ch0000013?line=273'>274</a>\u001b[0m     hparams[\u001b[39m'\u001b[39m\u001b[39mn_class\u001b[39m\u001b[39m'\u001b[39m], hparams[\u001b[39m'\u001b[39m\u001b[39mn_feats\u001b[39m\u001b[39m'\u001b[39m], hparams[\u001b[39m'\u001b[39m\u001b[39mstride\u001b[39m\u001b[39m'\u001b[39m], hparams[\u001b[39m'\u001b[39m\u001b[39mdropout\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/joe/projects/asr/train.ipynb#ch0000013?line=274'>275</a>\u001b[0m )\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/joe/projects/asr/train.ipynb#ch0000013?line=276'>277</a>\u001b[0m \u001b[39mif\u001b[39;00m checkpoint_path:\n\u001b[0;32m--> <a href='vscode-notebook-cell:/home/joe/projects/asr/train.ipynb#ch0000013?line=277'>278</a>\u001b[0m     model\u001b[39m.\u001b[39mload_state_dict(torch\u001b[39m.\u001b[39;49mload(checkpoint_path))\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/joe/projects/asr/train.ipynb#ch0000013?line=279'>280</a>\u001b[0m \u001b[39mif\u001b[39;00m quantize:\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/joe/projects/asr/train.ipynb#ch0000013?line=280'>281</a>\u001b[0m     model \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mquantization\u001b[39m.\u001b[39mquantize_dynamic(\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/joe/projects/asr/train.ipynb#ch0000013?line=281'>282</a>\u001b[0m         model,  \u001b[39m# the original model\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/joe/projects/asr/train.ipynb#ch0000013?line=282'>283</a>\u001b[0m         {torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mGRU, torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mLinear},  \u001b[39m# a set of layers to dynamically quantize\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/joe/projects/asr/train.ipynb#ch0000013?line=283'>284</a>\u001b[0m         dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mqint8)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/serialization.py:699\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    <a href='file:///home/joe/.local/lib/python3.8/site-packages/torch/serialization.py?line=695'>696</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mencoding\u001b[39m\u001b[39m'\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m pickle_load_args\u001b[39m.\u001b[39mkeys():\n\u001b[1;32m    <a href='file:///home/joe/.local/lib/python3.8/site-packages/torch/serialization.py?line=696'>697</a>\u001b[0m     pickle_load_args[\u001b[39m'\u001b[39m\u001b[39mencoding\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m--> <a href='file:///home/joe/.local/lib/python3.8/site-packages/torch/serialization.py?line=698'>699</a>\u001b[0m \u001b[39mwith\u001b[39;00m _open_file_like(f, \u001b[39m'\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m opened_file:\n\u001b[1;32m    <a href='file:///home/joe/.local/lib/python3.8/site-packages/torch/serialization.py?line=699'>700</a>\u001b[0m     \u001b[39mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m    <a href='file:///home/joe/.local/lib/python3.8/site-packages/torch/serialization.py?line=700'>701</a>\u001b[0m         \u001b[39m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/joe/.local/lib/python3.8/site-packages/torch/serialization.py?line=701'>702</a>\u001b[0m         \u001b[39m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/joe/.local/lib/python3.8/site-packages/torch/serialization.py?line=702'>703</a>\u001b[0m         \u001b[39m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/joe/.local/lib/python3.8/site-packages/torch/serialization.py?line=703'>704</a>\u001b[0m         orig_position \u001b[39m=\u001b[39m opened_file\u001b[39m.\u001b[39mtell()\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/serialization.py:236\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    <a href='file:///home/joe/.local/lib/python3.8/site-packages/torch/serialization.py?line=233'>234</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m _open_buffer_writer(name_or_buffer)\n\u001b[1;32m    <a href='file:///home/joe/.local/lib/python3.8/site-packages/torch/serialization.py?line=234'>235</a>\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mr\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m mode:\n\u001b[0;32m--> <a href='file:///home/joe/.local/lib/python3.8/site-packages/torch/serialization.py?line=235'>236</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m _open_buffer_reader(name_or_buffer)\n\u001b[1;32m    <a href='file:///home/joe/.local/lib/python3.8/site-packages/torch/serialization.py?line=236'>237</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    <a href='file:///home/joe/.local/lib/python3.8/site-packages/torch/serialization.py?line=237'>238</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mExpected \u001b[39m\u001b[39m'\u001b[39m\u001b[39mr\u001b[39m\u001b[39m'\u001b[39m\u001b[39m or \u001b[39m\u001b[39m'\u001b[39m\u001b[39mw\u001b[39m\u001b[39m'\u001b[39m\u001b[39m in mode but got \u001b[39m\u001b[39m{\u001b[39;00mmode\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/serialization.py:221\u001b[0m, in \u001b[0;36m_open_buffer_reader.__init__\u001b[0;34m(self, buffer)\u001b[0m\n\u001b[1;32m    <a href='file:///home/joe/.local/lib/python3.8/site-packages/torch/serialization.py?line=218'>219</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, buffer):\n\u001b[1;32m    <a href='file:///home/joe/.local/lib/python3.8/site-packages/torch/serialization.py?line=219'>220</a>\u001b[0m     \u001b[39msuper\u001b[39m(_open_buffer_reader, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(buffer)\n\u001b[0;32m--> <a href='file:///home/joe/.local/lib/python3.8/site-packages/torch/serialization.py?line=220'>221</a>\u001b[0m     _check_seekable(buffer)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/serialization.py:312\u001b[0m, in \u001b[0;36m_check_seekable\u001b[0;34m(f)\u001b[0m\n\u001b[1;32m    <a href='file:///home/joe/.local/lib/python3.8/site-packages/torch/serialization.py?line=309'>310</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/joe/.local/lib/python3.8/site-packages/torch/serialization.py?line=310'>311</a>\u001b[0m \u001b[39mexcept\u001b[39;00m (io\u001b[39m.\u001b[39mUnsupportedOperation, \u001b[39mAttributeError\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m--> <a href='file:///home/joe/.local/lib/python3.8/site-packages/torch/serialization.py?line=311'>312</a>\u001b[0m     raise_err_msg([\u001b[39m\"\u001b[39;49m\u001b[39mseek\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mtell\u001b[39;49m\u001b[39m\"\u001b[39;49m], e)\n\u001b[1;32m    <a href='file:///home/joe/.local/lib/python3.8/site-packages/torch/serialization.py?line=312'>313</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/serialization.py:305\u001b[0m, in \u001b[0;36m_check_seekable.<locals>.raise_err_msg\u001b[0;34m(patterns, e)\u001b[0m\n\u001b[1;32m    <a href='file:///home/joe/.local/lib/python3.8/site-packages/torch/serialization.py?line=300'>301</a>\u001b[0m     \u001b[39mif\u001b[39;00m p \u001b[39min\u001b[39;00m \u001b[39mstr\u001b[39m(e):\n\u001b[1;32m    <a href='file:///home/joe/.local/lib/python3.8/site-packages/torch/serialization.py?line=301'>302</a>\u001b[0m         msg \u001b[39m=\u001b[39m (\u001b[39mstr\u001b[39m(e) \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m. You can only torch.load from a file that is seekable.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///home/joe/.local/lib/python3.8/site-packages/torch/serialization.py?line=302'>303</a>\u001b[0m                         \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m Please pre-load the data into a buffer like io.BytesIO and\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///home/joe/.local/lib/python3.8/site-packages/torch/serialization.py?line=303'>304</a>\u001b[0m                         \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m try to load from it instead.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> <a href='file:///home/joe/.local/lib/python3.8/site-packages/torch/serialization.py?line=304'>305</a>\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mtype\u001b[39m(e)(msg)\n\u001b[1;32m    <a href='file:///home/joe/.local/lib/python3.8/site-packages/torch/serialization.py?line=305'>306</a>\u001b[0m \u001b[39mraise\u001b[39;00m e\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'seek'. You can only torch.load from a file that is seekable. Please pre-load the data into a buffer like io.BytesIO and try to load from it instead."
     ]
    }
   ],
   "source": [
    "checkpoint_path = \\\n",
    "        \"/home/joe/projects/asr/models/silent_speech_open_parallel/silent_speech_open_parallel(silent_and_vocal_preds_and_ground)/\"\\\n",
    "        \"ds2_DATASET_SILENT_SPEECH_EPOCHS_9_TEST_LOSS_2.351101124286652_WER_0.798235669013118\"\n",
    "\n",
    "\"\"\"\n",
    "checkpoint_path = \\\n",
    "        \"/home/joe/projects/asr/models/silent_speech_asr_100_pc/2/\"\\\n",
    "        \"ds2_DATASET_SILENT_SPEECH_EPOCHS_41_TEST_LOSS_0.6224472553241907_51_WER\"\n",
    "\"\"\"\n",
    "\n",
    "final_wer = go(\n",
    "    semg_eval=True,\n",
    "    epochs=1,\n",
    "    checkpoint_path = checkpoint_path,\n",
    "    beam=False)\n",
    "print(final_wer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Post-Training Quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference Time, Throughput (CPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://app.neptune.ai/miscellaneousstuff/asr-initial-experiments/e/AS-93\n",
      "Remember to stop your run once you’ve finished logging your metadata (https://docs.neptune.ai/api-reference/run#.stop). It will be stopped automatically only when the notebook kernel/interactive console is terminated.\n",
      "hparams: {'n_cnn_layers': 3, 'n_rnn_layers': 5, 'rnn_dim': 512, 'n_class': 39, 'n_feats': 128, 'stride': 2, 'dropout': 0.1, 'learning_rate': 0.0005, 'batch_size': 20, 'epochs': 1}\n",
      "(BATCHES) TRAIN LEN, TEST LEN: 23 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: string series 'monitoring/stdout' value was longer than 1000 characters and was truncated. This warning is printed only once per series.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QUANTIZE: True\n",
      "SpeechRecognitionModel(\n",
      "  (cnn): Conv2d(1, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "  (rescnn_layers): Sequential(\n",
      "    (0): ResidualCNN(\n",
      "      (cnn1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (cnn2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (dropout1): Dropout(p=0.1, inplace=False)\n",
      "      (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      (layer_norm1): CNNLayerNorm(\n",
      "        (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (layer_norm2): CNNLayerNorm(\n",
      "        (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "    (1): ResidualCNN(\n",
      "      (cnn1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (cnn2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (dropout1): Dropout(p=0.1, inplace=False)\n",
      "      (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      (layer_norm1): CNNLayerNorm(\n",
      "        (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (layer_norm2): CNNLayerNorm(\n",
      "        (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "    (2): ResidualCNN(\n",
      "      (cnn1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (cnn2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (dropout1): Dropout(p=0.1, inplace=False)\n",
      "      (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      (layer_norm1): CNNLayerNorm(\n",
      "        (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (layer_norm2): CNNLayerNorm(\n",
      "        (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (fully_connected): DynamicQuantizedLinear(in_features=2048, out_features=512, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "  (birnn_layers): Sequential(\n",
      "    (0): BidirectionalGRU(\n",
      "      (BiGRU): DynamicQuantizedGRU(512, 512, batch_first=True, bidirectional=True)\n",
      "      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): BidirectionalGRU(\n",
      "      (BiGRU): DynamicQuantizedGRU(1024, 512, bidirectional=True)\n",
      "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (2): BidirectionalGRU(\n",
      "      (BiGRU): DynamicQuantizedGRU(1024, 512, bidirectional=True)\n",
      "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (3): BidirectionalGRU(\n",
      "      (BiGRU): DynamicQuantizedGRU(1024, 512, bidirectional=True)\n",
      "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (4): BidirectionalGRU(\n",
      "      (BiGRU): DynamicQuantizedGRU(1024, 512, bidirectional=True)\n",
      "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): DynamicQuantizedLinear(in_features=1024, out_features=512, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "    (1): GELU()\n",
      "    (2): Dropout(p=0.1, inplace=False)\n",
      "    (3): DynamicQuantizedLinear(in_features=512, out_features=39, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "  )\n",
      ")\n",
      "Num Model Parameters 65792\n",
      "\n",
      "evaluating...\n",
      "DUMMY: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_24232/1912479192.py:61: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \"\".join(encoder.batch_decode(torch.tensor(cur_target)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Targets: ['11<unk>50 pm', 'july 31 1999']\n",
      "Preds: ['11<unk>5 pm', 'july 31 199']\n",
      "Targets: ['october 23 2006', 'monday july 09']\n",
      "Preds: ['october 23 26', 'monday july 9']\n",
      "Targets: ['monday february 28', 'march 31 1893']\n",
      "Preds: ['monday february 28', 'march 1 1893']\n",
      "Test set: Average loss: -0.3829, Average CER: 0.081869 Average WER: 0.3567\n",
      "\n",
      "Shutting down background jobs, please wait a moment...\n",
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Waiting for the remaining 13 operations to synchronize with Neptune. Do not kill this process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All 13 operations synced, thanks for waiting!\n"
     ]
    }
   ],
   "source": [
    "final_wer = go(device=\"cpu\", quantize=True)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
