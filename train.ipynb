{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython import display\n",
    "\n",
    "from jiwer import wer, cer\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torchaudio\n",
    "\n",
    "from torchnlp.encoders import LabelEncoder\n",
    "\n",
    "import os\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SilentSpeech(torch.utils.data.Dataset):\n",
    "    def __init__(self, metadata_path, dataset_type=None):\n",
    "        with open(metadata_path) as metadata:\n",
    "            flist = csv.reader(metadata, delimiter=\"|\", quotechar=\"'\", quoting=csv.QUOTE_MINIMAL)\n",
    "            self._flist = list(flist)\n",
    "            fis = []\n",
    "            if dataset_type:\n",
    "                for fi in self._flist:\n",
    "                    line = fi\n",
    "                    _, _, cur_dataset_type, modality = line\n",
    "                    if cur_dataset_type == dataset_type:\n",
    "                        fis.append(fi)\n",
    "            print(\"LIST OF FILES:\", self._flist[0])\n",
    "            self._flist = fis\n",
    "\n",
    "    def __getitem__(self, n):\n",
    "        line = self._flist[n]\n",
    "        cur_path, text, dataset_type, _ = line\n",
    "        waveform, sr = torchaudio.load(cur_path)\n",
    "        return (waveform, sr, text, dataset_type)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._flist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SilentSpeechPred(torch.utils.data.Dataset):\n",
    "    def __init__(self, metadata_path, dataset_type=None, silent_only=False, voiced_only=False):\n",
    "        with open(metadata_path) as metadata:\n",
    "            flist = csv.reader(metadata, delimiter=\"|\", quotechar=\"'\", quoting=csv.QUOTE_MINIMAL)\n",
    "            self._flist = list(flist)\n",
    "            fis = []\n",
    "            if dataset_type:\n",
    "                for fi in self._flist:\n",
    "                    line = fi\n",
    "                    _, _, cur_dataset_type, modality = line\n",
    "                    if cur_dataset_type == dataset_type:\n",
    "                        if silent_only and modality == \"silent\":\n",
    "                            fis.append(fi)\n",
    "                        elif voiced_only and modality == \"voiced\":\n",
    "                            fis.append(fi)\n",
    "                        elif not silent_only and not voiced_only:\n",
    "                            fis.append(fi)\n",
    "                        else:\n",
    "                            Exception(\"You've selected silent only and voiced only.\")\n",
    "\n",
    "            self._flist = fis\n",
    "            print(\"(1) LIST OF FILES:\", len(self._flist))\n",
    "            print(\"(2) LIST OF FILES:\", self._flist[0])\n",
    "\n",
    "    def __getitem__(self, n):\n",
    "        line = self._flist[n]\n",
    "        cur_path, text, dataset_type, _ = line\n",
    "        # waveform, sr = torchaudio.load(cur_path)\n",
    "        mel_spectrogram = torch.load(cur_path)\n",
    "        return (mel_spectrogram, text, dataset_type)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._flist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<unk>', ' ', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '-'] 29\n"
     ]
    }
   ],
   "source": [
    "# characters = [x for x in \" abcdefghijklmnopqrstuvwxyz0123456789-\"]\n",
    "characters = [x for x in \" abcdefghijklmnopqrstuvwxyz-\"]\n",
    "encoder = LabelEncoder(characters)\n",
    "\n",
    "print(encoder.vocab, len(encoder.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PretrainedFiles(lexicon='/home/joe/.cache/torch/hub/torchaudio/decoder-assets/librispeech/lexicon.txt', tokens='/home/joe/.cache/torch/hub/torchaudio/decoder-assets/librispeech/tokens.txt', lm=None)\n"
     ]
    }
   ],
   "source": [
    "from torchaudio.prototype.ctc_decoder import download_pretrained_files\n",
    "# files = download_pretrained_files(\"librispeech-4-gram\")\n",
    "files = download_pretrained_files(\"librispeech\")\n",
    "print(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joe/.local/lib/python3.8/site-packages/torchaudio/functional/functional.py:539: UserWarning: At least one mel filterbank has all zero values. The value for `n_mels` (128) may be set too high. Or, the value for `n_freqs` (257) may be set too low.\n",
      "  warnings.warn(\n",
      "/home/joe/.local/lib/python3.8/site-packages/torchaudio/functional/functional.py:539: UserWarning: At least one mel filterbank has all zero values. The value for `n_mels` (128) may be set too high. Or, the value for `n_freqs` (201) may be set too low.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from torchaudio.prototype.ctc_decoder import ctc_decoder\n",
    "\n",
    "MULTI_DATASET = True\n",
    "CUR_DATASET = \"SILENT_SPEECH\" # \"LJSPEECH\", \"SILENT_SPEECH\"\n",
    "if CUR_DATASET == \"SILENT_SPEECH\":\n",
    "    SR = 16_000 # Silent Speech 22_050 # LJSpeech\n",
    "else:\n",
    "    SR = 22_050\n",
    "\n",
    "train_audio_transforms = nn.Sequential(\n",
    "    torchaudio.transforms.MelSpectrogram(\n",
    "        sample_rate=SR,\n",
    "        n_mels=128,\n",
    "        hop_length=160,\n",
    "        win_length=432,\n",
    "        n_fft=512,\n",
    "        center=False),\n",
    "    torchaudio.transforms.FrequencyMasking(freq_mask_param=15),\n",
    "    torchaudio.transforms.TimeMasking(time_mask_param=35)\n",
    ")\n",
    "\n",
    "valid_audio_transforms = torchaudio.transforms.MelSpectrogram()\n",
    "\n",
    "import jiwer\n",
    "transformation = jiwer.Compose(\\\n",
    "        [jiwer.RemovePunctuation(), jiwer.ToLowerCase()])\n",
    "\n",
    "def data_processing(data, data_type=\"train\"):\n",
    "    spectrograms = []\n",
    "    labels = []\n",
    "    input_lengths = []\n",
    "    label_lengths = []\n",
    "\n",
    "    # for (waveform, _, utterance) in data:\n",
    "    for cur in data:\n",
    "        if CUR_DATASET == \"SILENT_SPEECH\":\n",
    "            waveform, _, utterance, dataset_type = cur\n",
    "        else:\n",
    "            waveform, _, _, utterance = cur\n",
    "\n",
    "        if data_type == 'train':\n",
    "            spec = train_audio_transforms(waveform).squeeze(0).transpose(0, 1)\n",
    "        elif data_type == \"valid\":\n",
    "            spec = valid_audio_transforms(waveform).squeeze(0).transpose(0, 1)\n",
    "        else:\n",
    "            raise Exception('data_type should be train or valid')\n",
    "        spectrograms.append(spec)\n",
    "\n",
    "        label = transformation(utterance)\n",
    "        label = encoder.batch_encode(utterance.lower())\n",
    "        labels.append(label)\n",
    "        input_lengths.append(spec.shape[0]//2)\n",
    "        label_lengths.append(len(label))\n",
    "\n",
    "    if model_name == \"ds2\":\n",
    "        spectrograms = nn.utils.rnn.pad_sequence(spectrograms, batch_first=True).unsqueeze(1).transpose(2, 3)\n",
    "    elif model_name == \"conformer\":\n",
    "        # spectrograms = nn.utils.rnn.pad_sequence(spectrograms, batch_first=True).unsqueeze(1)\n",
    "        spectrograms = nn.utils.rnn.pad_sequence(spectrograms, batch_first=True).unsqueeze(1).transpose(2, 3)\n",
    "    labels = nn.utils.rnn.pad_sequence(labels, batch_first=True)\n",
    "\n",
    "    return spectrograms, labels, input_lengths, label_lengths\n",
    "\n",
    "def data_processing_preds(data, data_type=\"train\"):\n",
    "    \"\"\"THIS IS ONLY FOR THE PREDICTED MEL_SPECTROGRAMS FOR THE SEMG SILENT SPEECH MODEL!\"\"\"\n",
    "    spectrograms = []\n",
    "    labels = []\n",
    "    input_lengths = []\n",
    "    label_lengths = []\n",
    "\n",
    "    # for (waveform, _, utterance) in data:\n",
    "    for cur in data:\n",
    "        mel_spectrogram, utterance, dataset_type = cur\n",
    "\n",
    "        #print(\"pre-mel shape:\", mel_spectrogram.shape)\n",
    "        # mel_spectrogram = mel_spectrogram.transpose(0, 1)\n",
    "        #print(\"post-mel shape:\", mel_spectrogram.shape)\n",
    "        spectrograms.append(mel_spectrogram)\n",
    "\n",
    "        label = transformation(utterance)\n",
    "        label = encoder.batch_encode(utterance.lower())\n",
    "        labels.append(label)\n",
    "        input_lengths.append(mel_spectrogram.shape[0]//2)\n",
    "        label_lengths.append(len(label))\n",
    "\n",
    "    if model_name == \"ds2\":\n",
    "        spectrograms = nn.utils.rnn.pad_sequence(spectrograms, batch_first=True).unsqueeze(1).transpose(2, 3)\n",
    "    elif model_name == \"conformer\":\n",
    "        spectrograms = nn.utils.rnn.pad_sequence(spectrograms, batch_first=True).unsqueeze(1)\n",
    "\n",
    "    labels = nn.utils.rnn.pad_sequence(labels, batch_first=True)\n",
    "\n",
    "    return spectrograms, labels, input_lengths, label_lengths\n",
    "\n",
    "def BeamDecoder(beam_search_decoder,\n",
    "                output,\n",
    "                labels,\n",
    "                label_lengths,\n",
    "                blank_label=28,\n",
    "                collapse_repeated=True):\n",
    "    print(\"beam output shape:\", output.shape)\n",
    "\n",
    "    decodes = []\n",
    "    targets = []\n",
    "\n",
    "    for i, pred in enumerate(output):\n",
    "        pred = pred.cpu()\n",
    "        pred = pred.unsqueeze(0)\n",
    "        \n",
    "        cur_target = labels[i][:label_lengths[i]]\n",
    "        if len(cur_target) > 0:\n",
    "            cur_target = \\\n",
    "                \"\".join(encoder.batch_decode(torch.tensor(cur_target)))\n",
    "        else:\n",
    "            cur_target = \"\"\n",
    "        targets.append(cur_target)\n",
    "\n",
    "        # ([5, 773, 29]) -> ([1, 773, 29])\n",
    "        # print(\"pred shape:\", pred.shape)\n",
    "        \n",
    "        beam_search_result     = beam_search_decoder(pred)\n",
    "        beam_search_transcript = \" \".join(beam_search_result[0][0].words).strip()\n",
    "        \n",
    "        \"\"\"\n",
    "        # https://machinelearningmastery.com/beam-search-decoder-natural-language-processing/\n",
    "        from math import log\n",
    "        k = 10\n",
    "        sequences = [[list(), 0.0]]\n",
    "        for row in pred.squeeze(0):\n",
    "            all_candidates = list()\n",
    "            for j in range(len(sequences)):\n",
    "                seq, score = sequences[j]\n",
    "                for l in range(len(row)):\n",
    "                    # print(\"ROW SHAPE:\", row.shape, row[l].shape, row[l])\n",
    "                    # candidate = [seq + [l], score - log(row[l])]\n",
    "                    candidate = [seq + [l], score - row[l]]\n",
    "                    all_candidates.append(candidate)\n",
    "            ordered = sorted(all_candidates, key=lambda tup: tup[1])\n",
    "            sequences = ordered[:k]\n",
    "        \n",
    "        decode = []\n",
    "        args = sequences[-1][0]\n",
    "        for j, index in enumerate(args):\n",
    "            if index != blank_label:\n",
    "                if collapse_repeated and j != 0 and index == args[j -1]:\n",
    "                    continue\n",
    "                decode.append(index)\n",
    "        cur_decode = decode\n",
    "        decode = \"\".join(encoder.batch_decode(torch.tensor(cur_decode)))\n",
    "        \n",
    "        # cur_decode = decode\n",
    "        cur_decode = decode\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        if len(cur_decode) > 0:\n",
    "            cur_decode = \\\n",
    "                \"\".join(encoder.batch_decode(torch.tensor(cur_decode)))\n",
    "        else:\n",
    "            cur_decode = \"\"\n",
    "        \"\"\"\n",
    "        decodes.append(beam_search_transcript)\n",
    "\n",
    "    return decodes, targets\n",
    "\n",
    "def GreedyDecoder(output, labels, label_lengths, blank_label=28, collapse_repeated=True):\n",
    "    arg_maxes = torch.argmax(output, dim=2)\n",
    "    print(\"greedy output shape:\", output.shape, arg_maxes.shape)\n",
    "    decodes = []\n",
    "    targets = []\n",
    "    for i, args in enumerate(arg_maxes):\n",
    "        decode = []\n",
    "        cur_target = labels[i][:label_lengths[i]]\n",
    "        if len(cur_target) > 0:\n",
    "            cur_target = \\\n",
    "                \"\".join(encoder.batch_decode(torch.tensor(cur_target)))\n",
    "        else:\n",
    "            cur_target = \"\"\n",
    "        targets.append(cur_target)\n",
    "\n",
    "        # Greedy decoding process\n",
    "        for j, index in enumerate(args):\n",
    "            if index != blank_label:\n",
    "                if collapse_repeated and j != 0 and index == args[j -1]:\n",
    "                    continue\n",
    "                decode.append(index)\n",
    "        cur_decode = decode\n",
    "        \n",
    "        if len(cur_decode) > 0:\n",
    "            cur_decode = \\\n",
    "                \"\".join(encoder.batch_decode(torch.tensor(cur_decode)))\n",
    "        else:\n",
    "            cur_decode = \"\"\n",
    "        decodes.append(cur_decode)\n",
    "\n",
    "    return decodes, targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DeepSpeech2 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNLayerNorm(nn.Module):\n",
    "    \"\"\"Layer normalization built for cnns input\"\"\"\n",
    "    def __init__(self, n_feats):\n",
    "        super(CNNLayerNorm, self).__init__()\n",
    "        self.layer_norm = nn.LayerNorm(n_feats)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x (batch, channel, feature, time)\n",
    "        x = x.transpose(2, 3).contiguous() # (batch, channel, time, feature)\n",
    "        x = self.layer_norm(x)\n",
    "        return x.transpose(2, 3).contiguous() # (batch, channel, feature, time) \n",
    "\n",
    "class ResidualCNN(nn.Module):\n",
    "    \"\"\"Residual CNN inspired by https://arxiv.org/pdf/1603.05027.pdf\n",
    "        except with layer norm instead of batch norm\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, kernel, stride, dropout, n_feats):\n",
    "        super(ResidualCNN, self).__init__()\n",
    "\n",
    "        self.cnn1 = nn.Conv2d(in_channels, out_channels, kernel, stride, padding=kernel//2)\n",
    "        self.cnn2 = nn.Conv2d(out_channels, out_channels, kernel, stride, padding=kernel//2)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.layer_norm1 = CNNLayerNorm(n_feats)\n",
    "        self.layer_norm2 = CNNLayerNorm(n_feats)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x  # (batch, channel, feature, time)\n",
    "        x = self.layer_norm1(x)\n",
    "        x = F.gelu(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.cnn1(x)\n",
    "        x = self.layer_norm2(x)\n",
    "        x = F.gelu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.cnn2(x)\n",
    "        x += residual\n",
    "        return x # (batch, channel, feature, time)\n",
    "\n",
    "class BidirectionalGRU(nn.Module):\n",
    "    def __init__(self, rnn_dim, hidden_size, dropout, batch_first):\n",
    "        super(BidirectionalGRU, self).__init__()\n",
    "\n",
    "        self.BiGRU = nn.GRU(\n",
    "            input_size=rnn_dim, hidden_size=hidden_size,\n",
    "            num_layers=1, batch_first=batch_first, bidirectional=True)\n",
    "        self.layer_norm = nn.LayerNorm(rnn_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer_norm(x)\n",
    "        x = F.gelu(x)\n",
    "        x, _ = self.BiGRU(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "class SpeechRecognitionModel(nn.Module):\n",
    "    def __init__(self, n_cnn_layers, n_rnn_layers, rnn_dim, n_class, n_feats, stride=2, dropout=0.1):\n",
    "        super(SpeechRecognitionModel, self).__init__()\n",
    "        n_feats = n_feats//2\n",
    "        self.cnn = nn.Conv2d(1, 32, 3, stride=stride, padding=3//2)  # cnn for extracting heirachal features\n",
    "\n",
    "        # n residual cnn layers with filter size of 32\n",
    "        self.rescnn_layers = nn.Sequential(*[\n",
    "            ResidualCNN(32, 32, kernel=3, stride=1, dropout=dropout, n_feats=n_feats) \n",
    "            for _ in range(n_cnn_layers)\n",
    "        ])\n",
    "        self.fully_connected = nn.Linear(n_feats*32, rnn_dim)\n",
    "        self.birnn_layers = nn.Sequential(*[\n",
    "            BidirectionalGRU(rnn_dim=rnn_dim if i==0 else rnn_dim*2,\n",
    "                             hidden_size=rnn_dim, dropout=dropout, batch_first=i==0)\n",
    "            for i in range(n_rnn_layers)\n",
    "        ])\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(rnn_dim*2, rnn_dim),  # birnn returns rnn_dim*2\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(rnn_dim, n_class)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # print(\"PRE MODEL INPUT SHAPE:\", x.shape)\n",
    "        x = self.cnn(x)\n",
    "        x = self.rescnn_layers(x)\n",
    "        # print(\"POST MODEL INPUT SHAPE:\", x.shape)\n",
    "        sizes = x.size()\n",
    "        x = x.view(sizes[0], sizes[1] * sizes[2], sizes[3])  # (batch, feature, time)\n",
    "        # print(\"VIEWED MODEL INPUT SHAPE:\", x.shape)\n",
    "        x = x.transpose(1, 2) # (batch, time, feature)\n",
    "        x = self.fully_connected(x)\n",
    "        x = self.birnn_layers(x)\n",
    "        x = self.classifier(x)\n",
    "        # print(\"POST-SEQ SHAPE:\", x.shape)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train / Test Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.cuda.amp.grad_scaler import GradScaler\n",
    "import random\n",
    "\n",
    "amp_enabled = True\n",
    "model_name = \"ds2\"\n",
    "\n",
    "from conformer import Conformer\n",
    "\n",
    "class IterMeter(object):\n",
    "    \"\"\"keeps track of total iterations\"\"\"\n",
    "    def __init__(self):\n",
    "        self.val = 0\n",
    "\n",
    "    def step(self):\n",
    "        self.val += 1\n",
    "\n",
    "    def get(self):\n",
    "        return self.val\n",
    "\n",
    "def train(model, device, train_loader, criterion, optimizer, scheduler, epoch, iter_meter, run):\n",
    "    model.train()\n",
    "    data_len = len(train_loader.dataset)\n",
    "\n",
    "    # AMP\n",
    "    scaler = GradScaler()\n",
    "\n",
    "    for batch_idx, _data in enumerate(train_loader):\n",
    "        spectrograms, labels, input_lengths, label_lengths = _data\n",
    "        spectrograms, labels = spectrograms.to(device), labels.to(device)\n",
    "\n",
    "        # optimizer.zero_grad()\n",
    "        \n",
    "        with torch.autocast(\n",
    "            enabled=amp_enabled,\n",
    "            dtype=torch.bfloat16,\n",
    "            device_type=\"cuda\"):\n",
    "\n",
    "            if model_name == \"ds2\":\n",
    "                output = model(spectrograms)  # (batch, time, n_class)\n",
    "                output = F.log_softmax(output, dim=2)\n",
    "                output = output.transpose(0, 1) # (time, batch, n_class)\n",
    "\n",
    "                # print(\"input lengths shape:\", input_lengths.shape)\n",
    "                loss = criterion(output, labels, input_lengths, label_lengths)\n",
    "            elif model_name == \"conformer\":\n",
    "                spectrograms = spectrograms.squeeze(1)\n",
    "                # print(\"spectrogram:\", spectrograms.shape)\n",
    "                input_lens = torch.IntTensor(input_lengths)\n",
    "                labels = torch.LongTensor(labels.cpu()) # .to(device)\n",
    "                label_lens = torch.LongTensor(label_lengths)\n",
    "\n",
    "                output, output_lengths = model(spectrograms, input_lens)\n",
    "                loss = criterion(output.transpose(0, 1), labels, output_lengths, label_lens)\n",
    "\n",
    "        # loss.backward()\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        # scaler.step(scheduler)\n",
    "        scaler.update()\n",
    "\n",
    "        if run:\n",
    "            run[\"train_loss\"].log(loss.item())\n",
    "            run[\"learning_rate\"].log(scheduler.get_last_lr())\n",
    "\n",
    "        #optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        iter_meter.step()\n",
    "        if batch_idx % 100 == 0 or batch_idx == data_len:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(spectrograms), data_len,\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "\n",
    "def test(model, device, test_loader, criterion, run, beam=False, beam_decoder=None):\n",
    "    print('\\nevaluating...')\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    test_cer, test_wer = [], []\n",
    "\n",
    "    if beam:\n",
    "        beam_test_cer, beam_test_wer = [], []\n",
    "\n",
    "    \"\"\"\n",
    "    starter, ender = torch.cuda.Event(enable_timing=True), torch.cuda.Event(enable_timing=True)\n",
    "    repetitions = 300\n",
    "    timings=np.zeros((repetitions,1))\n",
    "\n",
    "    N = 12\n",
    "    torch.set_num_threads(N)\n",
    "\n",
    "    # GPU warmup\n",
    "    DUMMY = [ x[0] for x in iter(test_loader).next() ]\n",
    "    print(\"DUMMY:\", len(DUMMY))\n",
    "    spectrograms, labels, input_lengths, label_lengths = DUMMY\n",
    "    dummy_input = spectrograms.unsqueeze(0).to(device)\n",
    "    for _ in range(10):\n",
    "        _ = model(dummy_input)\n",
    "\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        for i, _data in enumerate(test_loader):\n",
    "            spectrograms, labels, input_lengths, label_lengths = _data \n",
    "            spectrograms, labels = spectrograms.to(device), labels.to(device)\n",
    "\n",
    "            if model_name == \"ds2\":\n",
    "                print(\"SPECTROGRAM SHAPE:\", spectrograms.shape)\n",
    "                output = model(spectrograms)  # (batch, time, n_class)\n",
    "                output = F.log_softmax(output, dim=2)\n",
    "                output = output.transpose(0, 1) # (time, batch, n_class)\n",
    "\n",
    "                # print(\"input lengths shape:\", input_lengths.shape)\n",
    "                loss = criterion(output, labels, input_lengths, label_lengths)\n",
    "            elif model_name == \"conformer\":\n",
    "                # spectrograms = spectrograms.squeeze(1)\n",
    "                print(\"spectrogram:\", spectrograms.shape)\n",
    "                input_lens = torch.IntTensor(input_lengths)\n",
    "                labels = torch.LongTensor(labels.cpu()) # .to(device)\n",
    "                label_lens = torch.LongTensor(label_lengths)\n",
    "\n",
    "                output, output_lengths = model(spectrograms, input_lens)\n",
    "                output = F.log_softmax(output, dim=2)\n",
    "                loss = criterion(output.transpose(0, 1), labels, output_lengths, label_lens)\n",
    "            \n",
    "            \"\"\"\n",
    "            output = model(spectrograms)  # (batch, time, n_class)\n",
    "\n",
    "            output = F.log_softmax(output, dim=2)\n",
    "            output = output.transpose(0, 1) # (time, batch, n_class)\n",
    "\n",
    "            loss = criterion(output, labels, input_lengths, label_lengths)\n",
    "            \"\"\"\n",
    "            \n",
    "            test_loss += loss.item() / len(test_loader)\n",
    "\n",
    "            decoded_preds, decoded_targets = GreedyDecoder(output.transpose(0, 1), labels, label_lengths)\n",
    "            if beam:\n",
    "                beam_preds, beam_targets = \\\n",
    "                    BeamDecoder(beam_decoder, output.transpose(0, 1), labels, label_lengths)\n",
    "\n",
    "            print(f\"greedy TEST {i}\", decoded_preds[0:3], decoded_targets[0:3])\n",
    "            if beam:\n",
    "                print(f\"beam TEST {i}\", beam_preds, beam_targets)\n",
    "\n",
    "            #print(\"Targets:\", decoded_targets[0:2])\n",
    "            #print(\"Preds:\", decoded_preds[0:2])\n",
    "                \n",
    "            for j in range(len(decoded_preds)):\n",
    "                test_cer.append(cer(decoded_targets[j], decoded_preds[j]))\n",
    "                test_wer.append(wer(decoded_targets[j], decoded_preds[j]))\n",
    "\n",
    "                if beam:\n",
    "                    beam_test_cer.append(cer(beam_targets[j], beam_preds[j]))\n",
    "                    beam_test_wer.append(wer(beam_targets[j], beam_preds[j]))\n",
    "\n",
    "    \"\"\"\n",
    "    # inference\n",
    "    with torch.no_grad():\n",
    "        for rep in range(repetitions):\n",
    "            starter.record()\n",
    "            _ = model(dummy_input)\n",
    "            ender.record()\n",
    "            torch.cuda.synchronize()\n",
    "            curr_time = starter.elapsed_time(ender)\n",
    "            run[\"infer_log\"].log(curr_time)\n",
    "            timings[rep] = curr_time\n",
    "    mean_syn = np.sum(timings) / repetitions\n",
    "    std_syn = np.std(timings)\n",
    "\n",
    "    repetitions = 100\n",
    "\n",
    "    # throughput\n",
    "    total_time = 0\n",
    "    optimal_batch_size = 1\n",
    "    with torch.no_grad():\n",
    "        for rep in range(repetitions):\n",
    "            starter, ender = torch.cuda.Event(enable_timing=True), torch.cuda.Event(enable_timing=True)\n",
    "            starter.record()\n",
    "            _ = model(dummy_input)\n",
    "            ender.record()\n",
    "            torch.cuda.synchronize()\n",
    "            curr_time = starter.elapsed_time(ender)/1000\n",
    "            run[\"throughput_log\"].log(curr_time)\n",
    "            total_time += curr_time\n",
    "    throughput = (repetitions*optimal_batch_size) / total_time\n",
    "    \"\"\"\n",
    "\n",
    "    avg_cer = sum(test_cer) / len(test_cer)\n",
    "    avg_wer = sum(test_wer) / len(test_wer)\n",
    "\n",
    "    if beam:\n",
    "        avg_beam_cer = sum(beam_test_cer) / len(beam_test_cer)\n",
    "        avg_beam_wer = sum(beam_test_wer) / len(beam_test_wer)\n",
    "\n",
    "    if run:\n",
    "        run[\"test_loss\"].log(test_loss)\n",
    "        run[\"cer\"].log(avg_cer)\n",
    "        run[\"wer\"].log(avg_wer)\n",
    "        if beam:\n",
    "            run[\"beam_cer\"].log(avg_beam_cer)\n",
    "            run[\"beam_wer\"].log(avg_beam_wer)\n",
    "        #run[\"mean_syn\"].log(mean_syn)\n",
    "        #run[\"throughput\"].log(throughput)\n",
    "    \n",
    "    print('Test set: Average loss: {:.4f}, Average CER: {:4f} Average WER: {:.4f}\\n'.format(test_loss, avg_cer, avg_wer))\n",
    "    return test_loss, avg_wer\n",
    "\n",
    "def main(dataset_path, learning_rate=5e-4, batch_size=20, \\\n",
    "    epochs=10, checkpoint_path=\"\", run=None, device=None, quantize=False, semg_eval=False, \\\n",
    "    semg_train=False, silent_only=False, voiced_only=False, beam=False):\n",
    "    hparams = {\n",
    "        \"n_cnn_layers\":  3,\n",
    "        \"n_rnn_layers\":  5,\n",
    "        \"rnn_dim\":       512,\n",
    "        \"n_class\":       len(encoder.vocab),\n",
    "        \"n_feats\":       128,\n",
    "        \"stride\":        2,\n",
    "        \"dropout\":       0.1,\n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"batch_size\":    batch_size,\n",
    "        \"epochs\":        epochs\n",
    "    }\n",
    "\n",
    "    if run:\n",
    "        run[\"hparams\"] = hparams\n",
    "        run[\"quantize\"] = quantize\n",
    "\n",
    "    print(\"hparams:\", hparams)\n",
    "\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    seed = 7\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    if not device:\n",
    "        device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "        print(\"device:\", device)\n",
    "    else:\n",
    "        device = torch.device(device)\n",
    "\n",
    "    if CUR_DATASET == \"SILENT_SPEECH\":\n",
    "        if semg_eval:\n",
    "            train_dataset = None\n",
    "            test_dataset  = SilentSpeech(\"./utils/metadata_dgaddy.csv\", dataset_type=\"test\")\n",
    "            test_dataset  = SilentSpeechPred(\\\n",
    "                \"./utils/metadata_dgaddy_preds.csv\", dataset_type=\"test\", silent_only=True)\n",
    "        else:\n",
    "            if semg_train:\n",
    "                train_dataset = SilentSpeechPred(\\\n",
    "                    \"./utils/metadata_dgaddy_preds.csv\",\n",
    "                    dataset_type=\"train\",\n",
    "                    silent_only=silent_only,\n",
    "                    voiced_only=voiced_only)\n",
    "                test_dataset  = SilentSpeechPred(\\\n",
    "                    \"./utils/metadata_dgaddy_preds.csv\",\n",
    "                    dataset_type=\"test\",\n",
    "                    silent_only=True)\n",
    "                print(\"LEN TRAIN TEST:\", len(train_dataset), len(test_dataset))\n",
    "            else:\n",
    "                train_dataset = SilentSpeech(\"./utils/metadata_dgaddy.csv\", dataset_type=\"train\")\n",
    "                test_dataset  = SilentSpeech(\"./utils/metadata_dgaddy.csv\", dataset_type=\"test\")\n",
    "                print(\"LEN TRAIN TEST:\", len(train_dataset), len(test_dataset))\n",
    "            \n",
    "    else:\n",
    "        dataset = torchaudio.datasets.LJSPEECH(dataset_path, download=False)\n",
    "\n",
    "    kwargs = {'num_workers': 4, 'pin_memory': True} if use_cuda else {}\n",
    "\n",
    "    \"\"\"\n",
    "    dataset_len = 500 # int(len(dataset) * 1.0)\n",
    "    train_split = int(dataset_len * 0.9)\n",
    "    test_split  = dataset_len - train_split\n",
    "\n",
    "    dataset = torch.utils.data.Subset(dataset, range(0, dataset_len))\n",
    "    train_dataset, test_dataset = \\\n",
    "        torch.utils.data.random_split(dataset, [train_split, test_split])\n",
    "    \"\"\"\n",
    "    \n",
    "    if train_dataset:\n",
    "        if semg_train:\n",
    "            train_loader = data.DataLoader(dataset=train_dataset,\n",
    "                                        batch_size=hparams['batch_size'],\n",
    "                                        shuffle=True,\n",
    "                                        collate_fn=lambda x: data_processing_preds(x, 'train'),\n",
    "                                        **kwargs)\n",
    "        else:\n",
    "            train_loader = data.DataLoader(dataset=train_dataset,\n",
    "                                        batch_size=hparams['batch_size'],\n",
    "                                        shuffle=True,\n",
    "                                        collate_fn=lambda x: data_processing(x, 'train'),\n",
    "                                        **kwargs)\n",
    "\n",
    "    if semg_eval or semg_train:\n",
    "        test_loader = data.DataLoader(dataset=test_dataset,\n",
    "                                    batch_size=hparams['batch_size'],\n",
    "                                    shuffle=False,\n",
    "                                    collate_fn=lambda x: data_processing_preds(x, 'valid'),\n",
    "                                    **kwargs)\n",
    "    else:\n",
    "        test_loader = data.DataLoader(dataset=test_dataset,\n",
    "                                    batch_size=hparams['batch_size'],\n",
    "                                    shuffle=False,\n",
    "                                    collate_fn=lambda x: data_processing(x, 'valid'),\n",
    "                                    **kwargs)\n",
    "\n",
    "    if train_dataset:                       \n",
    "        print(\"(BATCHES) TRAIN LEN, TEST LEN:\", len(train_loader), len(test_loader))\n",
    "\n",
    "    if model_name == \"ds2\":\n",
    "        model = SpeechRecognitionModel(\n",
    "            hparams['n_cnn_layers'], hparams['n_rnn_layers'], hparams['rnn_dim'],\n",
    "            hparams['n_class'], hparams['n_feats'], hparams['stride'], hparams['dropout']\n",
    "        ).to(device)\n",
    "    else:\n",
    "        model = Conformer(\n",
    "            num_classes=hparams['n_class'],\n",
    "            input_dim=hparams['n_feats'],\n",
    "            encoder_dim=32,\n",
    "            num_encoder_layers=3\n",
    "        ).to(device)\n",
    "\n",
    "    if checkpoint_path:\n",
    "        model.load_state_dict(torch.load(checkpoint_path))\n",
    "    \n",
    "    if quantize:\n",
    "        model = torch.quantization.quantize_dynamic(\n",
    "            model,  # the original model\n",
    "            {torch.nn.GRU, torch.nn.Linear},  # a set of layers to dynamically quantize\n",
    "            dtype=torch.qint8)\n",
    "\n",
    "    # print(\"QUANTIZE:\", quantize)\n",
    "\n",
    "    print(model)\n",
    "    print('Num Model Parameters', sum([param.nelement() for param in model.parameters()]))\n",
    "\n",
    "    if beam:\n",
    "        tokens_path  = \"beam_decoder/tokens.txt\"\n",
    "        lm_weight    = 3.23 # 6.0 # 3.23 # 6.00 # 3.23\n",
    "        word_score   = -0.26 # -0.26 # -0.26\n",
    "        lexicon_path = \"beam_decoder/lexicon.txt\"\n",
    "        use_lm = True\n",
    "        beam_size    = 50\n",
    "        nbest        = 3\n",
    "\n",
    "        # beam params\n",
    "        run[\"beam_size\"] = beam_size\n",
    "        run[\"beam_use_lm\"] = use_lm\n",
    "        run[\"beam_lm_weight\"] = lm_weight\n",
    "        run[\"beam_word_score\"] = word_score\n",
    "        run[\"beam_nbest\"] = nbest\n",
    "\n",
    "        lm = files.lm if use_lm else None\n",
    "\n",
    "        beam_search_decoder = ctc_decoder(\n",
    "            lexicon=lexicon_path,\n",
    "            tokens=tokens_path,\n",
    "            lm=lm,\n",
    "            nbest=nbest,\n",
    "            beam_size=beam_size,\n",
    "            lm_weight=lm_weight,\n",
    "            word_score=word_score,\n",
    "            sil_token=\"<unk>\")\n",
    "    else:\n",
    "        beam_search_decoder = None\n",
    "\n",
    "    optimizer = optim.AdamW(model.parameters(), hparams['learning_rate'])\n",
    "    criterion = nn.CTCLoss(blank=28).to(device)\n",
    "    # criterion = nn.CTCLoss(blank=0).to(device)\n",
    "    if train_dataset:\n",
    "        scheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr=hparams['learning_rate'], \n",
    "                                                steps_per_epoch=int(len(train_loader)),\n",
    "                                                epochs=hparams['epochs'],\n",
    "                                                anneal_strategy='linear')\n",
    "    # scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', 0.5, patience=5)\n",
    "\n",
    "    best_test_loss = float(\"inf\")\n",
    "    best_avg_wer = float(\"inf\")\n",
    "\n",
    "    iter_meter = IterMeter()\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        if train_dataset:\n",
    "            train(model, device, train_loader, criterion, optimizer, scheduler, epoch, iter_meter, run)\n",
    "        test_loss, avg_wer = test(model, device, test_loader, criterion, run, beam, beam_search_decoder)\n",
    "\n",
    "        if train_dataset:\n",
    "            # if test_loss < best_test_loss:\n",
    "            if avg_wer < best_avg_wer:\n",
    "                torch.save(model.state_dict(), f\"./models/{model_name}_DATASET_{CUR_DATASET}_EPOCHS_{epoch}_TEST_LOSS_{test_loss}_WER_{avg_wer}\")\n",
    "                best_avg_wer = avg_wer\n",
    "    \n",
    "    return best_avg_wer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPU Runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri May  6 17:54:17 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 510.47.03    Driver Version: 510.47.03    CUDA Version: 11.6     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ...  On   | 00000000:2B:00.0  On |                  N/A |\n",
      "|  0%   43C    P8    28W / 200W |    768MiB /  8192MiB |     34%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      1083      G   /usr/lib/xorg/Xorg                 35MiB |\n",
      "|    0   N/A  N/A      1781      G   /usr/lib/xorg/Xorg                378MiB |\n",
      "|    0   N/A  N/A      1912      G   /usr/bin/gnome-shell               43MiB |\n",
      "|    0   N/A  N/A      2380      G   ...377297799085369794,131072      171MiB |\n",
      "|    0   N/A  N/A      5188      G   ...RendererForSitePerProcess      118MiB |\n",
      "|    0   N/A  N/A     69312      G   ...AAAAAAAAA= --shared-files        7MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def go(device=None,\n",
    "       quantize=False,\n",
    "       semg_eval=False,\n",
    "       semg_train=False,\n",
    "       silent_only=False,\n",
    "       voiced_only=False,\n",
    "       epochs=200,\n",
    "       checkpoint_path=\"\",\n",
    "       beam=False):\n",
    "    from dotenv import dotenv_values\n",
    "    import neptune.new as neptune\n",
    "    config = dotenv_values(\".env\")\n",
    "\n",
    "    learning_rate = 5e-4\n",
    "    # learning_rate = 5e-5\n",
    "    batch_size = 4 # 20\n",
    "    epochs = epochs # 200 # 50 # 10\n",
    "    dataset_path = \"/mnt/datasets/ljspeech/\"\n",
    "    # checkpoint_path = \"/home/joe/projects/asr/models/silent_speech_asr_5_pc/1/ds2_DATASET_SILENT_SPEECH_EPOCHS_255_TEST_LOSS_-0.4287225107351939\"\n",
    "    checkpoint_path = checkpoint_path\n",
    "\n",
    "    neptune_project = config[\"NEPTUNE_PROJECT\"]\n",
    "    neptune_token   = config[\"NEPTUNE_TOKEN\"]\n",
    "\n",
    "    run = neptune.init(project=neptune_project,\n",
    "                    api_token=neptune_token)\n",
    "    \n",
    "    # run = None\n",
    "\n",
    "    if run:\n",
    "        run[\"dataset\"] = CUR_DATASET\n",
    "        run[\"checkpoint_path\"] = checkpoint_path\n",
    "        run[\"device\"] = device\n",
    "        run[\"beam\"] = beam\n",
    "        \n",
    "    final_wer = main(\n",
    "        dataset_path,\n",
    "        learning_rate,\n",
    "        batch_size,\n",
    "        epochs,\n",
    "        checkpoint_path,\n",
    "        run,\n",
    "        device,\n",
    "        quantize,\n",
    "        semg_eval,\n",
    "        semg_train,\n",
    "        silent_only,\n",
    "        voiced_only,\n",
    "        beam)\n",
    "        \n",
    "    if run:\n",
    "        run.stop()\n",
    "    return final_wer\n",
    "\n",
    "# go(device=\"cuda\", quantize=False)\n",
    "\n",
    "#final_wer = go()\n",
    "#print(final_wer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train on Ground Truth Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://app.neptune.ai/miscellaneousstuff/asr-initial-experiments/e/AS-369\n",
      "Remember to stop your run once youâ€™ve finished logging your metadata (https://docs.neptune.ai/api-reference/run#.stop). It will be stopped automatically only when the notebook kernel/interactive console is terminated.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: string series 'monitoring/stdout' value was longer than 1000 characters and was truncated. This warning is printed only once per series.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hparams: {'n_cnn_layers': 3, 'n_rnn_layers': 5, 'rnn_dim': 512, 'n_class': 39, 'n_feats': 128, 'stride': 2, 'dropout': 0.1, 'learning_rate': 0.0005, 'batch_size': 5, 'epochs': 100}\n",
      "device: cuda\n",
      "LIST OF FILES: ['/mnt/datasets/semg_silent_speech/emg_data/voiced_parallel_data/5-9/255_audio_clean.flac', 'Then suddenly the white flashes of the Heat-Ray came leaping towards me.', 'test', 'voiced']\n",
      "LIST OF FILES: ['/mnt/datasets/semg_silent_speech/emg_data/voiced_parallel_data/5-9/255_audio_clean.flac', 'Then suddenly the white flashes of the Heat-Ray came leaping towards me.', 'test', 'voiced']\n",
      "LEN TRAIN TEST: 8192 198\n",
      "(BATCHES) TRAIN LEN, TEST LEN: 1639 40\n",
      "SpeechRecognitionModel(\n",
      "  (cnn): Conv2d(1, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "  (rescnn_layers): Sequential(\n",
      "    (0): ResidualCNN(\n",
      "      (cnn1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (cnn2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (dropout1): Dropout(p=0.1, inplace=False)\n",
      "      (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      (layer_norm1): CNNLayerNorm(\n",
      "        (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (layer_norm2): CNNLayerNorm(\n",
      "        (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "    (1): ResidualCNN(\n",
      "      (cnn1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (cnn2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (dropout1): Dropout(p=0.1, inplace=False)\n",
      "      (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      (layer_norm1): CNNLayerNorm(\n",
      "        (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (layer_norm2): CNNLayerNorm(\n",
      "        (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "    (2): ResidualCNN(\n",
      "      (cnn1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (cnn2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (dropout1): Dropout(p=0.1, inplace=False)\n",
      "      (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      (layer_norm1): CNNLayerNorm(\n",
      "        (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (layer_norm2): CNNLayerNorm(\n",
      "        (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (fully_connected): Linear(in_features=2048, out_features=512, bias=True)\n",
      "  (birnn_layers): Sequential(\n",
      "    (0): BidirectionalGRU(\n",
      "      (BiGRU): GRU(512, 512, batch_first=True, bidirectional=True)\n",
      "      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): BidirectionalGRU(\n",
      "      (BiGRU): GRU(1024, 512, bidirectional=True)\n",
      "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (2): BidirectionalGRU(\n",
      "      (BiGRU): GRU(1024, 512, bidirectional=True)\n",
      "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (3): BidirectionalGRU(\n",
      "      (BiGRU): GRU(1024, 512, bidirectional=True)\n",
      "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (4): BidirectionalGRU(\n",
      "      (BiGRU): GRU(1024, 512, bidirectional=True)\n",
      "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=512, bias=True)\n",
      "    (1): GELU(approximate=none)\n",
      "    (2): Dropout(p=0.1, inplace=False)\n",
      "    (3): Linear(in_features=512, out_features=39, bias=True)\n",
      "  )\n",
      ")\n",
      "Num Model Parameters 23710503\n",
      "PRE MODEL INPUT SHAPE: torch.Size([5, 1, 128, 1123])\n",
      "POST MODEL INPUT SHAPE: torch.Size([5, 32, 64, 562])\n",
      "VIEWED MODEL INPUT SHAPE: torch.Size([5, 2048, 562])\n",
      "POST-SEQ SHAPE: torch.Size([5, 562, 39])\n",
      "Train Epoch: 1 [0/8192 (0%)]\tLoss: 14.321017\n",
      "PRE MODEL INPUT SHAPE: torch.Size([5, 1, 128, 1623])\n",
      "POST MODEL INPUT SHAPE: torch.Size([5, 32, 64, 812])\n",
      "VIEWED MODEL INPUT SHAPE: torch.Size([5, 2048, 812])\n",
      "POST-SEQ SHAPE: torch.Size([5, 812, 39])\n",
      "PRE MODEL INPUT SHAPE: torch.Size([5, 1, 128, 1104])\n",
      "POST MODEL INPUT SHAPE: torch.Size([5, 32, 64, 552])\n",
      "VIEWED MODEL INPUT SHAPE: torch.Size([5, 2048, 552])\n",
      "POST-SEQ SHAPE: torch.Size([5, 552, 39])\n",
      "PRE MODEL INPUT SHAPE: torch.Size([5, 1, 128, 1170])\n",
      "POST MODEL INPUT SHAPE: torch.Size([5, 32, 64, 585])\n",
      "VIEWED MODEL INPUT SHAPE: torch.Size([5, 2048, 585])\n",
      "POST-SEQ SHAPE: torch.Size([5, 585, 39])\n",
      "PRE MODEL INPUT SHAPE: torch.Size([5, 1, 128, 1606])\n",
      "POST MODEL INPUT SHAPE: torch.Size([5, 32, 64, 803])\n",
      "VIEWED MODEL INPUT SHAPE: torch.Size([5, 2048, 803])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/joe/projects/asr/train.ipynb Cell 20'\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/joe/projects/asr/train.ipynb#ch0000019?line=0'>1</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/joe/projects/asr/train.ipynb#ch0000019?line=1'>2</a>\u001b[0m \u001b[39mcheckpoint_path = \\\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/joe/projects/asr/train.ipynb#ch0000019?line=2'>3</a>\u001b[0m \u001b[39m    \"/home/joe/projects/asr/models/silent_speech_asr_100_pc/1/\"\\\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/joe/projects/asr/train.ipynb#ch0000019?line=3'>4</a>\u001b[0m \u001b[39m    \"ds2_DATASET_SILENT_SPEECH_EPOCHS_23_TEST_LOSS_0.5270409451460275_45_WER\"\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/joe/projects/asr/train.ipynb#ch0000019?line=4'>5</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/joe/projects/asr/train.ipynb#ch0000019?line=6'>7</a>\u001b[0m checkpoint_path \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/joe/projects/asr/train.ipynb#ch0000019?line=8'>9</a>\u001b[0m final_wer \u001b[39m=\u001b[39m go(\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/joe/projects/asr/train.ipynb#ch0000019?line=9'>10</a>\u001b[0m     semg_eval\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/joe/projects/asr/train.ipynb#ch0000019?line=10'>11</a>\u001b[0m     semg_train\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/joe/projects/asr/train.ipynb#ch0000019?line=11'>12</a>\u001b[0m     epochs\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m, \u001b[39m# open vocab parallel := 100, closed vocab := 200\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/joe/projects/asr/train.ipynb#ch0000019?line=12'>13</a>\u001b[0m     checkpoint_path\u001b[39m=\u001b[39;49mcheckpoint_path)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/joe/projects/asr/train.ipynb#ch0000019?line=13'>14</a>\u001b[0m \u001b[39mprint\u001b[39m(final_wer)\n",
      "\u001b[1;32m/home/joe/projects/asr/train.ipynb Cell 18'\u001b[0m in \u001b[0;36mgo\u001b[0;34m(device, quantize, semg_eval, semg_train, silent_only, voiced_only, epochs, checkpoint_path, beam)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/joe/projects/asr/train.ipynb#ch0000017?line=32'>33</a>\u001b[0m     run[\u001b[39m\"\u001b[39m\u001b[39mdevice\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m device\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/joe/projects/asr/train.ipynb#ch0000017?line=33'>34</a>\u001b[0m     run[\u001b[39m\"\u001b[39m\u001b[39mbeam\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m beam\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/joe/projects/asr/train.ipynb#ch0000017?line=35'>36</a>\u001b[0m final_wer \u001b[39m=\u001b[39m main(\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/joe/projects/asr/train.ipynb#ch0000017?line=36'>37</a>\u001b[0m     dataset_path,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/joe/projects/asr/train.ipynb#ch0000017?line=37'>38</a>\u001b[0m     learning_rate,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/joe/projects/asr/train.ipynb#ch0000017?line=38'>39</a>\u001b[0m     batch_size,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/joe/projects/asr/train.ipynb#ch0000017?line=39'>40</a>\u001b[0m     epochs,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/joe/projects/asr/train.ipynb#ch0000017?line=40'>41</a>\u001b[0m     checkpoint_path,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/joe/projects/asr/train.ipynb#ch0000017?line=41'>42</a>\u001b[0m     run,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/joe/projects/asr/train.ipynb#ch0000017?line=42'>43</a>\u001b[0m     device,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/joe/projects/asr/train.ipynb#ch0000017?line=43'>44</a>\u001b[0m     quantize,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/joe/projects/asr/train.ipynb#ch0000017?line=44'>45</a>\u001b[0m     semg_eval,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/joe/projects/asr/train.ipynb#ch0000017?line=45'>46</a>\u001b[0m     semg_train,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/joe/projects/asr/train.ipynb#ch0000017?line=46'>47</a>\u001b[0m     silent_only,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/joe/projects/asr/train.ipynb#ch0000017?line=47'>48</a>\u001b[0m     voiced_only,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/joe/projects/asr/train.ipynb#ch0000017?line=48'>49</a>\u001b[0m     beam)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/joe/projects/asr/train.ipynb#ch0000017?line=50'>51</a>\u001b[0m \u001b[39mif\u001b[39;00m run:\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/joe/projects/asr/train.ipynb#ch0000017?line=51'>52</a>\u001b[0m     run\u001b[39m.\u001b[39mstop()\n",
      "\u001b[1;32m/home/joe/projects/asr/train.ipynb Cell 14'\u001b[0m in \u001b[0;36mmain\u001b[0;34m(dataset_path, learning_rate, batch_size, epochs, checkpoint_path, run, device, quantize, semg_eval, semg_train, silent_only, voiced_only, beam)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/joe/projects/asr/train.ipynb#ch0000013?line=379'>380</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, epochs \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m):\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/joe/projects/asr/train.ipynb#ch0000013?line=380'>381</a>\u001b[0m     \u001b[39mif\u001b[39;00m train_dataset:\n\u001b[0;32m--> <a href='vscode-notebook-cell:/home/joe/projects/asr/train.ipynb#ch0000013?line=381'>382</a>\u001b[0m         train(model, device, train_loader, criterion, optimizer, scheduler, epoch, iter_meter, run)\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/joe/projects/asr/train.ipynb#ch0000013?line=382'>383</a>\u001b[0m     test_loss, avg_wer \u001b[39m=\u001b[39m test(model, device, test_loader, criterion, run, beam, beam_search_decoder)\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/joe/projects/asr/train.ipynb#ch0000013?line=384'>385</a>\u001b[0m     \u001b[39mif\u001b[39;00m train_dataset:\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/joe/projects/asr/train.ipynb#ch0000013?line=385'>386</a>\u001b[0m         \u001b[39m# if test_loss < best_test_loss:\u001b[39;00m\n",
      "\u001b[1;32m/home/joe/projects/asr/train.ipynb Cell 14'\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, device, train_loader, criterion, optimizer, scheduler, epoch, iter_meter, run)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/joe/projects/asr/train.ipynb#ch0000013?line=32'>33</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautocast(\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/joe/projects/asr/train.ipynb#ch0000013?line=33'>34</a>\u001b[0m     enabled\u001b[39m=\u001b[39mamp_enabled,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/joe/projects/asr/train.ipynb#ch0000013?line=34'>35</a>\u001b[0m     dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mbfloat16,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/joe/projects/asr/train.ipynb#ch0000013?line=35'>36</a>\u001b[0m     device_type\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/joe/projects/asr/train.ipynb#ch0000013?line=37'>38</a>\u001b[0m     \u001b[39mif\u001b[39;00m model_name \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mds2\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/joe/projects/asr/train.ipynb#ch0000013?line=38'>39</a>\u001b[0m         output \u001b[39m=\u001b[39m model(spectrograms)  \u001b[39m# (batch, time, n_class)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/joe/projects/asr/train.ipynb#ch0000013?line=39'>40</a>\u001b[0m         output \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mlog_softmax(output, dim\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/joe/projects/asr/train.ipynb#ch0000013?line=40'>41</a>\u001b[0m         output \u001b[39m=\u001b[39m output\u001b[39m.\u001b[39mtranspose(\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m) \u001b[39m# (time, batch, n_class)\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1129\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/joe/.local/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1124'>1125</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/joe/.local/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1125'>1126</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/joe/.local/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1126'>1127</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///home/joe/.local/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1127'>1128</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///home/joe/.local/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1128'>1129</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///home/joe/.local/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1129'>1130</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/joe/.local/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1130'>1131</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/home/joe/projects/asr/train.ipynb Cell 12'\u001b[0m in \u001b[0;36mSpeechRecognitionModel.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/joe/projects/asr/train.ipynb#ch0000011?line=88'>89</a>\u001b[0m x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mtranspose(\u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m) \u001b[39m# (batch, time, feature)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/joe/projects/asr/train.ipynb#ch0000011?line=89'>90</a>\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfully_connected(x)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/joe/projects/asr/train.ipynb#ch0000011?line=90'>91</a>\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbirnn_layers(x)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/joe/projects/asr/train.ipynb#ch0000011?line=91'>92</a>\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclassifier(x)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/joe/projects/asr/train.ipynb#ch0000011?line=92'>93</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mPOST-SEQ SHAPE:\u001b[39m\u001b[39m\"\u001b[39m, x\u001b[39m.\u001b[39mshape)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1129\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/joe/.local/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1124'>1125</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/joe/.local/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1125'>1126</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/joe/.local/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1126'>1127</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///home/joe/.local/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1127'>1128</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///home/joe/.local/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1128'>1129</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///home/joe/.local/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1129'>1130</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/joe/.local/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1130'>1131</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/container.py:139\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    <a href='file:///home/joe/.local/lib/python3.8/site-packages/torch/nn/modules/container.py?line=136'>137</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    <a href='file:///home/joe/.local/lib/python3.8/site-packages/torch/nn/modules/container.py?line=137'>138</a>\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> <a href='file:///home/joe/.local/lib/python3.8/site-packages/torch/nn/modules/container.py?line=138'>139</a>\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    <a href='file:///home/joe/.local/lib/python3.8/site-packages/torch/nn/modules/container.py?line=139'>140</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1129\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/joe/.local/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1124'>1125</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/joe/.local/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1125'>1126</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/joe/.local/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1126'>1127</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///home/joe/.local/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1127'>1128</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///home/joe/.local/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1128'>1129</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///home/joe/.local/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1129'>1130</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/joe/.local/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1130'>1131</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/home/joe/projects/asr/train.ipynb Cell 12'\u001b[0m in \u001b[0;36mBidirectionalGRU.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/joe/projects/asr/train.ipynb#ch0000011?line=50'>51</a>\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer_norm(x)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/joe/projects/asr/train.ipynb#ch0000011?line=51'>52</a>\u001b[0m x \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mgelu(x)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/joe/projects/asr/train.ipynb#ch0000011?line=52'>53</a>\u001b[0m x, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mBiGRU(x)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/joe/projects/asr/train.ipynb#ch0000011?line=53'>54</a>\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(x)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/joe/projects/asr/train.ipynb#ch0000011?line=54'>55</a>\u001b[0m \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1129\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/joe/.local/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1124'>1125</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/joe/.local/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1125'>1126</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/joe/.local/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1126'>1127</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///home/joe/.local/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1127'>1128</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///home/joe/.local/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1128'>1129</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///home/joe/.local/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1129'>1130</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/joe/.local/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1130'>1131</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:950\u001b[0m, in \u001b[0;36mGRU.forward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    <a href='file:///home/joe/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py?line=947'>948</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcheck_forward_args(\u001b[39minput\u001b[39m, hx, batch_sizes)\n\u001b[1;32m    <a href='file:///home/joe/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py?line=948'>949</a>\u001b[0m \u001b[39mif\u001b[39;00m batch_sizes \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///home/joe/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py?line=949'>950</a>\u001b[0m     result \u001b[39m=\u001b[39m _VF\u001b[39m.\u001b[39;49mgru(\u001b[39minput\u001b[39;49m, hx, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_flat_weights, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_layers,\n\u001b[1;32m    <a href='file:///home/joe/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py?line=950'>951</a>\u001b[0m                      \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdropout, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbidirectional, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbatch_first)\n\u001b[1;32m    <a href='file:///home/joe/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py?line=951'>952</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    <a href='file:///home/joe/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py?line=952'>953</a>\u001b[0m     result \u001b[39m=\u001b[39m _VF\u001b[39m.\u001b[39mgru(\u001b[39minput\u001b[39m, batch_sizes, hx, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_flat_weights, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias,\n\u001b[1;32m    <a href='file:///home/joe/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py?line=953'>954</a>\u001b[0m                      \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_layers, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbidirectional)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "checkpoint_path = \\\n",
    "    \"/home/joe/projects/asr/models/silent_speech_asr_100_pc/1/\"\\\n",
    "    \"ds2_DATASET_SILENT_SPEECH_EPOCHS_23_TEST_LOSS_0.5270409451460275_45_WER\"\n",
    "\"\"\"\n",
    "\n",
    "checkpoint_path = \"\"\n",
    "\n",
    "final_wer = go(\n",
    "    semg_eval=False,\n",
    "    semg_train=False,\n",
    "    epochs=100, # open vocab parallel := 100, closed vocab := 200\n",
    "    checkpoint_path=checkpoint_path)\n",
    "print(final_wer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train on Transduction Silent Speech Predictions (E<sub>s</sub> Preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "checkpoint_path = \\\n",
    "    \"/home/joe/projects/asr/models/silent_speech_open_full/silent_speech_open_parallel(silent_and_vocal_preds_and_nonparallel_and_ground)/\"\\\n",
    "    \"ds2_DATASET_SILENT_SPEECH_EPOCHS_10_TEST_LOSS_1.8498832106590273_WER_0.6825681123095443\"\n",
    "checkpoint_path = \"\"\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "checkpoint_path = \\\n",
    "    \"/home/joe/projects/asr/models/silent_speech_closed_vocab/silent_speech_closed_vocab(no_model_preds)/\"\\\n",
    "    \"CLOSED_VOCAB_GROUND_155_EPOCHS_37_WER\"\n",
    "\"\"\"\n",
    "\n",
    "checkpoint_path = \"\"\n",
    "\n",
    "\"\"\"\n",
    "checkpoint_path = \\\n",
    "    \"/home/joe/projects/asr/models/silent_speech_asr_100_pc/1/\"\\\n",
    "    \"ds2_DATASET_SILENT_SPEECH_EPOCHS_23_TEST_LOSS_0.5270409451460275_45_WER\"\n",
    "\"\"\"\n",
    "\n",
    "final_wer = go(\n",
    "    semg_eval=False,\n",
    "    semg_train=True,\n",
    "    silent_only=False,\n",
    "    voiced_only=True,\n",
    "    beam=False,\n",
    "    epochs=100, # open vocab parallel := 100, closed vocab := 200\n",
    "    checkpoint_path=checkpoint_path)\n",
    "    \n",
    "print(final_wer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eval on Silent Speech Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/joe/projects/asr/models/silent_speech_open_full/silent_speech_open_parallel(silent_and_vocal_preds_and_nonparallel_and_ground)/ds2_DATASET_SILENT_SPEECH_EPOCHS_10_TEST_LOSS_1.8498832106590273_WER_0.6825681123095443\n",
      "https://app.neptune.ai/miscellaneousstuff/asr-initial-experiments/e/AS-375\n",
      "Remember to stop your run once youâ€™ve finished logging your metadata (https://docs.neptune.ai/api-reference/run#.stop). It will be stopped automatically only when the notebook kernel/interactive console is terminated.\n",
      "hparams: {'n_cnn_layers': 3, 'n_rnn_layers': 5, 'rnn_dim': 512, 'n_class': 29, 'n_feats': 128, 'stride': 2, 'dropout': 0.1, 'learning_rate': 0.0005, 'batch_size': 5, 'epochs': 1}\n",
      "device: cuda\n",
      "LIST OF FILES: ['/mnt/datasets/semg_silent_speech/emg_data/voiced_parallel_data/5-9/255_audio_clean.flac', 'Then suddenly the white flashes of the Heat-Ray came leaping towards me.', 'test', 'voiced']\n",
      "(1) LIST OF FILES: 99\n",
      "(2) LIST OF FILES: ['/home/joe/projects/silent_speech/pred_audio/open_vocab_parallel/silent/616', 'They seemed very helpless in that pit of theirs.', 'test', 'silent']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: string series 'monitoring/stdout' value was longer than 1000 characters and was truncated. This warning is printed only once per series.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SpeechRecognitionModel(\n",
      "  (cnn): Conv2d(1, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "  (rescnn_layers): Sequential(\n",
      "    (0): ResidualCNN(\n",
      "      (cnn1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (cnn2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (dropout1): Dropout(p=0.1, inplace=False)\n",
      "      (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      (layer_norm1): CNNLayerNorm(\n",
      "        (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (layer_norm2): CNNLayerNorm(\n",
      "        (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "    (1): ResidualCNN(\n",
      "      (cnn1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (cnn2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (dropout1): Dropout(p=0.1, inplace=False)\n",
      "      (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      (layer_norm1): CNNLayerNorm(\n",
      "        (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (layer_norm2): CNNLayerNorm(\n",
      "        (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "    (2): ResidualCNN(\n",
      "      (cnn1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (cnn2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (dropout1): Dropout(p=0.1, inplace=False)\n",
      "      (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      (layer_norm1): CNNLayerNorm(\n",
      "        (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (layer_norm2): CNNLayerNorm(\n",
      "        (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (fully_connected): Linear(in_features=2048, out_features=512, bias=True)\n",
      "  (birnn_layers): Sequential(\n",
      "    (0): BidirectionalGRU(\n",
      "      (BiGRU): GRU(512, 512, batch_first=True, bidirectional=True)\n",
      "      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): BidirectionalGRU(\n",
      "      (BiGRU): GRU(1024, 512, bidirectional=True)\n",
      "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (2): BidirectionalGRU(\n",
      "      (BiGRU): GRU(1024, 512, bidirectional=True)\n",
      "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (3): BidirectionalGRU(\n",
      "      (BiGRU): GRU(1024, 512, bidirectional=True)\n",
      "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (4): BidirectionalGRU(\n",
      "      (BiGRU): GRU(1024, 512, bidirectional=True)\n",
      "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=512, bias=True)\n",
      "    (1): GELU(approximate=none)\n",
      "    (2): Dropout(p=0.1, inplace=False)\n",
      "    (3): Linear(in_features=512, out_features=29, bias=True)\n",
      "  )\n",
      ")\n",
      "Num Model Parameters 23705373\n",
      "\n",
      "evaluating...\n",
      "SPECTROGRAM SHAPE: torch.Size([5, 1, 128, 1695])\n",
      "greedy output shape: torch.Size([5, 848, 29]) torch.Size([5, 848])\n",
      "greedy TEST 0 ['they samed feling uppthiss in that pid of thers<unk><unk>', '<unk>what dot news from theiy compo<unk><unk> said i<unk>', 'and tibesisu from the strangensessies of is the s  from myself and the world opout me<unk> set im wangens all fromy outsige from somebmere is to seem blyy reboate<unk> outs of the hime out ofspace out of the stress in drasshy of at ar<unk>'] ['they seemed very helpless in that pit of theirs<unk>', '<unk>what news from the common<unk><unk> said i<unk>', 'at times i suffer from the strangest sense of detachment from myself and the world about me<unk> i seem to watch it all from the outside<unk> from somewhere inconceivably remote<unk> out of time<unk> out of space<unk> out of the stress and tragedy of it all<unk>']\n",
      "SPECTROGRAM SHAPE: torch.Size([5, 1, 128, 1832])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_687389/1431714449.py:174: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \"\".join(encoder.batch_decode(torch.tensor(cur_target)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "greedy output shape: torch.Size([5, 916, 29]) torch.Size([5, 916])\n",
      "greedy TEST 1 ['the candisst excimemin ofthe ofvints ad toloud leve myme proseveng pas innd stete of andrith s<unk>', 'the tcraein imbick of trocks<unk> the shomb wasilt of thy ingense of the juction<unk> manggond with hir shelfs of er mens nors<unk>ecad then mid came into the sation abeut nigo oc<unk>clock with a credible sigtings<unk> and yosd doper stirmans the drtrs big tae dun<unk>', 'he tone<unk> stared<unk> palled something about<unk> crawling out in tae think lckid dish comber<unk> and ran on to the gade of the house at the grest<unk>'] ['the intense excitement of the events had no doubt left my perceptive powers in a state of erethism<unk>', 'the ringing impact of trucks<unk> the sharp whistle of the engines from the junction<unk> mingled with their shouts of <unk>men from mars<unk><unk> excited men came into the station about nine o<unk>clock with incredible tidings<unk> and caused no more disturbance than drunkards might have done<unk>', 'he turned<unk> stared<unk> bawled something about <unk>crawling out in a thing like a dish cover<unk><unk> and ran on to the gate of the house at the crest<unk>']\n",
      "SPECTROGRAM SHAPE: torch.Size([5, 1, 128, 1272])\n",
      "greedy output shape: torch.Size([5, 636, 29]) torch.Size([5, 636])\n",
      "greedy TEST 2 ['hat the einovi i sat<unk> dom bring nunce with a singret<unk> aregodning no gillolies rashinus<unk> and enonting the chord sient simited of the martians<unk>', '<unk>they popte ofoliseltley<unk><unk> said i<unk> frgriang byn wike lass<unk>', 'i did not now it by th at was thae last sevel idinern i was feat foravo maen strage and telripolae<unk>'] ['at the end of it i sat<unk> tempering nuts with a cigarette<unk> regretting ogilvy<unk>s rashness<unk> and denouncing the short-sighted timidity of the martians<unk>', '<unk>they have done a foolish thing<unk><unk> said i<unk> fingering my wineglass<unk>', 'i did not know it<unk> but that was the last civilised dinner i was to eat for very many strange and terrible days<unk>']\n",
      "SPECTROGRAM SHAPE: torch.Size([5, 1, 128, 1546])\n",
      "greedy output shape: torch.Size([5, 773, 29]) torch.Size([5, 773])\n",
      "greedy TEST 3 ['it was alls orieal and tiron the milla<unk><unk>', 'all ine long the martians were amvering and stiring<unk> slemed ess it nefiingble work ubon the pichin they weremakg ready<unk> and aver and agin aboh of gringh righte smoke wrled up to the sardling d sky<unk>', '<unk>you  heare morn gun<unk><unk>i said and wen ons my home<unk>'] ['it was all so real and so familiar<unk>', 'all night long the martians were hammering and stirring<unk> sleepless<unk> indefatigable<unk> at work upon the machines they were making ready<unk> and ever and again a puff of greenish-white smoke whirled up to the starlit sky<unk>', '<unk>you<unk>ll hear more yet<unk><unk> i said<unk> and went on to my home<unk>']\n",
      "SPECTROGRAM SHAPE: torch.Size([5, 1, 128, 871])\n",
      "greedy output shape: torch.Size([5, 436, 29]) torch.Size([5, 436])\n",
      "greedy TEST 4 ['lay ther daingrensmeg uss n no out<unk> they are bad with hergrar<unk><unk><unk>', 'be ond was a frencei of excideent<unk> and fadered that that fridge the anfulimation<unk> and not acraped askget<unk>', 'ind tardly seemedto veir fite to me ind that heyng<unk>'] ['<unk>they are dangerous because<unk> no doubt<unk> they are mad with terror<unk>', 'beyond was a fringe of excitement<unk> and farther than that fringe the inflammation had not crept as yet<unk>', 'it hardly seemed a fair fight to me at that time<unk>']\n",
      "SPECTROGRAM SHAPE: torch.Size([5, 1, 128, 1368])\n",
      "greedy output shape: torch.Size([5, 684, 29]) torch.Size([5, 684])\n",
      "greedy TEST 5 ['he<unk>s not andin sruend sationd<unk>', 'the noor behind o open<unk> andthe manhons oacross the landing cme men<unk> restonly i churt drousers and slippers<unk> as brics lose and dis wast<unk> his haired desordered from his midlow<unk>', 'afitriful paskers and the the dims of vady<unk>'] ['he is not an insurance agent<unk><unk>', 'then the door behind him opened<unk> and the man who lodged across the landing came in<unk> dressed only in shirt<unk> trousers<unk> and slippers<unk> his braces loose about his waist<unk> his hair disordered from his pillow<unk>', 'fearful massacres in the thames valley<unk><unk>']\n",
      "SPECTROGRAM SHAPE: torch.Size([5, 1, 128, 1067])\n",
      "greedy output shape: torch.Size([5, 534, 29]) torch.Size([5, 534])\n",
      "greedy TEST 6 ['probably this denton do the rillend strinkcts of the artsht crovititonal inderchy<unk> on that lastitext<unk> the lader rigter expended veryg caverdinlyy<unk>', 'the martians had bedr remuls<unk> that were not n defordermble<unk>', 'he read an re gread the papere<unk> fern the wors and hopene am mey<unk>'] ['probably this is due to the relative strength of the earth<unk>s gravitational energy<unk><unk> on that last text their leader-writer expanded very comfortingly<unk>', 'the martians had been repulsed<unk> they were not invulnerable<unk>', 'he read and re-read the paper<unk> fearing the worst had happened to me<unk>']\n",
      "SPECTROGRAM SHAPE: torch.Size([5, 1, 128, 1849])\n",
      "greedy output shape: torch.Size([5, 925, 29]) torch.Size([5, 925])\n",
      "greedy TEST 7 ['he heard fot is haps brenk fro it fro in the roades<unk> ad up i ound snars behind him<unk>', 'bentinly an came  pon i stin<unk> and crossickand folled of lbougth dore these ward<unk>', 'peopler fiting saregely ofb standincgroom in the carages evendad fo o<unk>clock<unk>'] ['he heard footsteps running to and fro in the rooms<unk> and up and down stairs behind him<unk>', 'presently he came upon a stile<unk> and<unk> crossing it<unk> followed a footpath northeastward<unk>', 'people were fighting savagely for standing-room in the carriages even at two o<unk>clock<unk>']\n",
      "SPECTROGRAM SHAPE: torch.Size([5, 1, 128, 972])\n",
      "greedy output shape: torch.Size([5, 486, 29]) torch.Size([5, 486])\n",
      "greedy TEST 8 ['suddenly he fanished<unk> and i coundeadve fanceed of faint shrink ad reats to me<unk>', 'and inrprised sweechd to hewar and the shmbabload<unk> ad then up sa with a bellse of green opls and chender benher<unk>', 'bes ide s tad<unk> there was quite agim of my scicles<unk>'] ['suddenly he vanished<unk> and i could have fancied a faint shriek had reached me<unk>', 'an enterprising sweet-stuff dealer in the chobham road had sent up his son with a barrow-load of green apples and ginger beer<unk>', 'besides that<unk> there was quite a heap of bicycles<unk>']\n",
      "SPECTROGRAM SHAPE: torch.Size([5, 1, 128, 1356])\n",
      "greedy output shape: torch.Size([5, 678, 29]) torch.Size([5, 678])\n",
      "greedy TEST 9 ['iv<unk>', 'i had bln<unk> he pe been i sad finks upon the stilinderstib for whoich utheir rendescoods wiw cown projrecting<unk> had benging ing pushing my wray bag from the edge of the pit<unk>', 'there was some foudthh undof the eyes<unk> the limbeds brim of whitcs quivered and mated<unk> and troped sil lifef'] ['iv<unk>', 'i half turned<unk> keeping my eyes fixed upon the cylinder still<unk> from which other tentacles were now projecting<unk> and began pushing my way back from the edge of the pit<unk>', 'there was a mouth under the eyes<unk> the lipless brim of which quivered and panted<unk> and dropped saliva<unk>']\n",
      "SPECTROGRAM SHAPE: torch.Size([5, 1, 128, 653])\n",
      "greedy output shape: torch.Size([5, 327, 29]) torch.Size([5, 327])\n",
      "greedy TEST 10 ['i saw u stonishment kaving macsed order on the faces of the people abou me<unk>', 'i do on my side to begaent to bovet were s themit<unk>', 'i loned again on the cylinder<unk> and uncevababled harrmld crip to me<unk>'] ['i saw astonishment giving place to horror on the faces of the people about me<unk>', 'i<unk> too<unk> on my side began to move towards the pit<unk>', 'i looked again at the cylinder<unk> and ungovernable terror gripped me<unk>']\n",
      "SPECTROGRAM SHAPE: torch.Size([5, 1, 128, 1193])\n",
      "greedy output shape: torch.Size([5, 597, 29]) torch.Size([5, 597])\n",
      "greedy TEST 11 ['i think them bereceive that no the cowossument i wrer the brisent e fon awaad breakfast and piders instelves<unk>', 'the bokgother had wmean ye ceenemece<unk> but a lider crowd powards ndeje was blacking scarsed wiye yusee<unk> and thes splillo m gning of a fuicall streepers of spoke<unk>', 'i kno inenad<unk>'] ['i think they perceived that nothing was to be done for the present<unk> and had gone away to breakfast at henderson<unk>s house<unk>', 'the burning heather had been extinguished<unk> but the level ground towards ottershaw was blackened as far as one could see<unk> and still giving off vertical streamers of smoke<unk>', 'i know i did<unk>']\n",
      "SPECTROGRAM SHAPE: torch.Size([5, 1, 128, 1736])\n",
      "greedy output shape: torch.Size([5, 868, 29]) torch.Size([5, 868])\n",
      "greedy TEST 12 ['how emember ivulted te excordnarim rern swation that i would man blaine weth that musintly wun to ie upon to the very firs of sivevd<unk> thie smizurt death i sqwiped as a pascied of light<unk> whel lene havto be from the pit afbo the cylin<unk> and traing meun<unk>', 'togetn un der wader<unk>', 'i coned of starten at once<unk> by mi vaket had been dang findirvous anddee in rom better the dat<unk>'] ['i remember i felt an extraordinary persuasion that i was being played with<unk> that presently<unk> when i was upon the very verge of safety<unk> this mysterious death--as swift as the passage of light--would leap after me from the pit about the cylinder<unk> and strike me down<unk>', 'to get under water<unk>', 'i should have started at once<unk> but my companion had been in active service and he knew better than that<unk>']\n",
      "SPECTROGRAM SHAPE: torch.Size([5, 1, 128, 649])\n",
      "greedy output shape: torch.Size([5, 325, 29]) torch.Size([5, 325])\n",
      "greedy TEST 13 ['the ar tileryuman gumee down the bank and the road and stlooen<unk>', 'that was dede<unk>', 'then suddenly the white flashes of the headray came at leaping towaeasman<unk>'] ['the artilleryman jumped down the bank into the road and saluted<unk>', 'that was it<unk>', 'then suddenly the white flashes of the heat-ray came leaping towards me<unk>']\n",
      "SPECTROGRAM SHAPE: torch.Size([5, 1, 128, 2014])\n",
      "greedy output shape: torch.Size([5, 1007, 29]) torch.Size([5, 1007])\n",
      "greedy TEST 14 ['eveing howv ithn be bott incfanst dueivitely cros the way<unk> and them vadods toward chertsey<unk> meneverything over there was dilil<unk><unk>', 'myple camea vanday<unk> along hunder of re poatedns<unk> one huas bey and wife were eaemay caring a small own outse towar mbetwent him we shimeor their hows to coos vylld ther run<unk>', 'be re far etwey on the rangeh of the heatrayd there<unk> andteaded ad men tor the siees desserison of sume of the houses<unk>the stering boben of backing inothers<unk> and the got of solders standing in the bridge over to ralway and staring town in the line a towards woking<unk> linty would hove seemed very like andy other sundyg<unk>'] ['every now and then people would glance nervously across the wey<unk> at the meadows towards chertsey<unk> but everything over there was still<unk>', 'people came panting along under heavy burdens<unk> one husband and wife were even carrying a small outhouse door between them<unk> with some of their household goods piled thereon<unk>', 'we were far beyond the range of the heat-ray there<unk> and had it not been for the silent desertion of some of the houses<unk> the stirring movement of packing in others<unk> and the knot of soldiers standing on the bridge over the railway and staring down the line towards woking<unk> the day would have seemed very like any other sunday<unk>']\n",
      "SPECTROGRAM SHAPE: torch.Size([5, 1, 128, 1055])\n",
      "greedy output shape: torch.Size([5, 528, 29]) torch.Size([5, 528])\n",
      "greedy TEST 15 ['by brn faedistanson we purged fror the bin trees<unk> and found the contry gome and mayful undtor the borning sundlight<unk>', 'for my knoon mpeart<unk> i had bent febershly exceted all ha<unk><unk>', 'i fulded ton in the reas<unk><unk>'] ['by byfleet station we emerged from the pine trees<unk> and found the country calm and peaceful under the morning sunlight<unk>', 'for my own part<unk> i had been feverishly excited all day<unk>', 'i felt a tug at the reins<unk>']\n",
      "SPECTROGRAM SHAPE: torch.Size([5, 1, 128, 1606])\n",
      "greedy output shape: torch.Size([5, 803, 29]) torch.Size([5, 803])\n",
      "greedy TEST 16 ['thorse look the bid but wich distim huand fults<unk>', 'there are stordim bhecame chactulot way<unk><unk>', 'heherned the bactsims rad of for a time and then becupsed<unk>'] ['the horse took the bit between his teeth and bolted<unk>', 'there his story became ejaculatory<unk>', 'he heard the maxims rattle for a time and then become still<unk>']\n",
      "SPECTROGRAM SHAPE: torch.Size([5, 1, 128, 435])\n",
      "greedy output shape: torch.Size([5, 218, 29]) torch.Size([5, 218])\n",
      "greedy TEST 17 ['i sondoved him<unk> wraiting for the nexfpl<unk><unk><unk><unk><unk><unk><unk>', '<unk>where are you going<unk><unk> i ast<unk>', 'the place was impassedabel<unk><unk>'] ['i stood over him waiting for the next flash<unk>', '<unk>where are you going<unk><unk> i asked<unk>', 'the place was impassable<unk>']\n",
      "SPECTROGRAM SHAPE: torch.Size([5, 1, 128, 935])\n",
      "greedy output shape: torch.Size([5, 468, 29]) torch.Size([5, 468])\n",
      "greedy TEST 18 ['it wre seem theire himber of men were hendbls and d rushed cross the bon<unk><unk>', 'iteors in he gaed heres a to that jorning starw es night<unk><unk>', 'but it some they bare than metrined<unk><unk>'] ['it would seem that a number of men or animals had rushed across the lawn<unk>', '<unk>henderson<unk><unk> he called<unk> <unk>you saw that shooting star last night<unk><unk>', '<unk>but it<unk>s something more than a meteorite<unk>']\n",
      "SPECTROGRAM SHAPE: torch.Size([4, 1, 128, 1086])\n",
      "greedy output shape: torch.Size([4, 543, 29]) torch.Size([4, 543])\n",
      "greedy TEST 19 ['he fanceiyd eve tan that the poly of the modie may ti do rodnes<unk> at one desturmbd that i gi was ae fact than the ass was fally only from the in on the cydlinder<unk>', 'prommin he scarcuriarayse whate his men an all thougre the het was ecxctissies he amor toown to the big closed the bok to seen thehthey bercleatly<unk><unk>', 'evete then khee scarsely understich with his inicated<unk> unto the  hears of pople tdrading soud and saw the blak mok te rkford andinture so<unk>'] ['he fancied even then that the cooling of the body might account for this<unk> but what disturbed that idea was the fact that the ash was falling only from the end of the cylinder<unk>', 'for a minute he scarcely realised what this meant<unk> and<unk> although the heat was excessive<unk> he clambered down into the pit close to the bulk to see the thing more clearly<unk>', 'even then he scarcely understood what this indicated<unk> until he heard a muffled grating sound and saw the black mark jerk forward an inch or so<unk>']\n",
      "Test set: Average loss: 1.8499, Average CER: 0.324832 Average WER: 0.6826\n",
      "\n",
      "Shutting down background jobs, please wait a moment...\n",
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Waiting for the remaining 386 operations to synchronize with Neptune. Do not kill this process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All 386 operations synced, thanks for waiting!\n",
      "inf\n"
     ]
    }
   ],
   "source": [
    "checkpoint_path = \\\n",
    "    \"/home/joe/projects/asr/models/silent_speech_open_full/silent_speech_open_parallel(silent_and_vocal_preds_and_nonparallel_and_ground)/\"\\\n",
    "    \"ds2_DATASET_SILENT_SPEECH_EPOCHS_10_TEST_LOSS_1.8498832106590273_WER_0.6825681123095443\"\n",
    "\n",
    "\"\"\"\n",
    "checkpoint_path = \\\n",
    "    \"/home/joe/projects/asr/models/silent_speech_asr_100_pc/1/\"\\\n",
    "    \"ds2_DATASET_SILENT_SPEECH_EPOCHS_23_TEST_LOSS_0.5270409451460275_45_WER\"\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "checkpoint_path = \\\n",
    "    \"/home/joe/projects/asr/models/silent_speech_closed_vocab/silent_speech_closed_vocab(silent_and_vocal_preds_and_ground)/\"\\\n",
    "    \"ds2_DATASET_SILENT_SPEECH_EPOCHS_140_TEST_LOSS_3.2858729362487793_WER_70\"\n",
    "\"\"\"\n",
    "\n",
    "print(checkpoint_path)\n",
    "\n",
    "final_wer = go(\n",
    "    semg_eval=True,\n",
    "    epochs=1,\n",
    "    checkpoint_path = checkpoint_path,\n",
    "    beam=False)\n",
    "print(final_wer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Post-Training Quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference Time, Throughput (CPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://app.neptune.ai/miscellaneousstuff/asr-initial-experiments/e/AS-93\n",
      "Remember to stop your run once youâ€™ve finished logging your metadata (https://docs.neptune.ai/api-reference/run#.stop). It will be stopped automatically only when the notebook kernel/interactive console is terminated.\n",
      "hparams: {'n_cnn_layers': 3, 'n_rnn_layers': 5, 'rnn_dim': 512, 'n_class': 39, 'n_feats': 128, 'stride': 2, 'dropout': 0.1, 'learning_rate': 0.0005, 'batch_size': 20, 'epochs': 1}\n",
      "(BATCHES) TRAIN LEN, TEST LEN: 23 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: string series 'monitoring/stdout' value was longer than 1000 characters and was truncated. This warning is printed only once per series.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QUANTIZE: True\n",
      "SpeechRecognitionModel(\n",
      "  (cnn): Conv2d(1, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "  (rescnn_layers): Sequential(\n",
      "    (0): ResidualCNN(\n",
      "      (cnn1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (cnn2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (dropout1): Dropout(p=0.1, inplace=False)\n",
      "      (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      (layer_norm1): CNNLayerNorm(\n",
      "        (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (layer_norm2): CNNLayerNorm(\n",
      "        (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "    (1): ResidualCNN(\n",
      "      (cnn1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (cnn2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (dropout1): Dropout(p=0.1, inplace=False)\n",
      "      (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      (layer_norm1): CNNLayerNorm(\n",
      "        (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (layer_norm2): CNNLayerNorm(\n",
      "        (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "    (2): ResidualCNN(\n",
      "      (cnn1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (cnn2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (dropout1): Dropout(p=0.1, inplace=False)\n",
      "      (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      (layer_norm1): CNNLayerNorm(\n",
      "        (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (layer_norm2): CNNLayerNorm(\n",
      "        (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (fully_connected): DynamicQuantizedLinear(in_features=2048, out_features=512, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "  (birnn_layers): Sequential(\n",
      "    (0): BidirectionalGRU(\n",
      "      (BiGRU): DynamicQuantizedGRU(512, 512, batch_first=True, bidirectional=True)\n",
      "      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): BidirectionalGRU(\n",
      "      (BiGRU): DynamicQuantizedGRU(1024, 512, bidirectional=True)\n",
      "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (2): BidirectionalGRU(\n",
      "      (BiGRU): DynamicQuantizedGRU(1024, 512, bidirectional=True)\n",
      "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (3): BidirectionalGRU(\n",
      "      (BiGRU): DynamicQuantizedGRU(1024, 512, bidirectional=True)\n",
      "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (4): BidirectionalGRU(\n",
      "      (BiGRU): DynamicQuantizedGRU(1024, 512, bidirectional=True)\n",
      "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): DynamicQuantizedLinear(in_features=1024, out_features=512, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "    (1): GELU()\n",
      "    (2): Dropout(p=0.1, inplace=False)\n",
      "    (3): DynamicQuantizedLinear(in_features=512, out_features=39, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "  )\n",
      ")\n",
      "Num Model Parameters 65792\n",
      "\n",
      "evaluating...\n",
      "DUMMY: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_24232/1912479192.py:61: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \"\".join(encoder.batch_decode(torch.tensor(cur_target)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Targets: ['11<unk>50 pm', 'july 31 1999']\n",
      "Preds: ['11<unk>5 pm', 'july 31 199']\n",
      "Targets: ['october 23 2006', 'monday july 09']\n",
      "Preds: ['october 23 26', 'monday july 9']\n",
      "Targets: ['monday february 28', 'march 31 1893']\n",
      "Preds: ['monday february 28', 'march 1 1893']\n",
      "Test set: Average loss: -0.3829, Average CER: 0.081869 Average WER: 0.3567\n",
      "\n",
      "Shutting down background jobs, please wait a moment...\n",
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Waiting for the remaining 13 operations to synchronize with Neptune. Do not kill this process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All 13 operations synced, thanks for waiting!\n"
     ]
    }
   ],
   "source": [
    "final_wer = go(device=\"cpu\", quantize=True)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
