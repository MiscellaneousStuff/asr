{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "\n",
    "from jiwer import wer, cer\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torchaudio\n",
    "\n",
    "from torchnlp.encoders import LabelEncoder\n",
    "\n",
    "import os\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SilentSpeech(torch.utils.data.Dataset):\n",
    "    def __init__(self, metadata_path):\n",
    "        with open(metadata_path) as metadata:\n",
    "            flist = csv.reader(metadata, delimiter=\"|\", quotechar=\"'\", quoting=csv.QUOTE_MINIMAL)\n",
    "            self._flist = list(flist)\n",
    "    \n",
    "    def __getitem__(self, n):\n",
    "        line = self._flist[n]\n",
    "        cur_path, text, = line\n",
    "        waveform, sr = torchaudio.load(cur_path)\n",
    "        return (waveform, sr, text)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._flist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<unk>', ' ', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '-'] 29\n"
     ]
    }
   ],
   "source": [
    "characters = [x for x in \" abcdefghijklmnopqrstuvwxyz-\"]\n",
    "encoder = LabelEncoder(characters)\n",
    "\n",
    "print(encoder.vocab, len(encoder.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joe/.local/lib/python3.8/site-packages/torchaudio/functional/functional.py:507: UserWarning: At least one mel filterbank has all zero values. The value for `n_mels` (128) may be set too high. Or, the value for `n_freqs` (201) may be set too low.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "CUR_DATASET = \"SILENT_SPEECH\"\n",
    "if CUR_DATASET == \"SILENT_SPEECH\":\n",
    "    SR = 16000 # Silent Speech 22_050 # LJSpeech\n",
    "else:\n",
    "    SR = 22_050\n",
    "\n",
    "train_audio_transforms = nn.Sequential(\n",
    "    torchaudio.transforms.MelSpectrogram(sample_rate=SR, n_mels=128),\n",
    "    torchaudio.transforms.FrequencyMasking(freq_mask_param=15),\n",
    "    torchaudio.transforms.TimeMasking(time_mask_param=35)\n",
    ")\n",
    "\n",
    "valid_audio_transforms = torchaudio.transforms.MelSpectrogram()\n",
    "\n",
    "import jiwer\n",
    "transformation = jiwer.Compose(\\\n",
    "        [jiwer.RemovePunctuation(), jiwer.ToLowerCase()])\n",
    "\n",
    "def data_processing(data, data_type=\"train\"):\n",
    "    spectrograms = []\n",
    "    labels = []\n",
    "    input_lengths = []\n",
    "    label_lengths = []\n",
    "\n",
    "    for (waveform, _, utterance) in data:\n",
    "        if data_type == 'train':\n",
    "            spec = train_audio_transforms(waveform).squeeze(0).transpose(0, 1)\n",
    "        elif data_type == \"valid\":\n",
    "            spec = valid_audio_transforms(waveform).squeeze(0).transpose(0, 1)\n",
    "        else:\n",
    "            raise Exception('data_type should be train or valid')\n",
    "        spectrograms.append(spec)\n",
    "\n",
    "        label = transformation(utterance)\n",
    "        label = encoder.batch_encode(utterance.lower())\n",
    "        labels.append(label)\n",
    "        input_lengths.append(spec.shape[0]//2)\n",
    "        label_lengths.append(len(label))\n",
    "\n",
    "    spectrograms = nn.utils.rnn.pad_sequence(spectrograms, batch_first=True).unsqueeze(1).transpose(2, 3)\n",
    "    labels = nn.utils.rnn.pad_sequence(labels, batch_first=True)\n",
    "\n",
    "    return spectrograms, labels, input_lengths, label_lengths\n",
    "\n",
    "def GreedyDecoder(output, labels, label_lengths, blank_label=28, collapse_repeated=True):\n",
    "    arg_maxes = torch.argmax(output, dim=2)\n",
    "    decodes = []\n",
    "    targets = []\n",
    "    for i, args in enumerate(arg_maxes):\n",
    "        decode = []\n",
    "\n",
    "        cur_target = labels[i][:label_lengths[i]]\n",
    "        if len(cur_target) > 0:\n",
    "            cur_target = \\\n",
    "                \"\".join(encoder.batch_decode(torch.tensor(cur_target)))\n",
    "        else:\n",
    "            cur_target = \"\"\n",
    "        targets.append(cur_target)\n",
    "\n",
    "        for j, index in enumerate(args):\n",
    "            if index != blank_label:\n",
    "                if collapse_repeated and j != 0 and index == args[j -1]:\n",
    "                    continue\n",
    "                decode.append(index.item())\n",
    "\n",
    "        cur_decode = decode\n",
    "        if len(cur_decode) > 0:\n",
    "            cur_decode = \\\n",
    "                \"\".join(encoder.batch_decode(torch.tensor(cur_decode)))\n",
    "        else:\n",
    "            cur_decode = \"\"\n",
    "        decodes.append(cur_decode)\n",
    "\n",
    "    return decodes, targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNLayerNorm(nn.Module):\n",
    "    \"\"\"Layer normalization built for cnns input\"\"\"\n",
    "    def __init__(self, n_feats):\n",
    "        super(CNNLayerNorm, self).__init__()\n",
    "        self.layer_norm = nn.LayerNorm(n_feats)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x (batch, channel, feature, time)\n",
    "        x = x.transpose(2, 3).contiguous() # (batch, channel, time, feature)\n",
    "        x = self.layer_norm(x)\n",
    "        return x.transpose(2, 3).contiguous() # (batch, channel, feature, time) \n",
    "\n",
    "class ResidualCNN(nn.Module):\n",
    "    \"\"\"Residual CNN inspired by https://arxiv.org/pdf/1603.05027.pdf\n",
    "        except with layer norm instead of batch norm\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, kernel, stride, dropout, n_feats):\n",
    "        super(ResidualCNN, self).__init__()\n",
    "\n",
    "        self.cnn1 = nn.Conv2d(in_channels, out_channels, kernel, stride, padding=kernel//2)\n",
    "        self.cnn2 = nn.Conv2d(out_channels, out_channels, kernel, stride, padding=kernel//2)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.layer_norm1 = CNNLayerNorm(n_feats)\n",
    "        self.layer_norm2 = CNNLayerNorm(n_feats)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x  # (batch, channel, feature, time)\n",
    "        x = self.layer_norm1(x)\n",
    "        x = F.gelu(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.cnn1(x)\n",
    "        x = self.layer_norm2(x)\n",
    "        x = F.gelu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.cnn2(x)\n",
    "        x += residual\n",
    "        return x # (batch, channel, feature, time)\n",
    "\n",
    "class BidirectionalGRU(nn.Module):\n",
    "    def __init__(self, rnn_dim, hidden_size, dropout, batch_first):\n",
    "        super(BidirectionalGRU, self).__init__()\n",
    "\n",
    "        self.BiGRU = nn.GRU(\n",
    "            input_size=rnn_dim, hidden_size=hidden_size,\n",
    "            num_layers=1, batch_first=batch_first, bidirectional=True)\n",
    "        self.layer_norm = nn.LayerNorm(rnn_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer_norm(x)\n",
    "        x = F.gelu(x)\n",
    "        x, _ = self.BiGRU(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "class SpeechRecognitionModel(nn.Module):\n",
    "    def __init__(self, n_cnn_layers, n_rnn_layers, rnn_dim, n_class, n_feats, stride=2, dropout=0.1):\n",
    "        super(SpeechRecognitionModel, self).__init__()\n",
    "        n_feats = n_feats//2\n",
    "        self.cnn = nn.Conv2d(1, 32, 3, stride=stride, padding=3//2)  # cnn for extracting heirachal features\n",
    "\n",
    "        # n residual cnn layers with filter size of 32\n",
    "        self.rescnn_layers = nn.Sequential(*[\n",
    "            ResidualCNN(32, 32, kernel=3, stride=1, dropout=dropout, n_feats=n_feats) \n",
    "            for _ in range(n_cnn_layers)\n",
    "        ])\n",
    "        self.fully_connected = nn.Linear(n_feats*32, rnn_dim)\n",
    "        self.birnn_layers = nn.Sequential(*[\n",
    "            BidirectionalGRU(rnn_dim=rnn_dim if i==0 else rnn_dim*2,\n",
    "                             hidden_size=rnn_dim, dropout=dropout, batch_first=i==0)\n",
    "            for i in range(n_rnn_layers)\n",
    "        ])\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(rnn_dim*2, rnn_dim),  # birnn returns rnn_dim*2\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(rnn_dim, n_class)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.cnn(x)\n",
    "        x = self.rescnn_layers(x)\n",
    "        sizes = x.size()\n",
    "        x = x.view(sizes[0], sizes[1] * sizes[2], sizes[3])  # (batch, feature, time)\n",
    "        x = x.transpose(1, 2) # (batch, time, feature)\n",
    "        x = self.fully_connected(x)\n",
    "        x = self.birnn_layers(x)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train / Test Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.cuda.amp.grad_scaler import GradScaler\n",
    "\n",
    "amp_enabled = True\n",
    "\n",
    "class IterMeter(object):\n",
    "    \"\"\"keeps track of total iterations\"\"\"\n",
    "    def __init__(self):\n",
    "        self.val = 0\n",
    "\n",
    "    def step(self):\n",
    "        self.val += 1\n",
    "\n",
    "    def get(self):\n",
    "        return self.val\n",
    "\n",
    "def train(model, device, train_loader, criterion, optimizer, scheduler, epoch, iter_meter):# experiment):\n",
    "    model.train()\n",
    "    data_len = len(train_loader.dataset)\n",
    "\n",
    "    # AMP\n",
    "    scaler = GradScaler()\n",
    "\n",
    "    # with experiment.train():\n",
    "    for batch_idx, _data in enumerate(train_loader):\n",
    "        spectrograms, labels, input_lengths, label_lengths = _data\n",
    "        spectrograms, labels = spectrograms.to(device), labels.to(device)\n",
    "\n",
    "        # optimizer.zero_grad()\n",
    "        \n",
    "        with torch.autocast(\n",
    "            enabled=amp_enabled,\n",
    "            dtype=torch.bfloat16,\n",
    "            device_type=\"cuda\"):\n",
    "\n",
    "            output = model(spectrograms)  # (batch, time, n_class)\n",
    "            output = F.log_softmax(output, dim=2)\n",
    "            output = output.transpose(0, 1) # (time, batch, n_class)\n",
    "\n",
    "            loss = criterion(output, labels, input_lengths, label_lengths)\n",
    "\n",
    "        # loss.backward()\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        # scaler.step(scheduler)\n",
    "        scaler.update()\n",
    "\n",
    "        #experiment.log_metric('loss', loss.item(), step=iter_meter.get())\n",
    "        #experiment.log_metric('learning_rate', scheduler.get_lr(), step=iter_meter.get())\n",
    "\n",
    "        #optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        iter_meter.step()\n",
    "        if batch_idx % 100 == 0 or batch_idx == data_len:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(spectrograms), data_len,\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "\n",
    "def test(model, device, test_loader, criterion):\n",
    "    print('\\nevaluating...')\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    test_cer, test_wer = [], []\n",
    "    #with experiment.test():\n",
    "    with torch.no_grad():\n",
    "        for i, _data in enumerate(test_loader):\n",
    "            spectrograms, labels, input_lengths, label_lengths = _data \n",
    "            spectrograms, labels = spectrograms.to(device), labels.to(device)\n",
    "\n",
    "            output = model(spectrograms)  # (batch, time, n_class)\n",
    "            output = F.log_softmax(output, dim=2)\n",
    "            output = output.transpose(0, 1) # (time, batch, n_class)\n",
    "\n",
    "            loss = criterion(output, labels, input_lengths, label_lengths)\n",
    "            test_loss += loss.item() / len(test_loader)\n",
    "\n",
    "            decoded_preds, decoded_targets = GreedyDecoder(output.transpose(0, 1), labels, label_lengths)\n",
    "            # print(f\"TEST {i}\", decoded_preds, decoded_targets)\n",
    "\n",
    "            if i in [1, 2, 3]:\n",
    "                print(decoded_targets[0:3], decoded_preds[0:3])\n",
    "                \n",
    "            for j in range(len(decoded_preds)):\n",
    "                test_cer.append(cer(decoded_targets[j], decoded_preds[j]))\n",
    "                test_wer.append(wer(decoded_targets[j], decoded_preds[j]))\n",
    "\n",
    "    avg_cer = sum(test_cer)/len(test_cer)\n",
    "    avg_wer = sum(test_wer)/len(test_wer)\n",
    "    #experiment.log_metric('test_loss', test_loss, step=iter_meter.get())\n",
    "    #experiment.log_metric('cer', avg_cer, step=iter_meter.get())\n",
    "    #experiment.log_metric('wer', avg_wer, step=iter_meter.get())\n",
    "\n",
    "    print('Test set: Average loss: {:.4f}, Average CER: {:4f} Average WER: {:.4f}\\n'.format(test_loss, avg_cer, avg_wer))\n",
    "    return test_loss\n",
    "\n",
    "def main(dataset_path, learning_rate=5e-4, batch_size=20, epochs=10):\n",
    "    hparams = {\n",
    "        \"n_cnn_layers\":  3,\n",
    "        \"n_rnn_layers\":  5,\n",
    "        \"rnn_dim\":       512,\n",
    "        \"n_class\":       len(encoder.vocab),\n",
    "        \"n_feats\":       128,\n",
    "        \"stride\":        2,\n",
    "        \"dropout\":       0.1,\n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"batch_size\":    batch_size,\n",
    "        \"epochs\":        epochs\n",
    "    }\n",
    "\n",
    "    # experiment.log_parameters(hparams)\n",
    "    print(\"hparams:\", hparams)\n",
    "\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    torch.manual_seed(7)\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "    print(\"device:\", device)\n",
    "\n",
    "    if CUR_DATASET == \"SILENT_SPEECH\":\n",
    "        dataset = SilentSpeech(\"./metadata_dgaddy.csv\")\n",
    "    else:\n",
    "        dataset = torchaudio.datasets.LJSPEECH(dataset_path, download=False)\n",
    "\n",
    "    kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n",
    "\n",
    "    dataset_len = len(dataset)\n",
    "    train_split = int(dataset_len * 0.9)\n",
    "    test_split  = dataset_len - train_split\n",
    "\n",
    "    \"\"\"\n",
    "    train_dataset, test_dataset = \\\n",
    "        torch.utils.data.random_split(dataset, [train_split, test_split])\n",
    "    \"\"\"\n",
    "\n",
    "    train_dataset, test_dataset = \\\n",
    "        torch.utils.data.random_split(dataset, [train_split, test_split])\n",
    "\n",
    "    train_loader = data.DataLoader(dataset=train_dataset,\n",
    "                                batch_size=hparams['batch_size'],\n",
    "                                shuffle=True,\n",
    "                                collate_fn=lambda x: data_processing(x, 'train'),\n",
    "                                **kwargs)\n",
    "    test_loader = data.DataLoader(dataset=test_dataset,\n",
    "                                batch_size=hparams['batch_size'],\n",
    "                                shuffle=False,\n",
    "                                collate_fn=lambda x: data_processing(x, 'valid'),\n",
    "                                **kwargs)\n",
    "    print(\"TRAIN LEN, TEST LEN:\", len(train_loader), len(test_loader))\n",
    "    model = SpeechRecognitionModel(\n",
    "        hparams['n_cnn_layers'], hparams['n_rnn_layers'], hparams['rnn_dim'],\n",
    "        hparams['n_class'], hparams['n_feats'], hparams['stride'], hparams['dropout']\n",
    "    ).to(device)\n",
    "\n",
    "    print(model)\n",
    "    print('Num Model Parameters', sum([param.nelement() for param in model.parameters()]))\n",
    "\n",
    "    optimizer = optim.AdamW(model.parameters(), hparams['learning_rate'])\n",
    "    criterion = nn.CTCLoss(blank=28).to(device)\n",
    "    scheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr=hparams['learning_rate'], \n",
    "                                            steps_per_epoch=int(len(train_loader)),\n",
    "                                            epochs=hparams['epochs'],\n",
    "                                            anneal_strategy='linear')\n",
    "    \n",
    "    best_test_loss = float(\"inf\")\n",
    "\n",
    "    iter_meter = IterMeter()\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        train(model, device, train_loader, criterion, optimizer, scheduler, epoch, iter_meter) #, experiment)\n",
    "        test_loss = test(model, device, test_loader, criterion) # , epoch, iter_meter, experiment)\n",
    "\n",
    "        if test_loss < best_test_loss:\n",
    "            torch.save(model.state_dict(), f\"./models/ds2_DATASET_{CUR_DATASET}_EPOCHS_{epoch}_TEST_LOSS_{test_loss}\")\n",
    "            best_test_loss = test_loss\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPU Runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Apr  8 05:00:11 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 510.47.03    Driver Version: 510.47.03    CUDA Version: 11.6     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ...  On   | 00000000:2B:00.0  On |                  N/A |\n",
      "|  0%   44C    P8    26W / 200W |   7410MiB /  8192MiB |      8%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A       905      G   /usr/lib/xorg/Xorg                 35MiB |\n",
      "|    0   N/A  N/A      1457      G   /usr/lib/xorg/Xorg                121MiB |\n",
      "|    0   N/A  N/A      1587      G   /usr/bin/gnome-shell               45MiB |\n",
      "|    0   N/A  N/A      2049      G   ...181418518745761657,131072      101MiB |\n",
      "|    0   N/A  N/A    483096      G   ...RendererForSitePerProcess       74MiB |\n",
      "|    0   N/A  N/A    483325      C   /usr/bin/python3                 7015MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hparams: {'n_cnn_layers': 3, 'n_rnn_layers': 5, 'rnn_dim': 512, 'n_class': 29, 'n_feats': 128, 'stride': 2, 'dropout': 0.1, 'learning_rate': 0.0005, 'batch_size': 6, 'epochs': 50}\n",
      "device: cuda\n",
      "TRAIN LEN, TEST LEN: 1135 127\n",
      "SpeechRecognitionModel(\n",
      "  (cnn): Conv2d(1, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "  (rescnn_layers): Sequential(\n",
      "    (0): ResidualCNN(\n",
      "      (cnn1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (cnn2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (dropout1): Dropout(p=0.1, inplace=False)\n",
      "      (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      (layer_norm1): CNNLayerNorm(\n",
      "        (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (layer_norm2): CNNLayerNorm(\n",
      "        (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "    (1): ResidualCNN(\n",
      "      (cnn1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (cnn2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (dropout1): Dropout(p=0.1, inplace=False)\n",
      "      (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      (layer_norm1): CNNLayerNorm(\n",
      "        (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (layer_norm2): CNNLayerNorm(\n",
      "        (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "    (2): ResidualCNN(\n",
      "      (cnn1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (cnn2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (dropout1): Dropout(p=0.1, inplace=False)\n",
      "      (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      (layer_norm1): CNNLayerNorm(\n",
      "        (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (layer_norm2): CNNLayerNorm(\n",
      "        (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (fully_connected): Linear(in_features=2048, out_features=512, bias=True)\n",
      "  (birnn_layers): Sequential(\n",
      "    (0): BidirectionalGRU(\n",
      "      (BiGRU): GRU(512, 512, batch_first=True, bidirectional=True)\n",
      "      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): BidirectionalGRU(\n",
      "      (BiGRU): GRU(1024, 512, bidirectional=True)\n",
      "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (2): BidirectionalGRU(\n",
      "      (BiGRU): GRU(1024, 512, bidirectional=True)\n",
      "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (3): BidirectionalGRU(\n",
      "      (BiGRU): GRU(1024, 512, bidirectional=True)\n",
      "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (4): BidirectionalGRU(\n",
      "      (BiGRU): GRU(1024, 512, bidirectional=True)\n",
      "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=512, bias=True)\n",
      "    (1): GELU()\n",
      "    (2): Dropout(p=0.1, inplace=False)\n",
      "    (3): Linear(in_features=512, out_features=29, bias=True)\n",
      "  )\n",
      ")\n",
      "Num Model Parameters 23705373\n",
      "Train Epoch: 1 [0/6808 (0%)]\tLoss: 8.763933\n",
      "Train Epoch: 1 [600/6808 (9%)]\tLoss: 3.073627\n",
      "Train Epoch: 1 [1200/6808 (18%)]\tLoss: 3.219058\n",
      "Train Epoch: 1 [1800/6808 (26%)]\tLoss: 3.253707\n",
      "Train Epoch: 1 [2400/6808 (35%)]\tLoss: 7.906544\n",
      "Train Epoch: 1 [3000/6808 (44%)]\tLoss: 3.101463\n",
      "Train Epoch: 1 [3600/6808 (53%)]\tLoss: 3.114317\n",
      "Train Epoch: 1 [4200/6808 (62%)]\tLoss: 2.991171\n",
      "Train Epoch: 1 [4800/6808 (70%)]\tLoss: 3.025132\n",
      "Train Epoch: 1 [5400/6808 (79%)]\tLoss: 2.905102\n",
      "Train Epoch: 1 [6000/6808 (88%)]\tLoss: 2.887359\n",
      "Train Epoch: 1 [6600/6808 (97%)]\tLoss: 2.881393\n",
      "\n",
      "evaluating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_483325/883500352.py:55: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \"\".join(encoder.batch_decode(torch.tensor(cur_target)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<unk>he was a very shy man<unk> mr<unk> holmes<unk>', 'i was wondering what i should say to this dear little woman to-night when she meets me at the door<unk><unk>', '<unk>why<unk> it<unk>s a dummy<unk><unk> said he<unk>'] ['', '', '']\n",
      "['but here she comes in person to resolve our doubts<unk><unk>', '<unk>it might have been difficult<unk> but friend lestrade held information in his hands the value of which he did not himself know<unk>', 'august <unk><unk> <unk><unk><unk><unk>'] ['', '', '']\n",
      "['<unk>of what day<unk><unk>', '<unk>god help me<unk> i would not have them ashamed of their father<unk>', 'such a thing i felt was impossible<unk>'] ['', '', '']\n",
      "Test set: Average loss: 2.9280, Average CER: 1.000000 Average WER: 1.0000\n",
      "\n",
      "Train Epoch: 2 [0/6808 (0%)]\tLoss: 2.946511\n",
      "Train Epoch: 2 [600/6808 (9%)]\tLoss: 2.946144\n",
      "Train Epoch: 2 [1200/6808 (18%)]\tLoss: 2.846111\n",
      "Train Epoch: 2 [1800/6808 (26%)]\tLoss: 2.878224\n",
      "Train Epoch: 2 [2400/6808 (35%)]\tLoss: 2.934357\n",
      "Train Epoch: 2 [3000/6808 (44%)]\tLoss: 2.809737\n",
      "Train Epoch: 2 [3600/6808 (53%)]\tLoss: 2.849675\n",
      "Train Epoch: 2 [4200/6808 (62%)]\tLoss: 2.744632\n",
      "Train Epoch: 2 [4800/6808 (70%)]\tLoss: 2.796273\n",
      "Train Epoch: 2 [5400/6808 (79%)]\tLoss: 2.882593\n",
      "Train Epoch: 2 [6000/6808 (88%)]\tLoss: 2.766690\n",
      "Train Epoch: 2 [6600/6808 (97%)]\tLoss: 2.970231\n",
      "\n",
      "evaluating...\n",
      "['<unk>he was a very shy man<unk> mr<unk> holmes<unk>', 'i was wondering what i should say to this dear little woman to-night when she meets me at the door<unk><unk>', '<unk>why<unk> it<unk>s a dummy<unk><unk> said he<unk>'] ['', '    ', '  ']\n",
      "['but here she comes in person to resolve our doubts<unk><unk>', '<unk>it might have been difficult<unk> but friend lestrade held information in his hands the value of which he did not himself know<unk>', 'august <unk><unk> <unk><unk><unk><unk>'] ['  ', '   ', '']\n",
      "['<unk>of what day<unk><unk>', '<unk>god help me<unk> i would not have them ashamed of their father<unk>', 'such a thing i felt was impossible<unk>'] ['', '  ', '   ']\n",
      "Test set: Average loss: 2.7197, Average CER: 1.000000 Average WER: 1.0000\n",
      "\n",
      "Train Epoch: 3 [0/6808 (0%)]\tLoss: 2.832345\n",
      "Train Epoch: 3 [600/6808 (9%)]\tLoss: 2.705128\n",
      "Train Epoch: 3 [1200/6808 (18%)]\tLoss: 2.582582\n",
      "Train Epoch: 3 [1800/6808 (26%)]\tLoss: 2.407532\n",
      "Train Epoch: 3 [2400/6808 (35%)]\tLoss: 2.552530\n",
      "Train Epoch: 3 [3000/6808 (44%)]\tLoss: 2.355106\n",
      "Train Epoch: 3 [3600/6808 (53%)]\tLoss: 2.317770\n",
      "Train Epoch: 3 [4200/6808 (62%)]\tLoss: 2.324232\n",
      "Train Epoch: 3 [4800/6808 (70%)]\tLoss: 2.620332\n",
      "Train Epoch: 3 [5400/6808 (79%)]\tLoss: 1.997459\n",
      "Train Epoch: 3 [6000/6808 (88%)]\tLoss: 1.907950\n",
      "Train Epoch: 3 [6600/6808 (97%)]\tLoss: 2.113764\n",
      "\n",
      "evaluating...\n",
      "['<unk>he was a very shy man<unk> mr<unk> holmes<unk>', 'i was wondering what i should say to this dear little woman to-night when she meets me at the door<unk><unk>', '<unk>why<unk> it<unk>s a dummy<unk><unk> said he<unk>'] ['he iesa bhe sha mainstros<unk>', 'i wis mo dern woat as sho sa tho whes thee we wone so mit wn sho m s at the dhor<unk> ', 'wo hsie da<unk> sai t<unk>']\n",
      "['but here she comes in person to resolve our doubts<unk><unk>', '<unk>it might have been difficult<unk> but friend lestrade held information in his hands the value of which he did not himself know<unk>', 'august <unk><unk> <unk><unk><unk><unk>'] ['whot i thashe coson bhersio tr so her dhas<unk>', 'it m a the det thbot wot crn whes scr hor tin fo basha in hes ananns whe toa wo breth te the mot an so no<unk>', 'as  <unk><unk>']\n",
      "['<unk>of what day<unk><unk>', '<unk>god help me<unk> i would not have them ashamed of their father<unk>', 'such a thing i felt was impossible<unk>'] ['o bra daa<unk>', 'he dho o a whi mot ho tin ashainn to tier foter<unk>', 'se the te a co woes n crosl<unk>']\n",
      "Test set: Average loss: 1.9662, Average CER: 0.582575 Average WER: 1.0370\n",
      "\n",
      "Train Epoch: 4 [0/6808 (0%)]\tLoss: 2.112331\n",
      "Train Epoch: 4 [600/6808 (9%)]\tLoss: 2.032781\n",
      "Train Epoch: 4 [1200/6808 (18%)]\tLoss: 1.632278\n",
      "Train Epoch: 4 [1800/6808 (26%)]\tLoss: 1.881211\n",
      "Train Epoch: 4 [2400/6808 (35%)]\tLoss: 1.857310\n",
      "Train Epoch: 4 [3000/6808 (44%)]\tLoss: 1.675190\n",
      "Train Epoch: 4 [3600/6808 (53%)]\tLoss: 1.467692\n",
      "Train Epoch: 4 [4200/6808 (62%)]\tLoss: 1.505010\n",
      "Train Epoch: 4 [4800/6808 (70%)]\tLoss: 1.780622\n",
      "Train Epoch: 4 [5400/6808 (79%)]\tLoss: 1.749029\n",
      "Train Epoch: 4 [6000/6808 (88%)]\tLoss: 1.696274\n",
      "Train Epoch: 4 [6600/6808 (97%)]\tLoss: 1.768798\n",
      "\n",
      "evaluating...\n",
      "['<unk>he was a very shy man<unk> mr<unk> holmes<unk>', 'i was wondering what i should say to this dear little woman to-night when she meets me at the door<unk><unk>', '<unk>why<unk> it<unk>s a dummy<unk><unk> said he<unk>'] ['<unk>he is a bery shai manestrohos<unk>', 'i wlas ondern<unk> wowat i ho say to whis dee wo lonlon se mit woen sho met tme at the dor<unk>', '<unk>wi it si dom<unk> said he<unk>']\n",
      "['but here she comes in person to resolve our doubts<unk><unk>', '<unk>it might have been difficult<unk> but friend lestrade held information in his hands the value of which he did not himself know<unk>', 'august <unk><unk> <unk><unk><unk><unk>'] ['but i harshou omton ferson torsa ber tas<unk>', 'it mi han ben ti tipot<unk> bo fren wa str ho in fomachiand in his has whe a ofbech he i not hand so hno<unk>', 'agest <unk> <unk><unk><unk><unk>']\n",
      "['<unk>of what day<unk><unk>', '<unk>god help me<unk> i would not have them ashamed of their father<unk>', 'such a thing i felt was impossible<unk>'] ['<unk>ou wat day<unk>', '<unk>he go holme<unk> i wi not ha len a shan of ther fother<unk>', '<unk>sit a ten i pfou wais on crosibl<unk>']\n",
      "Test set: Average loss: 1.4965, Average CER: 0.446864 Average WER: 0.9463\n",
      "\n",
      "Train Epoch: 5 [0/6808 (0%)]\tLoss: 1.599759\n",
      "Train Epoch: 5 [600/6808 (9%)]\tLoss: 1.464445\n",
      "Train Epoch: 5 [1200/6808 (18%)]\tLoss: 1.782340\n",
      "Train Epoch: 5 [1800/6808 (26%)]\tLoss: 1.662272\n",
      "Train Epoch: 5 [2400/6808 (35%)]\tLoss: 1.330230\n",
      "Train Epoch: 5 [3000/6808 (44%)]\tLoss: 1.738247\n",
      "Train Epoch: 5 [3600/6808 (53%)]\tLoss: 1.291448\n",
      "Train Epoch: 5 [4200/6808 (62%)]\tLoss: 1.520600\n",
      "Train Epoch: 5 [4800/6808 (70%)]\tLoss: 1.581934\n",
      "Train Epoch: 5 [5400/6808 (79%)]\tLoss: 1.413018\n",
      "Train Epoch: 5 [6000/6808 (88%)]\tLoss: 1.399690\n",
      "Train Epoch: 5 [6600/6808 (97%)]\tLoss: 1.369585\n",
      "\n",
      "evaluating...\n",
      "['<unk>he was a very shy man<unk> mr<unk> holmes<unk>', 'i was wondering what i should say to this dear little woman to-night when she meets me at the door<unk><unk>', '<unk>why<unk> it<unk>s a dummy<unk><unk> said he<unk>'] ['e is a bary shaid man sr holms<unk>', 'i was mondring wat i shod say to wis deua lo won tin so igt when sho ma smy at the dor<unk>', 'whiy isiadomy<unk> said he<unk>']\n",
      "['but here she comes in person to resolve our doubts<unk><unk>', '<unk>it might have been difficult<unk> but friend lestrade held information in his hands the value of which he did not himself know<unk>', 'august <unk><unk> <unk><unk><unk><unk>'] ['but harsho contin fersen trsal byre dous<unk>', 'it my hav pin dic icolt butt frine wae strad fo in comathian in his hans theh gay o bich ho ie not and solf fhno', 'august <unk> <unk><unk><unk><unk>']\n",
      "['<unk>of what day<unk><unk>', '<unk>god help me<unk> i would not have them ashamed of their father<unk>', 'such a thing i felt was impossible<unk>'] ['of watday<unk>', 'e gad hle my i wind not ha lhen a shan of ther fother<unk>', 'sit a thang i frult wison cosibl<unk>']\n",
      "Test set: Average loss: 1.2702, Average CER: 0.429638 Average WER: 0.8833\n",
      "\n",
      "Train Epoch: 6 [0/6808 (0%)]\tLoss: 1.603094\n",
      "Train Epoch: 6 [600/6808 (9%)]\tLoss: 1.342440\n",
      "Train Epoch: 6 [1200/6808 (18%)]\tLoss: 1.207247\n",
      "Train Epoch: 6 [1800/6808 (26%)]\tLoss: 1.398296\n",
      "Train Epoch: 6 [2400/6808 (35%)]\tLoss: 1.416543\n",
      "Train Epoch: 6 [3000/6808 (44%)]\tLoss: 1.288539\n",
      "Train Epoch: 6 [3600/6808 (53%)]\tLoss: 1.637681\n",
      "Train Epoch: 6 [4200/6808 (62%)]\tLoss: 1.457315\n",
      "Train Epoch: 6 [4800/6808 (70%)]\tLoss: 0.984599\n",
      "Train Epoch: 6 [5400/6808 (79%)]\tLoss: 1.392658\n",
      "Train Epoch: 6 [6000/6808 (88%)]\tLoss: 1.335597\n",
      "Train Epoch: 6 [6600/6808 (97%)]\tLoss: 1.430655\n",
      "\n",
      "evaluating...\n",
      "['<unk>he was a very shy man<unk> mr<unk> holmes<unk>', 'i was wondering what i should say to this dear little woman to-night when she meets me at the door<unk><unk>', '<unk>why<unk> it<unk>s a dummy<unk><unk> said he<unk>'] ['he mis avd bery shoi mannmsr holms<unk>', 'i was vondering what i shold say to this deilotwon tinso night when shoe meteso me at the dor<unk>', 'whid itsitomi<unk> said he<unk>']\n",
      "['but here she comes in person to resolve our doubts<unk><unk>', '<unk>it might have been difficult<unk> but friend lestrade held information in his hands the value of which he did not himself know<unk>', 'august <unk><unk> <unk><unk><unk><unk>'] ['but harsho contonperson torsa byr dous<unk>', 'it myt havpintditicolt fbut frined wae stravd fot in for matin tin his hans the voty of fbich hed titd not hand self f no<unk>', 'august <unk> <unk><unk><unk><unk>']\n",
      "['<unk>of what day<unk><unk>', '<unk>god help me<unk> i would not have them ashamed of their father<unk>', 'such a thing i felt was impossible<unk>'] ['of watday<unk>', 'he god hol mey i witd not havd them i shan dof ther fother<unk>', 'sit ething i fot wis onpossibl<unk>']\n",
      "Test set: Average loss: 1.1574, Average CER: 0.401288 Average WER: 0.8212\n",
      "\n",
      "Train Epoch: 7 [0/6808 (0%)]\tLoss: 1.201973\n",
      "Train Epoch: 7 [600/6808 (9%)]\tLoss: 1.458893\n",
      "Train Epoch: 7 [1200/6808 (18%)]\tLoss: 1.397808\n",
      "Train Epoch: 7 [1800/6808 (26%)]\tLoss: 1.245997\n",
      "Train Epoch: 7 [2400/6808 (35%)]\tLoss: 1.376141\n",
      "Train Epoch: 7 [3000/6808 (44%)]\tLoss: 1.238816\n",
      "Train Epoch: 7 [3600/6808 (53%)]\tLoss: 1.611536\n",
      "Train Epoch: 7 [4200/6808 (62%)]\tLoss: 1.298188\n",
      "Train Epoch: 7 [4800/6808 (70%)]\tLoss: 1.208243\n",
      "Train Epoch: 7 [5400/6808 (79%)]\tLoss: 1.128054\n",
      "Train Epoch: 7 [6000/6808 (88%)]\tLoss: 1.267850\n",
      "Train Epoch: 7 [6600/6808 (97%)]\tLoss: 1.159918\n",
      "\n",
      "evaluating...\n",
      "['<unk>he was a very shy man<unk> mr<unk> holmes<unk>', 'i was wondering what i should say to this dear little woman to-night when she meets me at the door<unk><unk>', '<unk>why<unk> it<unk>s a dummy<unk><unk> said he<unk>'] ['he was av vary shoi maanm sr<unk>holmes<unk>', 'i was ondering what i should say to this dera litlwoln lhn tonight when sho mee s me at the dor<unk>', '<unk>whi it s odomni<unk><unk> said he<unk>']\n",
      "['but here she comes in person to resolve our doubts<unk><unk>', '<unk>it might have been difficult<unk> but friend lestrade held information in his hands the value of which he did not himself know<unk>', 'august <unk><unk> <unk><unk><unk><unk>'] ['but harshe comtom ferson tursal ber dous<unk>', 'it mit have fen dit acolt but fruend wle strave hl in fom mation win his hans theh dauty of fich he tasd not hand self f no<unk>', 'august <unk> <unk><unk><unk>']\n",
      "['<unk>of what day<unk><unk>', '<unk>god help me<unk> i would not have them ashamed of their father<unk>', 'such a thing i felt was impossible<unk>'] ['of watday<unk>', '<unk>he gad hele me<unk> i wald not have them a shame d of ther fother<unk> ', 'sut ef thing i fult was omtossibl<unk>']\n",
      "Test set: Average loss: 1.0130, Average CER: 0.337999 Average WER: 0.7851\n",
      "\n",
      "Train Epoch: 8 [0/6808 (0%)]\tLoss: 1.378267\n",
      "Train Epoch: 8 [600/6808 (9%)]\tLoss: 1.086902\n",
      "Train Epoch: 8 [1200/6808 (18%)]\tLoss: 1.235385\n",
      "Train Epoch: 8 [1800/6808 (26%)]\tLoss: 1.171377\n",
      "Train Epoch: 8 [2400/6808 (35%)]\tLoss: 1.324545\n",
      "Train Epoch: 8 [3000/6808 (44%)]\tLoss: 1.138460\n",
      "Train Epoch: 8 [3600/6808 (53%)]\tLoss: 1.003440\n",
      "Train Epoch: 8 [4200/6808 (62%)]\tLoss: 1.075482\n",
      "Train Epoch: 8 [4800/6808 (70%)]\tLoss: 1.026065\n",
      "Train Epoch: 8 [5400/6808 (79%)]\tLoss: 0.855193\n",
      "Train Epoch: 8 [6000/6808 (88%)]\tLoss: 1.120329\n",
      "Train Epoch: 8 [6600/6808 (97%)]\tLoss: 1.048937\n",
      "\n",
      "evaluating...\n",
      "['<unk>he was a very shy man<unk> mr<unk> holmes<unk>', 'i was wondering what i should say to this dear little woman to-night when she meets me at the door<unk><unk>', '<unk>why<unk> it<unk>s a dummy<unk><unk> said he<unk>'] ['he was av very shi manr<unk>holmes<unk>', 'i was mondering wat i should say to his der lit won min sonight when shoe mes me at the dor<unk> ', 'why it s adonin<unk><unk> said he<unk>']\n",
      "['but here she comes in person to resolve our doubts<unk><unk>', '<unk>it might have been difficult<unk> but friend lestrade held information in his hands the value of which he did not himself know<unk>', 'august <unk><unk> <unk><unk><unk><unk>'] ['but her shou comtempersen tursa fer douse<unk>', 'it miht have pen diicolt buthe frin wae strae hu in fom mation in his hans wieh vouty ofbich he hid not hndself f no<unk>', 'august <unk> <unk><unk><unk>']\n",
      "['<unk>of what day<unk><unk>', '<unk>god help me<unk> i would not have them ashamed of their father<unk>', 'such a thing i felt was impossible<unk>'] ['of wat day<unk>', 'he god hele me<unk> i wuld not have tlhem ui shan of ther fother<unk> ', 'sut af theing i fupt was om cossible<unk>']\n",
      "Test set: Average loss: 0.9234, Average CER: 0.335955 Average WER: 0.7512\n",
      "\n",
      "Train Epoch: 9 [0/6808 (0%)]\tLoss: 1.078937\n",
      "Train Epoch: 9 [600/6808 (9%)]\tLoss: 0.958244\n",
      "Train Epoch: 9 [1200/6808 (18%)]\tLoss: 1.428388\n",
      "Train Epoch: 9 [1800/6808 (26%)]\tLoss: 1.231367\n",
      "Train Epoch: 9 [2400/6808 (35%)]\tLoss: 0.990191\n",
      "Train Epoch: 9 [3000/6808 (44%)]\tLoss: 1.692653\n",
      "Train Epoch: 9 [3600/6808 (53%)]\tLoss: 0.710431\n",
      "Train Epoch: 9 [4200/6808 (62%)]\tLoss: 1.039038\n",
      "Train Epoch: 9 [4800/6808 (70%)]\tLoss: 1.120817\n",
      "Train Epoch: 9 [5400/6808 (79%)]\tLoss: 1.054943\n",
      "Train Epoch: 9 [6000/6808 (88%)]\tLoss: 1.011884\n",
      "Train Epoch: 9 [6600/6808 (97%)]\tLoss: 1.036774\n",
      "\n",
      "evaluating...\n",
      "['<unk>he was a very shy man<unk> mr<unk> holmes<unk>', 'i was wondering what i should say to this dear little woman to-night when she meets me at the door<unk><unk>', '<unk>why<unk> it<unk>s a dummy<unk><unk> said he<unk>'] ['he was hav vary sha i ma n<unk> mir<unk> holmes<unk>', 'i wasld ondgring what i should say to this der litlwon into night whn she mepso me at the dor<unk>', 'whi it sa donvy<unk><unk> said he<unk><unk><unk><unk>']\n",
      "['but here she comes in person to resolve our doubts<unk><unk>', '<unk>it might have been difficult<unk> but friend lestrade held information in his hands the value of which he did not himself know<unk>', 'august <unk><unk> <unk><unk><unk><unk>'] ['but here shou contom person torsa biredous<unk><unk><unk>', 'it miht have fen diacolt but frind wle strade tholl in fommation in his hans whih viu of which he dhild not and self f no<unk>', 'august <unk> <unk><unk><unk>']\n",
      "['<unk>of what day<unk><unk>', '<unk>god help me<unk> i would not have them ashamed of their father<unk>', 'such a thing i felt was impossible<unk>'] ['i what day<unk>', 'he gd help me<unk> i would not havd them i shamne d of ther fother<unk>', 'sute thing i fut was um cossible<unk>']\n",
      "Test set: Average loss: 0.8751, Average CER: 0.367106 Average WER: 0.7288\n",
      "\n",
      "Train Epoch: 10 [0/6808 (0%)]\tLoss: 0.956111\n",
      "Train Epoch: 10 [600/6808 (9%)]\tLoss: 0.779663\n",
      "Train Epoch: 10 [1200/6808 (18%)]\tLoss: 0.830260\n",
      "Train Epoch: 10 [1800/6808 (26%)]\tLoss: 0.700374\n",
      "Train Epoch: 10 [2400/6808 (35%)]\tLoss: 0.882881\n",
      "Train Epoch: 10 [3000/6808 (44%)]\tLoss: 1.018685\n",
      "Train Epoch: 10 [3600/6808 (53%)]\tLoss: 1.006846\n",
      "Train Epoch: 10 [4200/6808 (62%)]\tLoss: 0.905641\n",
      "Train Epoch: 10 [4800/6808 (70%)]\tLoss: 1.163808\n",
      "Train Epoch: 10 [5400/6808 (79%)]\tLoss: 1.108336\n",
      "Train Epoch: 10 [6000/6808 (88%)]\tLoss: 1.015033\n",
      "Train Epoch: 10 [6600/6808 (97%)]\tLoss: 1.249305\n",
      "\n",
      "evaluating...\n",
      "['<unk>he was a very shy man<unk> mr<unk> holmes<unk>', 'i was wondering what i should say to this dear little woman to-night when she meets me at the door<unk><unk>', '<unk>why<unk> it<unk>s a dummy<unk><unk> said he<unk>'] ['he was hav very shai man r<unk> holmes<unk>', 'i was mongdering what i should say to this der litl won intoigt when she me s me at the do<unk>', 'whi itf sadomming<unk><unk> said he<unk>']\n",
      "['but here she comes in person to resolve our doubts<unk><unk>', '<unk>it might have been difficult<unk> but friend lestrade held information in his hands the value of which he did not himself know<unk>', 'august <unk><unk> <unk><unk><unk><unk>'] ['but her she contimpersen thoresa byr boups<unk>', 'it migt hav fen dificult but frine we strave he in fromatin in his hands whe viy of which he did not hand self now<unk>', 'august <unk> <unk><unk><unk>']\n",
      "['<unk>of what day<unk><unk>', '<unk>god help me<unk> i would not have them ashamed of their father<unk>', 'such a thing i felt was impossible<unk>'] ['aof wat day<unk>', 'hegad helpe me<unk> i wuld not have them ia shame of ther father<unk>', 'slit af thing i felt was um prossible<unk>']\n",
      "Test set: Average loss: 0.8399, Average CER: 0.302561 Average WER: 0.6962\n",
      "\n",
      "Train Epoch: 11 [0/6808 (0%)]\tLoss: 0.831671\n",
      "Train Epoch: 11 [600/6808 (9%)]\tLoss: 0.870696\n",
      "Train Epoch: 11 [1200/6808 (18%)]\tLoss: 0.857031\n",
      "Train Epoch: 11 [1800/6808 (26%)]\tLoss: 0.934402\n",
      "Train Epoch: 11 [2400/6808 (35%)]\tLoss: 1.130227\n",
      "Train Epoch: 11 [3000/6808 (44%)]\tLoss: 0.909669\n",
      "Train Epoch: 11 [3600/6808 (53%)]\tLoss: 0.757679\n",
      "Train Epoch: 11 [4200/6808 (62%)]\tLoss: 0.859261\n",
      "Train Epoch: 11 [4800/6808 (70%)]\tLoss: 0.911603\n",
      "Train Epoch: 11 [5400/6808 (79%)]\tLoss: 0.862796\n",
      "Train Epoch: 11 [6000/6808 (88%)]\tLoss: 0.787322\n",
      "Train Epoch: 11 [6600/6808 (97%)]\tLoss: 0.964186\n",
      "\n",
      "evaluating...\n",
      "['<unk>he was a very shy man<unk> mr<unk> holmes<unk>', 'i was wondering what i should say to this dear little woman to-night when she meets me at the door<unk><unk>', '<unk>why<unk> it<unk>s a dummy<unk><unk> said he<unk>'] ['he was a bary shai man mr<unk> holmes<unk>', 'i was mondering what i should say to this dear littlwon intonight wheins she mep t me at the dor<unk>', '<unk>why itsadonmiy<unk><unk> said he<unk>']\n",
      "['but here she comes in person to resolve our doubts<unk><unk>', '<unk>it might have been difficult<unk> but friend lestrade held information in his hands the value of which he did not himself know<unk>', 'august <unk><unk> <unk><unk><unk><unk>'] ['but sher so cometomperson toresal far douts<unk>', 'it might havben dificult but fruind lestrade hell in frommation in his hans the vodi ofwhich he did not hindself now<unk>', 'august <unk> <unk><unk><unk>']\n",
      "['<unk>of what day<unk><unk>', '<unk>god help me<unk> i would not have them ashamed of their father<unk>', 'such a thing i felt was impossible<unk>'] ['of whatday<unk>', '<unk>he goid help me<unk> i would not have them a shand of ther father<unk>', 'sut o thing i felt was ompossible<unk>']\n",
      "Test set: Average loss: 0.7628, Average CER: 0.283782 Average WER: 0.6523\n",
      "\n",
      "Train Epoch: 12 [0/6808 (0%)]\tLoss: 0.815364\n",
      "Train Epoch: 12 [600/6808 (9%)]\tLoss: 1.029932\n",
      "Train Epoch: 12 [1200/6808 (18%)]\tLoss: 0.797757\n",
      "Train Epoch: 12 [1800/6808 (26%)]\tLoss: 0.949406\n",
      "Train Epoch: 12 [2400/6808 (35%)]\tLoss: 0.908691\n",
      "Train Epoch: 12 [3000/6808 (44%)]\tLoss: 0.675306\n",
      "Train Epoch: 12 [3600/6808 (53%)]\tLoss: 0.802604\n",
      "Train Epoch: 12 [4200/6808 (62%)]\tLoss: 0.854939\n",
      "Train Epoch: 12 [4800/6808 (70%)]\tLoss: 0.761289\n",
      "Train Epoch: 12 [5400/6808 (79%)]\tLoss: 0.910907\n",
      "Train Epoch: 12 [6000/6808 (88%)]\tLoss: 0.663695\n",
      "Train Epoch: 12 [6600/6808 (97%)]\tLoss: 0.880667\n",
      "\n",
      "evaluating...\n",
      "['<unk>he was a very shy man<unk> mr<unk> holmes<unk>', 'i was wondering what i should say to this dear little woman to-night when she meets me at the door<unk><unk>', '<unk>why<unk> it<unk>s a dummy<unk><unk> said he<unk>'] ['he was a bery shi man<unk> sr<unk> holmes<unk>', 'i was monedering<unk> what i should say to this der littl loen en tonight<unk> when she mee s me at the dor<unk>', '<unk>why its a donmiy<unk><unk> said he<unk>']\n",
      "['but here she comes in person to resolve our doubts<unk><unk>', '<unk>it might have been difficult<unk> but friend lestrade held information in his hands the value of which he did not himself know<unk>', 'august <unk><unk> <unk><unk><unk><unk>'] ['but he she comes inperson tho res al fur doups<unk>', 'it myht have been dificult but frind le strad he in promation in his hands the boute ofe which he dild not hand self now<unk>', 'august <unk> <unk><unk>']\n",
      "['<unk>of what day<unk><unk>', '<unk>god help me<unk> i would not have them ashamed of their father<unk>', 'such a thing i felt was impossible<unk>'] ['aof what day<unk>', 'he god help me<unk> i would not have them a shamnee of ther fother<unk>', 'slit ao thing i fult was im prossible<unk>']\n",
      "Test set: Average loss: 0.7779, Average CER: 0.281659 Average WER: 0.6577\n",
      "\n",
      "Train Epoch: 13 [0/6808 (0%)]\tLoss: 0.736689\n",
      "Train Epoch: 13 [600/6808 (9%)]\tLoss: 0.723956\n",
      "Train Epoch: 13 [1200/6808 (18%)]\tLoss: 0.926405\n",
      "Train Epoch: 13 [1800/6808 (26%)]\tLoss: 0.874866\n",
      "Train Epoch: 13 [2400/6808 (35%)]\tLoss: 0.960277\n",
      "Train Epoch: 13 [3000/6808 (44%)]\tLoss: 0.846292\n",
      "Train Epoch: 13 [3600/6808 (53%)]\tLoss: 0.696514\n",
      "Train Epoch: 13 [4200/6808 (62%)]\tLoss: 0.853876\n",
      "Train Epoch: 13 [4800/6808 (70%)]\tLoss: 0.734186\n",
      "Train Epoch: 13 [5400/6808 (79%)]\tLoss: 0.811137\n",
      "Train Epoch: 13 [6000/6808 (88%)]\tLoss: 0.897803\n",
      "Train Epoch: 13 [6600/6808 (97%)]\tLoss: 0.895524\n",
      "\n",
      "evaluating...\n",
      "['<unk>he was a very shy man<unk> mr<unk> holmes<unk>', 'i was wondering what i should say to this dear little woman to-night when she meets me at the door<unk><unk>', '<unk>why<unk> it<unk>s a dummy<unk><unk> said he<unk>'] ['he was av vary shd may n<unk> mr<unk> holmes<unk>', 'i was ondering what i should say to this dera littlon in tonight whens she me so me at the dor<unk>', 'why it s atoin<unk><unk> said he<unk>']\n",
      "['but here she comes in person to resolve our doubts<unk><unk>', '<unk>it might have been difficult<unk> but friend lestrade held information in his hands the value of which he did not himself know<unk>', 'august <unk><unk> <unk><unk><unk><unk>'] ['but her syou come t inmpersn to resal haure douts<unk>', 'it migt havepend dificult but friend we strade held infrommation in his hands thth votoy offwhich he dild not hind self now<unk>', 'august <unk><unk> <unk><unk><unk><unk>']\n",
      "['<unk>of what day<unk><unk>', '<unk>god help me<unk> i would not have them ashamed of their father<unk>', 'such a thing i felt was impossible<unk>'] ['of what day<unk>', 'he god help me<unk> i would not have them a shaned of ther father<unk>', 'hut af thing i felt was im possible<unk>']\n",
      "Test set: Average loss: 0.7291, Average CER: 0.264112 Average WER: 0.6323\n",
      "\n",
      "Train Epoch: 14 [0/6808 (0%)]\tLoss: 0.767296\n",
      "Train Epoch: 14 [600/6808 (9%)]\tLoss: 0.820185\n",
      "Train Epoch: 14 [1200/6808 (18%)]\tLoss: 0.804571\n",
      "Train Epoch: 14 [1800/6808 (26%)]\tLoss: 0.919856\n",
      "Train Epoch: 14 [2400/6808 (35%)]\tLoss: 0.783410\n",
      "Train Epoch: 14 [3000/6808 (44%)]\tLoss: 0.623148\n",
      "Train Epoch: 14 [3600/6808 (53%)]\tLoss: 0.797814\n",
      "Train Epoch: 14 [4200/6808 (62%)]\tLoss: 0.737746\n",
      "Train Epoch: 14 [4800/6808 (70%)]\tLoss: 0.721899\n",
      "Train Epoch: 14 [5400/6808 (79%)]\tLoss: 0.636471\n",
      "Train Epoch: 14 [6000/6808 (88%)]\tLoss: 0.632221\n",
      "Train Epoch: 14 [6600/6808 (97%)]\tLoss: 0.855112\n",
      "\n",
      "evaluating...\n",
      "['<unk>he was a very shy man<unk> mr<unk> holmes<unk>', 'i was wondering what i should say to this dear little woman to-night when she meets me at the door<unk><unk>', '<unk>why<unk> it<unk>s a dummy<unk><unk> said he<unk>'] ['he was a very shi man<unk> r<unk> holmes<unk>', 'i was mondring what i should say to this der littlbonon tonight when she me s me at the dor<unk>', 'why is a doniy<unk><unk> said he<unk>']\n",
      "['but here she comes in person to resolve our doubts<unk><unk>', '<unk>it might have been difficult<unk> but friend lestrade held information in his hands the value of which he did not himself know<unk>', 'august <unk><unk> <unk><unk><unk><unk>'] ['but he shye cone omperson tors aar douts<unk>', 'it might have ben diffacult but frind lestrave held infomation in his hands tit vio ofwhich he did not handself now<unk>', 'august <unk><unk> <unk><unk><unk>']\n",
      "['<unk>of what day<unk><unk>', '<unk>god help me<unk> i would not have them ashamed of their father<unk>', 'such a thing i felt was impossible<unk>'] ['af what day<unk>', 'he gd helpe me<unk> i would not have them i shaned of their father<unk>', 'sut a thiing i fet was imcrossible<unk>']\n",
      "Test set: Average loss: 0.7063, Average CER: 0.250763 Average WER: 0.5930\n",
      "\n",
      "Train Epoch: 15 [0/6808 (0%)]\tLoss: 0.784766\n",
      "Train Epoch: 15 [600/6808 (9%)]\tLoss: 0.952528\n",
      "Train Epoch: 15 [1200/6808 (18%)]\tLoss: 0.542597\n",
      "Train Epoch: 15 [1800/6808 (26%)]\tLoss: 0.618629\n",
      "Train Epoch: 15 [2400/6808 (35%)]\tLoss: 0.732953\n",
      "Train Epoch: 15 [3000/6808 (44%)]\tLoss: 0.810178\n",
      "Train Epoch: 15 [3600/6808 (53%)]\tLoss: 0.938243\n",
      "Train Epoch: 15 [4200/6808 (62%)]\tLoss: 0.763440\n",
      "Train Epoch: 15 [4800/6808 (70%)]\tLoss: 0.674162\n",
      "Train Epoch: 15 [5400/6808 (79%)]\tLoss: 0.882073\n",
      "Train Epoch: 15 [6000/6808 (88%)]\tLoss: 0.573810\n",
      "Train Epoch: 15 [6600/6808 (97%)]\tLoss: 0.674669\n",
      "\n",
      "evaluating...\n",
      "['<unk>he was a very shy man<unk> mr<unk> holmes<unk>', 'i was wondering what i should say to this dear little woman to-night when she meets me at the door<unk><unk>', '<unk>why<unk> it<unk>s a dummy<unk><unk> said he<unk>'] ['he was a very shid ma an mr<unk> holmes<unk>', 'i was wondering what ishould say to this dear little wonin tonight when she me to me at the dor<unk>', 'why its a domny<unk><unk> said he<unk>']\n",
      "['but here she comes in person to resolve our doubts<unk><unk>', '<unk>it might have been difficult<unk> but friend lestrade held information in his hands the value of which he did not himself know<unk>', 'august <unk><unk> <unk><unk><unk><unk>'] ['but he shyoe conme ton person tores af aur douts<unk>', 'it might have been difficult bute frin wle strade held in frommation in his hands be viou of which he did not hamnself now<unk>', 'august <unk><unk> <unk><unk><unk><unk>']\n",
      "['<unk>of what day<unk><unk>', '<unk>god help me<unk> i would not have them ashamed of their father<unk>', 'such a thing i felt was impossible<unk>'] ['aof want daay<unk>', 'he god hele me i would not have them i shan d of their father<unk>', 'sutdt a thing i felt was im cossible<unk>']\n",
      "Test set: Average loss: 0.6787, Average CER: 0.263101 Average WER: 0.6132\n",
      "\n",
      "Train Epoch: 16 [0/6808 (0%)]\tLoss: 0.585374\n",
      "Train Epoch: 16 [600/6808 (9%)]\tLoss: 0.735892\n",
      "Train Epoch: 16 [1200/6808 (18%)]\tLoss: 0.951348\n",
      "Train Epoch: 16 [1800/6808 (26%)]\tLoss: 0.810024\n",
      "Train Epoch: 16 [2400/6808 (35%)]\tLoss: 0.818230\n",
      "Train Epoch: 16 [3000/6808 (44%)]\tLoss: 0.854274\n",
      "Train Epoch: 16 [3600/6808 (53%)]\tLoss: 0.860446\n",
      "Train Epoch: 16 [4200/6808 (62%)]\tLoss: 0.820058\n",
      "Train Epoch: 16 [4800/6808 (70%)]\tLoss: 0.760005\n",
      "Train Epoch: 16 [5400/6808 (79%)]\tLoss: 0.821569\n",
      "Train Epoch: 16 [6000/6808 (88%)]\tLoss: 0.760266\n",
      "Train Epoch: 16 [6600/6808 (97%)]\tLoss: 0.555801\n",
      "\n",
      "evaluating...\n",
      "['<unk>he was a very shy man<unk> mr<unk> holmes<unk>', 'i was wondering what i should say to this dear little woman to-night when she meets me at the door<unk><unk>', '<unk>why<unk> it<unk>s a dummy<unk><unk> said he<unk>'] ['he wias a very shid man<unk> mr<unk> holmes<unk>', 'i was mondering what i should say to this dear littl wonen tonight when she mea so me at the door<unk>', 'why it s a demy<unk><unk> said he<unk>']\n",
      "['but here she comes in person to resolve our doubts<unk><unk>', '<unk>it might have been difficult<unk> but friend lestrade held information in his hands the value of which he did not himself know<unk>', 'august <unk><unk> <unk><unk><unk><unk>'] ['but heu she come s omperson tores allf aur douts<unk>', 'it might have be dificult bute friend lestradve hel infrommation in his hands the vou of which he did not hamself know<unk>', 'august <unk><unk> <unk><unk><unk>']\n",
      "['<unk>of what day<unk><unk>', '<unk>god help me<unk> i would not have them ashamed of their father<unk>', 'such a thing i felt was impossible<unk>'] ['of wat day<unk>', 'egod help me<unk> i would not have tem a shamed of their father<unk>', 'sit ha thing i felt was imcosible<unk>']\n",
      "Test set: Average loss: 0.6454, Average CER: 0.239419 Average WER: 0.5790\n",
      "\n",
      "Train Epoch: 17 [0/6808 (0%)]\tLoss: 0.538262\n",
      "Train Epoch: 17 [600/6808 (9%)]\tLoss: 0.684333\n",
      "Train Epoch: 17 [1200/6808 (18%)]\tLoss: 0.642975\n",
      "Train Epoch: 17 [1800/6808 (26%)]\tLoss: 0.570745\n",
      "Train Epoch: 17 [2400/6808 (35%)]\tLoss: 0.788715\n",
      "Train Epoch: 17 [3000/6808 (44%)]\tLoss: 0.717691\n",
      "Train Epoch: 17 [3600/6808 (53%)]\tLoss: 0.454020\n",
      "Train Epoch: 17 [4200/6808 (62%)]\tLoss: 0.546913\n",
      "Train Epoch: 17 [4800/6808 (70%)]\tLoss: 0.878846\n",
      "Train Epoch: 17 [5400/6808 (79%)]\tLoss: 0.525952\n",
      "Train Epoch: 17 [6000/6808 (88%)]\tLoss: 0.636785\n",
      "Train Epoch: 17 [6600/6808 (97%)]\tLoss: 0.795161\n",
      "\n",
      "evaluating...\n",
      "['<unk>he was a very shy man<unk> mr<unk> holmes<unk>', 'i was wondering what i should say to this dear little woman to-night when she meets me at the door<unk><unk>', '<unk>why<unk> it<unk>s a dummy<unk><unk> said he<unk>'] ['he was av vary shyd man<unk>mr<unk> holmes<unk>', 'i was mondering what i should say to this der littl onin tonight when she me t me at the dor<unk>', 'why its adomy<unk><unk> said he<unk>']\n",
      "['but here she comes in person to resolve our doubts<unk><unk>', '<unk>it might have been difficult<unk> but friend lestrade held information in his hands the value of which he did not himself know<unk>', 'august <unk><unk> <unk><unk><unk><unk>'] ['<unk>but sheu she comesomperson to resalfour douts<unk>', 'it might have beendiicult but frend westra hellinformationin his hands the vd of fwhich he did not hinself now<unk>', 'august <unk><unk> <unk><unk><unk>']\n",
      "['<unk>of what day<unk><unk>', '<unk>god help me<unk> i would not have them ashamed of their father<unk>', 'such a thing i felt was impossible<unk>'] ['af what day<unk>', 'he god help me i would not have them a shange o theirfathr<unk>', ' sut a thing i felt was imcossible<unk>']\n",
      "Test set: Average loss: 0.6189, Average CER: 0.236428 Average WER: 0.5672\n",
      "\n",
      "Train Epoch: 18 [0/6808 (0%)]\tLoss: 0.582134\n",
      "Train Epoch: 18 [600/6808 (9%)]\tLoss: 0.500884\n",
      "Train Epoch: 18 [1200/6808 (18%)]\tLoss: 0.663470\n",
      "Train Epoch: 18 [1800/6808 (26%)]\tLoss: 0.512316\n",
      "Train Epoch: 18 [2400/6808 (35%)]\tLoss: 0.718840\n",
      "Train Epoch: 18 [3000/6808 (44%)]\tLoss: 0.671832\n",
      "Train Epoch: 18 [3600/6808 (53%)]\tLoss: 0.649826\n",
      "Train Epoch: 18 [4200/6808 (62%)]\tLoss: 0.621037\n",
      "Train Epoch: 18 [4800/6808 (70%)]\tLoss: 0.503334\n",
      "Train Epoch: 18 [5400/6808 (79%)]\tLoss: 0.470987\n",
      "Train Epoch: 18 [6000/6808 (88%)]\tLoss: 0.622764\n",
      "Train Epoch: 18 [6600/6808 (97%)]\tLoss: 0.718227\n",
      "\n",
      "evaluating...\n",
      "['<unk>he was a very shy man<unk> mr<unk> holmes<unk>', 'i was wondering what i should say to this dear little woman to-night when she meets me at the door<unk><unk>', '<unk>why<unk> it<unk>s a dummy<unk><unk> said he<unk>'] ['<unk>he was a very shi man<unk> mr<unk> holmes<unk>', 'i was mondring what i should say to this deary little wonmon toight whns she meso me at the door<unk>', '<unk>why it<unk>sa donmey<unk><unk> said he<unk>']\n",
      "['but here she comes in person to resolve our doubts<unk><unk>', '<unk>it might have been difficult<unk> but friend lestrade held information in his hands the value of which he did not himself know<unk>', 'august <unk><unk> <unk><unk><unk><unk>'] ['<unk>but heu<unk> syoe comes omperson to res alfour douts<unk>', 'it might havebendifficult but frimnd lestrade hel im frmmation in his hands the volou offwhich he did not himself no<unk>', 'august <unk><unk> <unk><unk><unk><unk>']\n",
      "['<unk>of what day<unk><unk>', '<unk>god help me<unk> i would not have them ashamed of their father<unk>', 'such a thing i felt was impossible<unk>'] ['aof wlhat day<unk>', 'hegod hel me<unk> i would not have them a shamed of their father<unk>', 'sedt a thing i felt was umcrossible<unk>']\n",
      "Test set: Average loss: 0.6415, Average CER: 0.222255 Average WER: 0.5498\n",
      "\n",
      "Train Epoch: 19 [0/6808 (0%)]\tLoss: 0.651789\n",
      "Train Epoch: 19 [600/6808 (9%)]\tLoss: 0.602351\n",
      "Train Epoch: 19 [1200/6808 (18%)]\tLoss: 0.698272\n",
      "Train Epoch: 19 [1800/6808 (26%)]\tLoss: 0.586251\n",
      "Train Epoch: 19 [2400/6808 (35%)]\tLoss: 0.508181\n",
      "Train Epoch: 19 [3000/6808 (44%)]\tLoss: 0.608401\n",
      "Train Epoch: 19 [3600/6808 (53%)]\tLoss: 0.568462\n",
      "Train Epoch: 19 [4200/6808 (62%)]\tLoss: 0.363753\n",
      "Train Epoch: 19 [4800/6808 (70%)]\tLoss: 0.714877\n",
      "Train Epoch: 19 [5400/6808 (79%)]\tLoss: 0.331479\n",
      "Train Epoch: 19 [6000/6808 (88%)]\tLoss: 0.651312\n",
      "Train Epoch: 19 [6600/6808 (97%)]\tLoss: 0.399707\n",
      "\n",
      "evaluating...\n",
      "['<unk>he was a very shy man<unk> mr<unk> holmes<unk>', 'i was wondering what i should say to this dear little woman to-night when she meets me at the door<unk><unk>', '<unk>why<unk> it<unk>s a dummy<unk><unk> said he<unk>'] ['<unk>he was a very shi man<unk> mr<unk> holmes<unk>', 'i was mondring what i should say to this deary little wonmon toight whns she meso me at the door<unk>', '<unk>why it<unk>sa donmey<unk><unk> said he<unk>']\n",
      "['but here she comes in person to resolve our doubts<unk><unk>', '<unk>it might have been difficult<unk> but friend lestrade held information in his hands the value of which he did not himself know<unk>', 'august <unk><unk> <unk><unk><unk><unk>'] ['<unk>but heu<unk> syoe comes omperson to res alfour douts<unk>', 'it might havebendifficult but frimnd lestrade hel im frmmation in his hands the volou offwhich he did not himself no<unk>', 'august <unk><unk> <unk><unk><unk><unk>']\n",
      "['<unk>of what day<unk><unk>', '<unk>god help me<unk> i would not have them ashamed of their father<unk>', 'such a thing i felt was impossible<unk>'] ['aof wlhat day<unk>', 'hegod hel me<unk> i would not have them a shamed of their father<unk>', 'sedt a thing i felt was umcrossible<unk>']\n",
      "Test set: Average loss: 0.6415, Average CER: 0.222255 Average WER: 0.5498\n",
      "\n",
      "Train Epoch: 20 [0/6808 (0%)]\tLoss: 0.715843\n",
      "Train Epoch: 20 [600/6808 (9%)]\tLoss: 0.768358\n",
      "Train Epoch: 20 [1200/6808 (18%)]\tLoss: 0.541351\n",
      "Train Epoch: 20 [1800/6808 (26%)]\tLoss: 0.487372\n",
      "Train Epoch: 20 [2400/6808 (35%)]\tLoss: 0.559039\n",
      "Train Epoch: 20 [3000/6808 (44%)]\tLoss: 0.587077\n",
      "Train Epoch: 20 [3600/6808 (53%)]\tLoss: 1.222852\n",
      "Train Epoch: 20 [4200/6808 (62%)]\tLoss: 0.639052\n",
      "Train Epoch: 20 [4800/6808 (70%)]\tLoss: 0.953623\n",
      "Train Epoch: 20 [5400/6808 (79%)]\tLoss: 0.542034\n",
      "Train Epoch: 20 [6000/6808 (88%)]\tLoss: 0.601277\n",
      "Train Epoch: 20 [6600/6808 (97%)]\tLoss: 0.590510\n",
      "\n",
      "evaluating...\n",
      "['<unk>he was a very shy man<unk> mr<unk> holmes<unk>', 'i was wondering what i should say to this dear little woman to-night when she meets me at the door<unk><unk>', '<unk>why<unk> it<unk>s a dummy<unk><unk> said he<unk>'] ['<unk>he was a very shi man<unk> mr<unk> holmes<unk>', 'i was mondring what i should say to this deary little wonmon toight whns she meso me at the door<unk>', '<unk>why it<unk>sa donmey<unk><unk> said he<unk>']\n",
      "['but here she comes in person to resolve our doubts<unk><unk>', '<unk>it might have been difficult<unk> but friend lestrade held information in his hands the value of which he did not himself know<unk>', 'august <unk><unk> <unk><unk><unk><unk>'] ['<unk>but heu<unk> syoe comes omperson to res alfour douts<unk>', 'it might havebendifficult but frimnd lestrade hel im frmmation in his hands the volou offwhich he did not himself no<unk>', 'august <unk><unk> <unk><unk><unk><unk>']\n",
      "['<unk>of what day<unk><unk>', '<unk>god help me<unk> i would not have them ashamed of their father<unk>', 'such a thing i felt was impossible<unk>'] ['aof wlhat day<unk>', 'hegod hel me<unk> i would not have them a shamed of their father<unk>', 'sedt a thing i felt was umcrossible<unk>']\n",
      "Test set: Average loss: 0.6415, Average CER: 0.222255 Average WER: 0.5498\n",
      "\n",
      "Train Epoch: 21 [0/6808 (0%)]\tLoss: 0.470608\n",
      "Train Epoch: 21 [600/6808 (9%)]\tLoss: 0.579286\n",
      "Train Epoch: 21 [1200/6808 (18%)]\tLoss: 0.418419\n",
      "Train Epoch: 21 [1800/6808 (26%)]\tLoss: 0.708411\n",
      "Train Epoch: 21 [2400/6808 (35%)]\tLoss: 0.518302\n",
      "Train Epoch: 21 [3000/6808 (44%)]\tLoss: 0.528397\n",
      "Train Epoch: 21 [3600/6808 (53%)]\tLoss: 0.569785\n",
      "Train Epoch: 21 [4200/6808 (62%)]\tLoss: 0.676903\n",
      "Train Epoch: 21 [4800/6808 (70%)]\tLoss: 0.707150\n",
      "Train Epoch: 21 [5400/6808 (79%)]\tLoss: 0.650767\n",
      "Train Epoch: 21 [6000/6808 (88%)]\tLoss: 0.723095\n",
      "Train Epoch: 21 [6600/6808 (97%)]\tLoss: 0.506538\n",
      "\n",
      "evaluating...\n",
      "['<unk>he was a very shy man<unk> mr<unk> holmes<unk>', 'i was wondering what i should say to this dear little woman to-night when she meets me at the door<unk><unk>', '<unk>why<unk> it<unk>s a dummy<unk><unk> said he<unk>'] ['<unk>he was a very shi man<unk> mr<unk> holmes<unk>', 'i was mondring what i should say to this deary little wonmon toight whns she meso me at the door<unk>', '<unk>why it<unk>sa donmey<unk><unk> said he<unk>']\n",
      "['but here she comes in person to resolve our doubts<unk><unk>', '<unk>it might have been difficult<unk> but friend lestrade held information in his hands the value of which he did not himself know<unk>', 'august <unk><unk> <unk><unk><unk><unk>'] ['<unk>but heu<unk> syoe comes omperson to res alfour douts<unk>', 'it might havebendifficult but frimnd lestrade hel im frmmation in his hands the volou offwhich he did not himself no<unk>', 'august <unk><unk> <unk><unk><unk><unk>']\n",
      "['<unk>of what day<unk><unk>', '<unk>god help me<unk> i would not have them ashamed of their father<unk>', 'such a thing i felt was impossible<unk>'] ['aof wlhat day<unk>', 'hegod hel me<unk> i would not have them a shamed of their father<unk>', 'sedt a thing i felt was umcrossible<unk>']\n",
      "Test set: Average loss: 0.6415, Average CER: 0.222255 Average WER: 0.5498\n",
      "\n",
      "Train Epoch: 22 [0/6808 (0%)]\tLoss: 0.599068\n",
      "Train Epoch: 22 [600/6808 (9%)]\tLoss: 0.761498\n",
      "Train Epoch: 22 [1200/6808 (18%)]\tLoss: 0.727208\n",
      "Train Epoch: 22 [1800/6808 (26%)]\tLoss: 0.555543\n",
      "Train Epoch: 22 [2400/6808 (35%)]\tLoss: 0.606638\n",
      "Train Epoch: 22 [3000/6808 (44%)]\tLoss: 0.497934\n",
      "Train Epoch: 22 [3600/6808 (53%)]\tLoss: 0.709567\n",
      "Train Epoch: 22 [4200/6808 (62%)]\tLoss: 0.543911\n",
      "Train Epoch: 22 [4800/6808 (70%)]\tLoss: 0.652600\n",
      "Train Epoch: 22 [5400/6808 (79%)]\tLoss: 0.817387\n",
      "Train Epoch: 22 [6000/6808 (88%)]\tLoss: 0.649319\n",
      "Train Epoch: 22 [6600/6808 (97%)]\tLoss: 0.562697\n",
      "\n",
      "evaluating...\n",
      "['<unk>he was a very shy man<unk> mr<unk> holmes<unk>', 'i was wondering what i should say to this dear little woman to-night when she meets me at the door<unk><unk>', '<unk>why<unk> it<unk>s a dummy<unk><unk> said he<unk>'] ['<unk>he was a very shi man<unk> mr<unk> holmes<unk>', 'i was mondring what i should say to this deary little wonmon toight whns she meso me at the door<unk>', '<unk>why it<unk>sa donmey<unk><unk> said he<unk>']\n",
      "['but here she comes in person to resolve our doubts<unk><unk>', '<unk>it might have been difficult<unk> but friend lestrade held information in his hands the value of which he did not himself know<unk>', 'august <unk><unk> <unk><unk><unk><unk>'] ['<unk>but heu<unk> syoe comes omperson to res alfour douts<unk>', 'it might havebendifficult but frimnd lestrade hel im frmmation in his hands the volou offwhich he did not himself no<unk>', 'august <unk><unk> <unk><unk><unk><unk>']\n",
      "['<unk>of what day<unk><unk>', '<unk>god help me<unk> i would not have them ashamed of their father<unk>', 'such a thing i felt was impossible<unk>'] ['aof wlhat day<unk>', 'hegod hel me<unk> i would not have them a shamed of their father<unk>', 'sedt a thing i felt was umcrossible<unk>']\n",
      "Test set: Average loss: 0.6415, Average CER: 0.222255 Average WER: 0.5498\n",
      "\n",
      "Train Epoch: 23 [0/6808 (0%)]\tLoss: 0.563420\n",
      "Train Epoch: 23 [600/6808 (9%)]\tLoss: 0.487434\n",
      "Train Epoch: 23 [1200/6808 (18%)]\tLoss: 0.360607\n",
      "Train Epoch: 23 [1800/6808 (26%)]\tLoss: 0.809620\n",
      "Train Epoch: 23 [2400/6808 (35%)]\tLoss: 0.674002\n",
      "Train Epoch: 23 [3000/6808 (44%)]\tLoss: 0.683796\n",
      "Train Epoch: 23 [3600/6808 (53%)]\tLoss: 0.550531\n",
      "Train Epoch: 23 [4200/6808 (62%)]\tLoss: 0.447276\n",
      "Train Epoch: 23 [4800/6808 (70%)]\tLoss: 0.535971\n",
      "Train Epoch: 23 [5400/6808 (79%)]\tLoss: 0.697571\n",
      "Train Epoch: 23 [6000/6808 (88%)]\tLoss: 0.563712\n",
      "Train Epoch: 23 [6600/6808 (97%)]\tLoss: 0.611959\n",
      "\n",
      "evaluating...\n",
      "['<unk>he was a very shy man<unk> mr<unk> holmes<unk>', 'i was wondering what i should say to this dear little woman to-night when she meets me at the door<unk><unk>', '<unk>why<unk> it<unk>s a dummy<unk><unk> said he<unk>'] ['<unk>he was a very shi man<unk> mr<unk> holmes<unk>', 'i was mondring what i should say to this deary little wonmon toight whns she meso me at the door<unk>', '<unk>why it<unk>sa donmey<unk><unk> said he<unk>']\n",
      "['but here she comes in person to resolve our doubts<unk><unk>', '<unk>it might have been difficult<unk> but friend lestrade held information in his hands the value of which he did not himself know<unk>', 'august <unk><unk> <unk><unk><unk><unk>'] ['<unk>but heu<unk> syoe comes omperson to res alfour douts<unk>', 'it might havebendifficult but frimnd lestrade hel im frmmation in his hands the volou offwhich he did not himself no<unk>', 'august <unk><unk> <unk><unk><unk><unk>']\n",
      "['<unk>of what day<unk><unk>', '<unk>god help me<unk> i would not have them ashamed of their father<unk>', 'such a thing i felt was impossible<unk>'] ['aof wlhat day<unk>', 'hegod hel me<unk> i would not have them a shamed of their father<unk>', 'sedt a thing i felt was umcrossible<unk>']\n",
      "Test set: Average loss: 0.6415, Average CER: 0.222255 Average WER: 0.5498\n",
      "\n",
      "Train Epoch: 24 [0/6808 (0%)]\tLoss: 0.582773\n",
      "Train Epoch: 24 [600/6808 (9%)]\tLoss: 0.562302\n",
      "Train Epoch: 24 [1200/6808 (18%)]\tLoss: 0.497389\n",
      "Train Epoch: 24 [1800/6808 (26%)]\tLoss: 0.731442\n",
      "Train Epoch: 24 [2400/6808 (35%)]\tLoss: 0.467766\n",
      "Train Epoch: 24 [3000/6808 (44%)]\tLoss: 0.483716\n",
      "Train Epoch: 24 [3600/6808 (53%)]\tLoss: 0.537014\n",
      "Train Epoch: 24 [4200/6808 (62%)]\tLoss: 0.519399\n",
      "Train Epoch: 24 [4800/6808 (70%)]\tLoss: 0.575172\n",
      "Train Epoch: 24 [5400/6808 (79%)]\tLoss: 0.586858\n",
      "Train Epoch: 24 [6000/6808 (88%)]\tLoss: 0.785861\n",
      "Train Epoch: 24 [6600/6808 (97%)]\tLoss: 0.681400\n",
      "\n",
      "evaluating...\n",
      "['<unk>he was a very shy man<unk> mr<unk> holmes<unk>', 'i was wondering what i should say to this dear little woman to-night when she meets me at the door<unk><unk>', '<unk>why<unk> it<unk>s a dummy<unk><unk> said he<unk>'] ['<unk>he was a very shi man<unk> mr<unk> holmes<unk>', 'i was mondring what i should say to this deary little wonmon toight whns she meso me at the door<unk>', '<unk>why it<unk>sa donmey<unk><unk> said he<unk>']\n",
      "['but here she comes in person to resolve our doubts<unk><unk>', '<unk>it might have been difficult<unk> but friend lestrade held information in his hands the value of which he did not himself know<unk>', 'august <unk><unk> <unk><unk><unk><unk>'] ['<unk>but heu<unk> syoe comes omperson to res alfour douts<unk>', 'it might havebendifficult but frimnd lestrade hel im frmmation in his hands the volou offwhich he did not himself no<unk>', 'august <unk><unk> <unk><unk><unk><unk>']\n",
      "['<unk>of what day<unk><unk>', '<unk>god help me<unk> i would not have them ashamed of their father<unk>', 'such a thing i felt was impossible<unk>'] ['aof wlhat day<unk>', 'hegod hel me<unk> i would not have them a shamed of their father<unk>', 'sedt a thing i felt was umcrossible<unk>']\n",
      "Test set: Average loss: 0.6415, Average CER: 0.222255 Average WER: 0.5498\n",
      "\n",
      "Train Epoch: 25 [0/6808 (0%)]\tLoss: 0.641729\n",
      "Train Epoch: 25 [600/6808 (9%)]\tLoss: 0.629569\n",
      "Train Epoch: 25 [1200/6808 (18%)]\tLoss: 0.770202\n",
      "Train Epoch: 25 [1800/6808 (26%)]\tLoss: 0.577269\n",
      "Train Epoch: 25 [2400/6808 (35%)]\tLoss: 0.631632\n",
      "Train Epoch: 25 [3000/6808 (44%)]\tLoss: 0.640913\n",
      "Train Epoch: 25 [3600/6808 (53%)]\tLoss: 0.635829\n",
      "Train Epoch: 25 [4200/6808 (62%)]\tLoss: 0.544853\n",
      "Train Epoch: 25 [4800/6808 (70%)]\tLoss: 0.618152\n",
      "Train Epoch: 25 [5400/6808 (79%)]\tLoss: 0.401377\n",
      "Train Epoch: 25 [6000/6808 (88%)]\tLoss: 0.765387\n",
      "Train Epoch: 25 [6600/6808 (97%)]\tLoss: 0.654373\n",
      "\n",
      "evaluating...\n",
      "['<unk>he was a very shy man<unk> mr<unk> holmes<unk>', 'i was wondering what i should say to this dear little woman to-night when she meets me at the door<unk><unk>', '<unk>why<unk> it<unk>s a dummy<unk><unk> said he<unk>'] ['<unk>he was a very shi man<unk> mr<unk> holmes<unk>', 'i was mondring what i should say to this deary little wonmon toight whns she meso me at the door<unk>', '<unk>why it<unk>sa donmey<unk><unk> said he<unk>']\n",
      "['but here she comes in person to resolve our doubts<unk><unk>', '<unk>it might have been difficult<unk> but friend lestrade held information in his hands the value of which he did not himself know<unk>', 'august <unk><unk> <unk><unk><unk><unk>'] ['<unk>but heu<unk> syoe comes omperson to res alfour douts<unk>', 'it might havebendifficult but frimnd lestrade hel im frmmation in his hands the volou offwhich he did not himself no<unk>', 'august <unk><unk> <unk><unk><unk><unk>']\n",
      "['<unk>of what day<unk><unk>', '<unk>god help me<unk> i would not have them ashamed of their father<unk>', 'such a thing i felt was impossible<unk>'] ['aof wlhat day<unk>', 'hegod hel me<unk> i would not have them a shamed of their father<unk>', 'sedt a thing i felt was umcrossible<unk>']\n",
      "Test set: Average loss: 0.6415, Average CER: 0.222255 Average WER: 0.5498\n",
      "\n",
      "Train Epoch: 26 [0/6808 (0%)]\tLoss: 0.792375\n",
      "Train Epoch: 26 [600/6808 (9%)]\tLoss: 0.538133\n",
      "Train Epoch: 26 [1200/6808 (18%)]\tLoss: 0.979943\n",
      "Train Epoch: 26 [1800/6808 (26%)]\tLoss: 0.796259\n",
      "Train Epoch: 26 [2400/6808 (35%)]\tLoss: 0.701309\n",
      "Train Epoch: 26 [3000/6808 (44%)]\tLoss: 0.762962\n",
      "Train Epoch: 26 [3600/6808 (53%)]\tLoss: 0.493761\n",
      "Train Epoch: 26 [4200/6808 (62%)]\tLoss: 0.441264\n",
      "Train Epoch: 26 [4800/6808 (70%)]\tLoss: 0.476655\n",
      "Train Epoch: 26 [5400/6808 (79%)]\tLoss: 0.532753\n",
      "Train Epoch: 26 [6000/6808 (88%)]\tLoss: 0.515245\n",
      "Train Epoch: 26 [6600/6808 (97%)]\tLoss: 0.569057\n",
      "\n",
      "evaluating...\n",
      "['<unk>he was a very shy man<unk> mr<unk> holmes<unk>', 'i was wondering what i should say to this dear little woman to-night when she meets me at the door<unk><unk>', '<unk>why<unk> it<unk>s a dummy<unk><unk> said he<unk>'] ['<unk>he was a very shi man<unk> mr<unk> holmes<unk>', 'i was mondring what i should say to this deary little wonmon toight whns she meso me at the door<unk>', '<unk>why it<unk>sa donmey<unk><unk> said he<unk>']\n",
      "['but here she comes in person to resolve our doubts<unk><unk>', '<unk>it might have been difficult<unk> but friend lestrade held information in his hands the value of which he did not himself know<unk>', 'august <unk><unk> <unk><unk><unk><unk>'] ['<unk>but heu<unk> syoe comes omperson to res alfour douts<unk>', 'it might havebendifficult but frimnd lestrade hel im frmmation in his hands the volou offwhich he did not himself no<unk>', 'august <unk><unk> <unk><unk><unk><unk>']\n",
      "['<unk>of what day<unk><unk>', '<unk>god help me<unk> i would not have them ashamed of their father<unk>', 'such a thing i felt was impossible<unk>'] ['aof wlhat day<unk>', 'hegod hel me<unk> i would not have them a shamed of their father<unk>', 'sedt a thing i felt was umcrossible<unk>']\n",
      "Test set: Average loss: 0.6415, Average CER: 0.222255 Average WER: 0.5498\n",
      "\n",
      "Train Epoch: 27 [0/6808 (0%)]\tLoss: 0.544036\n",
      "Train Epoch: 27 [600/6808 (9%)]\tLoss: 0.624651\n",
      "Train Epoch: 27 [1200/6808 (18%)]\tLoss: 0.641537\n",
      "Train Epoch: 27 [1800/6808 (26%)]\tLoss: 0.533174\n",
      "Train Epoch: 27 [2400/6808 (35%)]\tLoss: 0.626862\n",
      "Train Epoch: 27 [3000/6808 (44%)]\tLoss: 0.808130\n",
      "Train Epoch: 27 [3600/6808 (53%)]\tLoss: 0.524234\n",
      "Train Epoch: 27 [4200/6808 (62%)]\tLoss: 0.534518\n",
      "Train Epoch: 27 [4800/6808 (70%)]\tLoss: 0.652743\n",
      "Train Epoch: 27 [5400/6808 (79%)]\tLoss: 0.419144\n",
      "Train Epoch: 27 [6000/6808 (88%)]\tLoss: 0.629231\n",
      "Train Epoch: 27 [6600/6808 (97%)]\tLoss: 0.538902\n",
      "\n",
      "evaluating...\n",
      "['<unk>he was a very shy man<unk> mr<unk> holmes<unk>', 'i was wondering what i should say to this dear little woman to-night when she meets me at the door<unk><unk>', '<unk>why<unk> it<unk>s a dummy<unk><unk> said he<unk>'] ['<unk>he was a very shi man<unk> mr<unk> holmes<unk>', 'i was mondring what i should say to this deary little wonmon toight whns she meso me at the door<unk>', '<unk>why it<unk>sa donmey<unk><unk> said he<unk>']\n",
      "['but here she comes in person to resolve our doubts<unk><unk>', '<unk>it might have been difficult<unk> but friend lestrade held information in his hands the value of which he did not himself know<unk>', 'august <unk><unk> <unk><unk><unk><unk>'] ['<unk>but heu<unk> syoe comes omperson to res alfour douts<unk>', 'it might havebendifficult but frimnd lestrade hel im frmmation in his hands the volou offwhich he did not himself no<unk>', 'august <unk><unk> <unk><unk><unk><unk>']\n",
      "['<unk>of what day<unk><unk>', '<unk>god help me<unk> i would not have them ashamed of their father<unk>', 'such a thing i felt was impossible<unk>'] ['aof wlhat day<unk>', 'hegod hel me<unk> i would not have them a shamed of their father<unk>', 'sedt a thing i felt was umcrossible<unk>']\n",
      "Test set: Average loss: 0.6415, Average CER: 0.222255 Average WER: 0.5498\n",
      "\n",
      "Train Epoch: 28 [0/6808 (0%)]\tLoss: 0.689916\n",
      "Train Epoch: 28 [600/6808 (9%)]\tLoss: 0.520801\n",
      "Train Epoch: 28 [1200/6808 (18%)]\tLoss: 0.603519\n",
      "Train Epoch: 28 [1800/6808 (26%)]\tLoss: 0.339099\n",
      "Train Epoch: 28 [2400/6808 (35%)]\tLoss: 0.736386\n",
      "Train Epoch: 28 [3000/6808 (44%)]\tLoss: 0.759008\n",
      "Train Epoch: 28 [3600/6808 (53%)]\tLoss: 0.578616\n",
      "Train Epoch: 28 [4200/6808 (62%)]\tLoss: 0.564310\n",
      "Train Epoch: 28 [4800/6808 (70%)]\tLoss: 0.729020\n",
      "Train Epoch: 28 [5400/6808 (79%)]\tLoss: 0.656763\n",
      "Train Epoch: 28 [6000/6808 (88%)]\tLoss: 0.659390\n",
      "Train Epoch: 28 [6600/6808 (97%)]\tLoss: 0.423373\n",
      "\n",
      "evaluating...\n",
      "['<unk>he was a very shy man<unk> mr<unk> holmes<unk>', 'i was wondering what i should say to this dear little woman to-night when she meets me at the door<unk><unk>', '<unk>why<unk> it<unk>s a dummy<unk><unk> said he<unk>'] ['<unk>he was a very shi man<unk> mr<unk> holmes<unk>', 'i was mondring what i should say to this deary little wonmon toight whns she meso me at the door<unk>', '<unk>why it<unk>sa donmey<unk><unk> said he<unk>']\n",
      "['but here she comes in person to resolve our doubts<unk><unk>', '<unk>it might have been difficult<unk> but friend lestrade held information in his hands the value of which he did not himself know<unk>', 'august <unk><unk> <unk><unk><unk><unk>'] ['<unk>but heu<unk> syoe comes omperson to res alfour douts<unk>', 'it might havebendifficult but frimnd lestrade hel im frmmation in his hands the volou offwhich he did not himself no<unk>', 'august <unk><unk> <unk><unk><unk><unk>']\n",
      "['<unk>of what day<unk><unk>', '<unk>god help me<unk> i would not have them ashamed of their father<unk>', 'such a thing i felt was impossible<unk>'] ['aof wlhat day<unk>', 'hegod hel me<unk> i would not have them a shamed of their father<unk>', 'sedt a thing i felt was umcrossible<unk>']\n",
      "Test set: Average loss: 0.6415, Average CER: 0.222255 Average WER: 0.5498\n",
      "\n",
      "Train Epoch: 29 [0/6808 (0%)]\tLoss: 0.483966\n",
      "Train Epoch: 29 [600/6808 (9%)]\tLoss: 0.709815\n",
      "Train Epoch: 29 [1200/6808 (18%)]\tLoss: 0.411446\n",
      "Train Epoch: 29 [1800/6808 (26%)]\tLoss: 0.530337\n",
      "Train Epoch: 29 [2400/6808 (35%)]\tLoss: 0.665450\n",
      "Train Epoch: 29 [3000/6808 (44%)]\tLoss: 0.673582\n",
      "Train Epoch: 29 [3600/6808 (53%)]\tLoss: 0.630073\n",
      "Train Epoch: 29 [4200/6808 (62%)]\tLoss: 0.555442\n",
      "Train Epoch: 29 [4800/6808 (70%)]\tLoss: 0.530891\n",
      "Train Epoch: 29 [5400/6808 (79%)]\tLoss: 0.590709\n",
      "Train Epoch: 29 [6000/6808 (88%)]\tLoss: 0.518234\n",
      "Train Epoch: 29 [6600/6808 (97%)]\tLoss: 0.651582\n",
      "\n",
      "evaluating...\n",
      "['<unk>he was a very shy man<unk> mr<unk> holmes<unk>', 'i was wondering what i should say to this dear little woman to-night when she meets me at the door<unk><unk>', '<unk>why<unk> it<unk>s a dummy<unk><unk> said he<unk>'] ['<unk>he was a very shi man<unk> mr<unk> holmes<unk>', 'i was mondring what i should say to this deary little wonmon toight whns she meso me at the door<unk>', '<unk>why it<unk>sa donmey<unk><unk> said he<unk>']\n",
      "['but here she comes in person to resolve our doubts<unk><unk>', '<unk>it might have been difficult<unk> but friend lestrade held information in his hands the value of which he did not himself know<unk>', 'august <unk><unk> <unk><unk><unk><unk>'] ['<unk>but heu<unk> syoe comes omperson to res alfour douts<unk>', 'it might havebendifficult but frimnd lestrade hel im frmmation in his hands the volou offwhich he did not himself no<unk>', 'august <unk><unk> <unk><unk><unk><unk>']\n",
      "['<unk>of what day<unk><unk>', '<unk>god help me<unk> i would not have them ashamed of their father<unk>', 'such a thing i felt was impossible<unk>'] ['aof wlhat day<unk>', 'hegod hel me<unk> i would not have them a shamed of their father<unk>', 'sedt a thing i felt was umcrossible<unk>']\n",
      "Test set: Average loss: 0.6415, Average CER: 0.222255 Average WER: 0.5498\n",
      "\n",
      "Train Epoch: 30 [0/6808 (0%)]\tLoss: 0.637829\n",
      "Train Epoch: 30 [600/6808 (9%)]\tLoss: 0.553259\n",
      "Train Epoch: 30 [1200/6808 (18%)]\tLoss: 0.658311\n",
      "Train Epoch: 30 [1800/6808 (26%)]\tLoss: 0.477030\n",
      "Train Epoch: 30 [2400/6808 (35%)]\tLoss: 0.654610\n",
      "Train Epoch: 30 [3000/6808 (44%)]\tLoss: 0.678258\n",
      "Train Epoch: 30 [3600/6808 (53%)]\tLoss: 0.566816\n",
      "Train Epoch: 30 [4200/6808 (62%)]\tLoss: 0.527538\n",
      "Train Epoch: 30 [4800/6808 (70%)]\tLoss: 0.599831\n",
      "Train Epoch: 30 [5400/6808 (79%)]\tLoss: 0.520983\n",
      "Train Epoch: 30 [6000/6808 (88%)]\tLoss: 0.572647\n",
      "Train Epoch: 30 [6600/6808 (97%)]\tLoss: 0.721272\n",
      "\n",
      "evaluating...\n",
      "['<unk>he was a very shy man<unk> mr<unk> holmes<unk>', 'i was wondering what i should say to this dear little woman to-night when she meets me at the door<unk><unk>', '<unk>why<unk> it<unk>s a dummy<unk><unk> said he<unk>'] ['<unk>he was a very shi man<unk> mr<unk> holmes<unk>', 'i was mondring what i should say to this deary little wonmon toight whns she meso me at the door<unk>', '<unk>why it<unk>sa donmey<unk><unk> said he<unk>']\n",
      "['but here she comes in person to resolve our doubts<unk><unk>', '<unk>it might have been difficult<unk> but friend lestrade held information in his hands the value of which he did not himself know<unk>', 'august <unk><unk> <unk><unk><unk><unk>'] ['<unk>but heu<unk> syoe comes omperson to res alfour douts<unk>', 'it might havebendifficult but frimnd lestrade hel im frmmation in his hands the volou offwhich he did not himself no<unk>', 'august <unk><unk> <unk><unk><unk><unk>']\n",
      "['<unk>of what day<unk><unk>', '<unk>god help me<unk> i would not have them ashamed of their father<unk>', 'such a thing i felt was impossible<unk>'] ['aof wlhat day<unk>', 'hegod hel me<unk> i would not have them a shamed of their father<unk>', 'sedt a thing i felt was umcrossible<unk>']\n",
      "Test set: Average loss: 0.6415, Average CER: 0.222255 Average WER: 0.5498\n",
      "\n",
      "Train Epoch: 31 [0/6808 (0%)]\tLoss: 0.507191\n",
      "Train Epoch: 31 [600/6808 (9%)]\tLoss: 0.607614\n",
      "Train Epoch: 31 [1200/6808 (18%)]\tLoss: 0.464473\n",
      "Train Epoch: 31 [1800/6808 (26%)]\tLoss: 0.608483\n",
      "Train Epoch: 31 [2400/6808 (35%)]\tLoss: 0.642753\n",
      "Train Epoch: 31 [3000/6808 (44%)]\tLoss: 0.634313\n",
      "Train Epoch: 31 [3600/6808 (53%)]\tLoss: 0.525953\n",
      "Train Epoch: 31 [4200/6808 (62%)]\tLoss: 0.722024\n",
      "Train Epoch: 31 [4800/6808 (70%)]\tLoss: 0.468154\n",
      "Train Epoch: 31 [5400/6808 (79%)]\tLoss: 0.580455\n",
      "Train Epoch: 31 [6000/6808 (88%)]\tLoss: 0.582685\n",
      "Train Epoch: 31 [6600/6808 (97%)]\tLoss: 0.697442\n",
      "\n",
      "evaluating...\n",
      "['<unk>he was a very shy man<unk> mr<unk> holmes<unk>', 'i was wondering what i should say to this dear little woman to-night when she meets me at the door<unk><unk>', '<unk>why<unk> it<unk>s a dummy<unk><unk> said he<unk>'] ['<unk>he was a very shi man<unk> mr<unk> holmes<unk>', 'i was mondring what i should say to this deary little wonmon toight whns she meso me at the door<unk>', '<unk>why it<unk>sa donmey<unk><unk> said he<unk>']\n",
      "['but here she comes in person to resolve our doubts<unk><unk>', '<unk>it might have been difficult<unk> but friend lestrade held information in his hands the value of which he did not himself know<unk>', 'august <unk><unk> <unk><unk><unk><unk>'] ['<unk>but heu<unk> syoe comes omperson to res alfour douts<unk>', 'it might havebendifficult but frimnd lestrade hel im frmmation in his hands the volou offwhich he did not himself no<unk>', 'august <unk><unk> <unk><unk><unk><unk>']\n",
      "['<unk>of what day<unk><unk>', '<unk>god help me<unk> i would not have them ashamed of their father<unk>', 'such a thing i felt was impossible<unk>'] ['aof wlhat day<unk>', 'hegod hel me<unk> i would not have them a shamed of their father<unk>', 'sedt a thing i felt was umcrossible<unk>']\n",
      "Test set: Average loss: 0.6415, Average CER: 0.222255 Average WER: 0.5498\n",
      "\n",
      "Train Epoch: 32 [0/6808 (0%)]\tLoss: 0.740877\n",
      "Train Epoch: 32 [600/6808 (9%)]\tLoss: 0.459429\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/joe/projects/asr/train.ipynb Cell 16'\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/joe/projects/asr/train.ipynb#ch0000014?line=2'>3</a>\u001b[0m epochs \u001b[39m=\u001b[39m \u001b[39m50\u001b[39m \u001b[39m# 10\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/joe/projects/asr/train.ipynb#ch0000014?line=3'>4</a>\u001b[0m dataset_path \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m/home/joe/datasets/ljspeech/\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/joe/projects/asr/train.ipynb#ch0000014?line=4'>5</a>\u001b[0m model \u001b[39m=\u001b[39m main(dataset_path, learning_rate, batch_size, epochs)\n",
      "\u001b[1;32m/home/joe/projects/asr/train.ipynb Cell 12'\u001b[0m in \u001b[0;36mmain\u001b[0;34m(dataset_path, learning_rate, batch_size, epochs)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/joe/projects/asr/train.ipynb#ch0000010?line=164'>165</a>\u001b[0m iter_meter \u001b[39m=\u001b[39m IterMeter()\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/joe/projects/asr/train.ipynb#ch0000010?line=165'>166</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, epochs \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m):\n\u001b[0;32m--> <a href='vscode-notebook-cell:/home/joe/projects/asr/train.ipynb#ch0000010?line=166'>167</a>\u001b[0m     train(model, device, train_loader, criterion, optimizer, scheduler, epoch, iter_meter) \u001b[39m#, experiment)\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/joe/projects/asr/train.ipynb#ch0000010?line=167'>168</a>\u001b[0m     test_loss \u001b[39m=\u001b[39m test(model, device, test_loader, criterion) \u001b[39m# , epoch, iter_meter, experiment)\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/joe/projects/asr/train.ipynb#ch0000010?line=169'>170</a>\u001b[0m     \u001b[39mif\u001b[39;00m test_loss \u001b[39m<\u001b[39m best_test_loss:\n",
      "\u001b[1;32m/home/joe/projects/asr/train.ipynb Cell 12'\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, device, train_loader, criterion, optimizer, scheduler, epoch, iter_meter)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/joe/projects/asr/train.ipynb#ch0000010?line=40'>41</a>\u001b[0m \u001b[39m# loss.backward()\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/joe/projects/asr/train.ipynb#ch0000010?line=41'>42</a>\u001b[0m scaler\u001b[39m.\u001b[39mscale(loss)\u001b[39m.\u001b[39mbackward()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/joe/projects/asr/train.ipynb#ch0000010?line=42'>43</a>\u001b[0m scaler\u001b[39m.\u001b[39;49mstep(optimizer)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/joe/projects/asr/train.ipynb#ch0000010?line=43'>44</a>\u001b[0m \u001b[39m# scaler.step(scheduler)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/joe/projects/asr/train.ipynb#ch0000010?line=44'>45</a>\u001b[0m scaler\u001b[39m.\u001b[39mupdate()\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/cuda/amp/grad_scaler.py:338\u001b[0m, in \u001b[0;36mGradScaler.step\u001b[0;34m(self, optimizer, *args, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/joe/.local/lib/python3.8/site-packages/torch/cuda/amp/grad_scaler.py?line=333'>334</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39munscale_(optimizer)\n\u001b[1;32m    <a href='file:///home/joe/.local/lib/python3.8/site-packages/torch/cuda/amp/grad_scaler.py?line=335'>336</a>\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mlen\u001b[39m(optimizer_state[\u001b[39m\"\u001b[39m\u001b[39mfound_inf_per_device\u001b[39m\u001b[39m\"\u001b[39m]) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mNo inf checks were recorded for this optimizer.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> <a href='file:///home/joe/.local/lib/python3.8/site-packages/torch/cuda/amp/grad_scaler.py?line=337'>338</a>\u001b[0m retval \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_maybe_opt_step(optimizer, optimizer_state, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    <a href='file:///home/joe/.local/lib/python3.8/site-packages/torch/cuda/amp/grad_scaler.py?line=339'>340</a>\u001b[0m optimizer_state[\u001b[39m\"\u001b[39m\u001b[39mstage\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m OptState\u001b[39m.\u001b[39mSTEPPED\n\u001b[1;32m    <a href='file:///home/joe/.local/lib/python3.8/site-packages/torch/cuda/amp/grad_scaler.py?line=341'>342</a>\u001b[0m \u001b[39mreturn\u001b[39;00m retval\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/cuda/amp/grad_scaler.py:284\u001b[0m, in \u001b[0;36mGradScaler._maybe_opt_step\u001b[0;34m(self, optimizer, optimizer_state, *args, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/joe/.local/lib/python3.8/site-packages/torch/cuda/amp/grad_scaler.py?line=281'>282</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_maybe_opt_step\u001b[39m(\u001b[39mself\u001b[39m, optimizer, optimizer_state, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    <a href='file:///home/joe/.local/lib/python3.8/site-packages/torch/cuda/amp/grad_scaler.py?line=282'>283</a>\u001b[0m     retval \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> <a href='file:///home/joe/.local/lib/python3.8/site-packages/torch/cuda/amp/grad_scaler.py?line=283'>284</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39msum\u001b[39;49m(v\u001b[39m.\u001b[39;49mitem() \u001b[39mfor\u001b[39;49;00m v \u001b[39min\u001b[39;49;00m optimizer_state[\u001b[39m\"\u001b[39;49m\u001b[39mfound_inf_per_device\u001b[39;49m\u001b[39m\"\u001b[39;49m]\u001b[39m.\u001b[39;49mvalues()):\n\u001b[1;32m    <a href='file:///home/joe/.local/lib/python3.8/site-packages/torch/cuda/amp/grad_scaler.py?line=284'>285</a>\u001b[0m         retval \u001b[39m=\u001b[39m optimizer\u001b[39m.\u001b[39mstep(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    <a href='file:///home/joe/.local/lib/python3.8/site-packages/torch/cuda/amp/grad_scaler.py?line=285'>286</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m retval\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/cuda/amp/grad_scaler.py:284\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    <a href='file:///home/joe/.local/lib/python3.8/site-packages/torch/cuda/amp/grad_scaler.py?line=281'>282</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_maybe_opt_step\u001b[39m(\u001b[39mself\u001b[39m, optimizer, optimizer_state, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    <a href='file:///home/joe/.local/lib/python3.8/site-packages/torch/cuda/amp/grad_scaler.py?line=282'>283</a>\u001b[0m     retval \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> <a href='file:///home/joe/.local/lib/python3.8/site-packages/torch/cuda/amp/grad_scaler.py?line=283'>284</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39msum\u001b[39m(v\u001b[39m.\u001b[39;49mitem() \u001b[39mfor\u001b[39;00m v \u001b[39min\u001b[39;00m optimizer_state[\u001b[39m\"\u001b[39m\u001b[39mfound_inf_per_device\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mvalues()):\n\u001b[1;32m    <a href='file:///home/joe/.local/lib/python3.8/site-packages/torch/cuda/amp/grad_scaler.py?line=284'>285</a>\u001b[0m         retval \u001b[39m=\u001b[39m optimizer\u001b[39m.\u001b[39mstep(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    <a href='file:///home/joe/.local/lib/python3.8/site-packages/torch/cuda/amp/grad_scaler.py?line=285'>286</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m retval\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "learning_rate = 5e-4\n",
    "batch_size = 6\n",
    "epochs = 50 # 10\n",
    "dataset_path = \"/home/joe/datasets/ljspeech/\"\n",
    "model = main(dataset_path, learning_rate, batch_size, epochs)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
